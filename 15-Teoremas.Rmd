---
editor_options: 
  markdown: 
    wrap: 72
header-includes:
  - \usepackage{xcolor}
---

# - Teoremas

Apresentamos aqui alguns teoremas importantes para um aprofundamento em Regressão e Estatística

## Independência e Correlação

Sejam $X$ e $Z$ duas variáveis aleatórias, com médias $\mu_x$ e $\mu_z$ respectivamente.

Definição 15.1 (independência): Dizemos que $X$ e $Z$ são independentes, denotado por $X \perp Z$, se $Z$ não traz nenhuma informação sobre $X$ e vice-versa. Formalmente, isso quer dizer que:

$f(x,z) = f_x(x)f_z(x)$, isto é, a função densidade conjunta é o produto das marginais.

Proposição 15.1 (independência na média): Se $X$ e $Z$ são independentes, então são independentes na média (o reverso não é verdade). 

Prova:

$\mathbb{E}[X|Z] = \sum xP_{x|z}(x) = \sum x \frac{P_x(x)P_Z(z)}{P_Z(z)} = \sum x P_x(x) = \mu_x$

Da segunda igualdade para a terceira usamos a definição de probabilidade condicional.


Definição 15.1.2 (independência linear): Dizemos que $X$ e $Z$ são não-correlacionados ou linearmente independentes se $\mathbb{E}[XZ] = \mu_x\mu_z \iff cov(X,Z) = 0$

Proposição 15.1.2 (independência na média para independência linear): Se $X$ e $Y$ São independentes na média, então são linearmente independentes. O reverso não é verdade.

\begin{aligned}
\mathbb{E}[XZ]
&= \mathbb{E}\!\big[\mathbb{E}[XZ \mid Z]\big]
\quad \text{(lei das esperanças iteradas)}\\
&= \mathbb{E}\!\big[ Z\,\mathbb{E}[X \mid Z] \big]
\quad \text{(condicionado em $Z$, ele é constante)}\\
&= \mathbb{E}\!\big[ Z\,\mathbb{E}[X] \big]
\quad \text{(independência na média)}\\
&= \mathbb{E}[X]\,\mathbb{E}[Z]
\quad \text{(constante sai da esperança)}\\
&= \mu_X \mu_Z,
\end{aligned}
logo $\mathrm{cov}(X,Z)=\mathbb{E}[XZ]-\mu_X\mu_Z=0$.

**Observação**:

O reverso não vale: por exemplo, se $Z$ é uniforme em $(-1,1)$ e $X = Z^2$, então $\mathrm{cov}(X,Z) = 0$, mas $\mathbb{E}[X \mid Z] \neq \mathbb{E}[X]$.

Para ver que $\mathrm{cov}(X,Z) = 0$, considere:

\begin{aligned}
\mathbb{E}[XZ]-\mu_X\mu_Z
&= \mathbb{E}[XZ]-\mu_X 0
&= \mathbb{E}[XZ]
&= \mathbb{E}[Z^3] = 0
\end{aligned}

## Propriedade de estimadores

### Viés

Definição 15.2 (viés). Seja $\theta_0$ o verdadeiro valor de um parâmetro de uma variável aleatória e seja $\hat{\theta}$ um um estimador de $\theta_0$. O viés de um estimador é o desvio absoluto entre o verdadeiro valor do parâmetro e a esperança do estimador. 

$|\theta_0 - \mathbb{E}[\hat{\theta}]|$. 

Dizemos que um estimador é não-viesado se e somente se o viés é zero. Lembrem-se que **se e somente se** é uma relação de biimplicação, ou seja, se é não viesado, o viés é zero. Se o viés é zero, é não viesado.

Observaçao: o estimador é que pode ser viesado ou não. Isso não se aplica a uma estimativa. Incorremos em erro quando dizemos que a estimativa é viesada (ou não viesada). O que é viesado ou não é o estimador. Similarmente, não cabe falar em amostra viesada ou não-viesada.

### Variância

Definição 15.3. (variância do estimador). Seja $\theta_0$ o verdadeiro valor de um parâmetro de uma variável aleatória e seja $\hat{\theta}$ um um estimador de $\theta_0$. A variância de um estimador é valor esperado do quadrado dos desvios amostrais.

$Var[\hat{\theta}] = \mathbb{E}[\left(\hat{\theta} - \mathbb{E}[\hat{\theta}]  \right)^2]$

Proposição 15.3. Seja $Z_1, Z_2, ...$ uma sequência de variáveis aleatórias i.i.d tal que, para todo $i$, $\mathbb{E}[Z_i] = \mu$ e $Var[Z_i] = \sigma^2$. Seja $X_n$ a média amostral das primeiras $n$ variáveis. A variância amostral é igual à variância de $Z$ dividida por $n$.

$Var[X_n] = \frac{Var[Z]}{n} = \frac{\sigma^2}{n}$

Prova:

### Eficiência

Entre um número de estimadores da mesma classe (por exemplo, todos são não-viesados e têm variância finita), o estimador com a menor variância é chamado de estimador eficiente.

### Consistência

Definição:
Um estimador $\hat\theta_n$ é **consistente** para $\theta$ se $\hat\theta_n \xrightarrow{p} \theta$, i.e.,
$\forall \varepsilon>0,\ \lim_{n\to\infty}\Pr(|\hat\theta_n-\theta|>\varepsilon)=0$.

Proposição: Condição suficiente via Chebyshev
Se $\hat\theta_n$ é não-viesado e $\mathrm{Var}(\hat\theta_n)\to 0$ quando $n\to\infty$, então $\hat\theta_n$ é consistente.

Pela desigualdade de Chebyshev,
$\Pr(|\hat\theta_n-\theta|>\varepsilon)\le \mathrm{Var}(\hat\theta_n)/\varepsilon^2 \to 0$.

## Exercícios para o leitor 

1. Suponha que $Z$ é uma distribuição uniforme $Z \sim U(-1,1)$. 
Mosrte que se $X = Z^2$, então são independentes na média, mas não independentes entre si.


Mostre que $X$ e $Z$ são independentes na média, mas não independentes entre si no seguinte caso:

2. Seja $Y$ uma variável aleatória uniforme $Y \sim U(-1,1)$. Defina $X = Y+ \epsilon_1$ e $Z = Y+ \epsilon_2$, em que $\epsilon_1$ e $\epsilon_2$ são ruídos aleatórios com média zero, independentes entre si, $X$, $Z$ e $Y$.

Dica: em algum momento, use a definição (15.2) de que se $Cov(X,Z) \neq 0$, então $X$ e $Z$ não são independentes.

3. Mostre que a média amostral é um estimador não-viesado para a média populacional.

## Lei das Esperanças Iteradas

Teorema: Lei simples das Esperanças Iteradas
Se $\mathbb{E}[Y] < \infty$ então para qualquer vetor de variáveis aleatórias $X$, $\mathbb{E}[\mathbb{E}[Y|X]] = \mathbb{E}[Y]$.

Prova: Para $X$ discreto:
Como a esperença condicional é uma função de $X$ apenas, a espereança é calculada somando apenas com respeito a $X$.

\begin{align*}
1.\mathbb{E}[\mathbb{E}[Y|X]] &= \sum_{j=1}^{\infty}\mathbb{E}[Y|X=x_j]P[X=x_j] \\
2. &= \sum_{j=1}^{\infty}\left(\sum_{i=1}^{\infty} y_i P[Y=y_i|X=x_j]\right)P[X=x_j] \\
2.5. &= \sum_{j=1}^{\infty}y_1 P[Y=y_1|X=x_j]P[X=x_j] + y_2 P[Y=y_2|X=x_j]) P[X=x_j] + \cdots \\
3. &= \sum_{j=1}^{\infty}\sum_{i=1}^{\infty} y_i P[Y=y_i|X=x_j]P[X=x_j] \\
4. &= \sum_{j=1}^{\infty}\sum_{i=1}^{\infty} y_i P[Y=y_i, X=x_j] \\
5. \ &= \sum_{i=1}^{\infty} y_i \color{red}{\sum_{j=1}^{\infty}} P[Y=y_i, X=x_j] \quad \text{(Troca na ordem dos somatórios)} \\
6. &= \sum_{i=1}^{\infty} y_i P[Y=y_i] \\
7. &= \mathbb{E}[Y]
\end{align*}

Teorema: Lei das Esperanças Iteradas:
Se $\mathbb{E}[Y] < \infty$ então para quaisquer vetores de variáveis aleatórias $X_1$ e $X_2$, $\mathbb{E}[\mathbb{E}[Y|X_1,X_2]|X_1] = \mathbb{E}[Y|X_1]$.

A esperança interna condiciona em $X_1$ e $X_2$, e a externa apenas em $X_1$. E o resultado é o condiionamento apenas em $X_1$. Uma forma de memorizar o teorema é dizer que "o menor conjunto de informações ganha".

## Teorema do Condicionamento

Propisição: Se $\mathbb{E}[Y] < \infty$, então
$\mathbb{E}[g(x)Y|X] = g(x) \mathbb{E}[Y|X]$.

Se, além disso, $\mathbb{E}[g(x)] < \infty$, então
$\mathbb{E}[g(x)Y] = \mathbb{E}[g(x) \mathbb{E}[Y|X]]$

Prova 1:

$\mathbb{E}[g(x)Y|X] = \sum_{y} g(x) y P(Y=y|X=x) = g(x) \sum_{y} y P(Y=y|X=x) = g(x)\mathbb{E}[Y|X]$.

Como estamos somando em $Y$, $g(x)$ age como uma constante com respeito à somatória de $Y$. Depois é só aplicar a definição de esperança condicional.

Prova 2.
Basta aplicar a lei das esperanças iteradas e a prova acima:
$\mathbb{E}[g(x)Y] = \mathbb{E}[\mathbb{E}[g(x)Y|X]] = \mathbb{E}[g(x)\mathbb{E}[Y|X]]$