[["index.html", "Introdução à Regresssão para Ciências Sociais Capítulo 1 - Prefácio", " Introdução à Regresssão para Ciências Sociais Manoel Galdino 2025-11-11 Capítulo 1 - Prefácio Esse livro é, por enquanto, apenas uma repositório de notas de aula do curso de graduação de Ciências Sociais da USP, FLP0468 Métodos Quantitativos de Pesquisa na Ciência Política IV, de introdução à regressão, bem como o curso de pós-graduação, Métodos II. Agradeço aos alunos do curso pelos feedbacks (futuros e presentes) sobre esse material, bem como aos monitores do curso ao longo dos anos, Davi Veronese, Gabriel e Pedro. Futuramente, pretendo transformar as notas de aulas em um livro, que poderá ser utilizado pela comunidade brasileira de ciências sociais interessada em aprender mais sobre métodos quantitativos, em particular regressão com R. A motivação para disponibilizar as notas de aula em formato de livro é a quase inexistência de um material bom de econometria básica, em português, voltado para a área de ciências sociais, em particular a ciência política. Existem bons manuais em inglês para ciência política e bons manuais em português para economia, mas não em português para ciência política. Além disso, os livros existentes nem sempre fazem um bom trabalho de diferenciar regressão estatística e modelo estrutural (causal) de regressão. Uma abordagem moderna de inferência causal requer que essa distinção seja ensinada ao aluno e esse livro também pretende preencher essa lacuna. Por fim, a utilização do R é uma forma de dar ênfase à parte prática. Embora a teoria seja importante e a boa compreensão dos fundamentos é o que permite o aprofundamento dos temas cobertos no livro, estatística é uma disciplina aplicada, cujo maior valor está na sua aplicação prática. Assim, boa parte do curso e do conteúdo do livro é dedicado a implementar a teoria no R, bem como na interpretação dos dados. O livro começa com capítulos iniciais de introdução/revisão do R e de estatística básica (incluindo probabilidade). Assim, pressupomos que um estudante que utilize esse curso tenha conhecimentos de estatística e probabilidade básica. Não é necessário conhecimento prévio de R, nem de cálculo ou álgebra linerar. O restante do livro está organizado da seguinte maneira. Após as revisões, o texto introduz o conceito de esperança condicional, para motivar a regressão como aproximação da esperança condicional. Apresentamos então alguns estimadores (plug-in estimators e MQO), para então discutir causalidade, a partir da abordagem de resultados potenciais. Mostramos então estimação por Máxima Verossimilhança e checagem do modelo. Seguinos a ordem do livro do Cosma Shalizi (The Truth About Linear Regression), de discutir checagem antes de inferência, pois esta depende dos presssupostos serem válidos. Passamos então à inferência, regressão múltipla e por fim uma rápida introdução à regressão logística e outros modelos de variáveis resposta categórica. As referências principais para o presente livro são o já referido livro do Shalizi, além do manual consagrado do Hansen, Econometrics. O capítulo de resultados potenciais e intepretação da regressão como aproximação da CEF beberam extensamente do livro de Angrrst e Pschike, Mostly Harmless Econometris, e o capítulo sobre logística do Livro de Gelman, Hill e Vehtari, Regression and other Stories. Demais modelos categóricos seguem de perto o livro de Clark e Golder, Interaction Models. Specificatio and Interpretation. O leitor atento verá que várias derivações seguem fielmente o que se encontra nesses livros. Por fim, destaco a discussão sobre análise de sensibilidade, ausente de todos os manuais de econometria que olhei. Ainda um tanto confusa, creio que é tópico fundamental e da fronteira das análises causais e que acho uma boa contribuição (quando revisar o texto) para quem se interesse por análise causal e não encontra tutoriais sobre o tema disponível, nem mesmo na internet. E um alerta. Durante as aulas, notei vários erros de derivação, ausência de parêntesis (que mudavam completamente os resultados) entre muitos erros que não consigo reportar aqui. A maior parte já foi corrigida, mas ainda restam vários erros. Os alunos puderam anotar as passagens erradas, mas quem ler o texto pela internet sem acesso às aulas, não. Então, chequem em outras fontes todas as derivações. Espero poder corrigir os erros logo. Mas até lá, fica o alerta. "],["revisão-de-r.html", "Capítulo 2 - Revisão de R 2.1 R e Rstudio 2.2 R como calculadora 2.3 Objetos no R 2.4 Tipos de objetos 2.5 Armazenando dados 2.6 Bibliotecas/pacotes 2.7 importando dados 2.8 Data wrangling 2.9 Visualização", " Capítulo 2 - Revisão de R 2.1 R e Rstudio O R é uma linguagem de programação voltada para análise de dados. O Rstudio é uma IDE (interface de desenvolvimento), que nos ajuda a programar em R. No curso utilizaremos o Rstudio para facilitar programar em R. Normalmente, iremos escrever um comando aqui no Script, clicar em executar (run) ou apertar ctrl + enter, e o Rstudio vai copiar o comando, colar no console e executá-los para nós. 2.2 R como calculadora o R pode funcionar como calculadora. 2+2 [1] 4 3*4 [1] 12 10/2 [1] 5 2.3 Objetos no R Tudo no R é um objeto. Isso significa que um número é um objeto. pi [1] 3.141593 Isso significa que funções (comandos) também são objetos sum(c(1,2,3)) [1] 6 sum function (…, na.rm = FALSE) .Primitive(“sum”) E nós podemos criar nossos próprios objetos, dando os nomes que quisermos (exceto se já existe um objeto no R com aquele nome, como por exemplo o objeto “sum”). x &lt;- 3 y &lt;- 7 x+y [1] 10 2.4 Tipos de objetos O R tem muitos tipos de objetos. Vamos listar aqui apenas os mais básicos. 2.4.1 Numeric Objetos do tipo numeric são … números (“reais”). # Exemplos pi [1] 3.141593 1/3 [1] 0.3333333 4 [1] 4 2.4.2 character Objetos do tipo character são do tipo texto. Sempre são escritos entre aspas (simples ou duplas, tanto faz) # Exemplos &quot;Manoel Galdino&quot; [1] “Manoel Galdino” &#39;abc&#39; [1] “abc” &quot;7&quot; [1] “7” 2.5 Armazenando dados Para armazenar dados, usualmente teremos 4 tipos de objetos: 1. vetor, 2. Matriz. 3. data.frame, 4. lista. Não vou falar de lista agora (nem de array, que é uma generalização da matriz para mais de duas dimensões). 2.5.1 Vetor Um vetor é uma sequência de objetos. # Exemplos c(1,2,3) [1] 1 2 3 1:3 [1] 1 2 3 c(&quot;Manoel&quot;, &quot;Hugo&quot;, &quot;Lia&quot;, &quot;Juliana&quot;, &quot;Jéssica&quot;) [1] “Manoel” “Hugo” “Lia” “Juliana” “Jéssica” c(c(1,2,3), c(2,3,1)) [1] 1 2 3 2 3 1 os elementos de um vetor devem ser todos do mesmo tipo: # Exemplos x &lt;- c(&quot;1&quot;, 1) x[2] [1] “1” # sedundo elemento fica armazenado como character # não é possível somar texto # x[2] + x[2] # erro 2.5.2 Matriz Uma matriz são vetores organizados por coluna, todas as colunas (vetores) só podem ser de um tipo, ou seja, não posso ter uma coluna numeric e outra de character, por exemplo. library(knitr) library(kableExtra) # Exemplos mat &lt;- matrix(1:6, nrow=3, ncol=2) kable(mat) ## Warning in attr(x, &quot;align&quot;): &#39;xfun::attr()&#39; is deprecated. ## Use &#39;xfun::attr2()&#39; instead. ## See help(&quot;Deprecated&quot;) ## Warning in attr(x, &quot;format&quot;): &#39;xfun::attr()&#39; is deprecated. ## Use &#39;xfun::attr2()&#39; instead. ## See help(&quot;Deprecated&quot;) 1 4 2 5 3 6 2.5.3 Data Frame O data.frame é uma tabela/planilha, e é onde normalmente armazenamos nossos bancos de dados no R. # Exemplos df &lt;- data.frame(x=1:3, y=c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;)) kable(df) ## Warning in attr(x, &quot;align&quot;): &#39;xfun::attr()&#39; is deprecated. ## Use &#39;xfun::attr2()&#39; instead. ## See help(&quot;Deprecated&quot;) ## Warning in attr(x, &quot;format&quot;): &#39;xfun::attr()&#39; is deprecated. ## Use &#39;xfun::attr2()&#39; instead. ## See help(&quot;Deprecated&quot;) x y 1 a 2 b 3 c 2.5.3.1 datas ## explicando data rapidamente data_ex &lt;- &quot;2020-10-23&quot; data_ex1 &lt;- as.Date(data_ex) data_ex1 + 0:9 [1] “2020-10-23” “2020-10-24” “2020-10-25” “2020-10-26” “2020-10-27” [6] “2020-10-28” “2020-10-29” “2020-10-30” “2020-10-31” “2020-11-01” ## fim da explicação rápida de data ## criando data de forma repetitiva e tediosa. O que queremos evitar! minha_data &lt;- c(as.Date(&#39;2009-01-01&#39;), as.Date(&#39;2009-01-02&#39;), as.Date(&#39;2009-01-03&#39;), as.Date(&#39;2009-01-04&#39;), as.Date(&#39;2009-01-05&#39;), as.Date(&#39;2009-01-06&#39;), as.Date(&#39;2009-01-07&#39;), as.Date(&#39;2009-01-08&#39;), as.Date(&#39;2009-01-09&#39;), as.Date(&#39;2009-01-10&#39;)) acoes &lt;- data.frame( tempo = minha_data, X = rnorm(10, 0, 1), Y = rnorm(10, 0, 2), Z = rnorm(10, 0, 4) ) ## Warning in attr(x, &quot;align&quot;): &#39;xfun::attr()&#39; is deprecated. ## Use &#39;xfun::attr2()&#39; instead. ## See help(&quot;Deprecated&quot;) ## Warning in attr(x, &quot;format&quot;): &#39;xfun::attr()&#39; is deprecated. ## Use &#39;xfun::attr2()&#39; instead. ## See help(&quot;Deprecated&quot;) tempo X Y Z 2009-01-01 -0.4000496 0.5978521 -4.6564520 2009-01-02 -0.4894875 -1.3450869 -1.8553446 2009-01-03 0.4235218 -1.1430639 2.1885962 2009-01-04 1.1192385 -5.1809918 3.7828378 2009-01-05 -0.4978148 2.0540545 1.6848339 2009-01-06 0.1729700 1.7077182 1.8734220 2009-01-07 1.4232332 0.5796282 -3.4753243 2009-01-08 -0.6877207 -0.0609202 -0.3353897 2009-01-09 1.4909664 3.0133114 -6.3818028 2009-01-10 -0.6771148 4.3874982 -0.3771961 # criando data.frame de maneira mais inteligente acoes &lt;- data.frame( tempo = as.Date(&#39;2009-01-01&#39;) + 0:9, X = rnorm(10, 0, 1), Y = rnorm(10, 0, 2), Z = rnorm(10, 0, 4) ) ## Warning in attr(x, &quot;align&quot;): &#39;xfun::attr()&#39; is deprecated. ## Use &#39;xfun::attr2()&#39; instead. ## See help(&quot;Deprecated&quot;) ## Warning in attr(x, &quot;format&quot;): &#39;xfun::attr()&#39; is deprecated. ## Use &#39;xfun::attr2()&#39; instead. ## See help(&quot;Deprecated&quot;) tempo X Y Z 2009-01-01 0.6202357 -2.4766558 -1.4175991 2009-01-02 0.4187787 -1.3235020 -4.0950106 2009-01-03 0.4364590 -3.3995125 3.2809764 2009-01-04 -0.6584942 0.4000266 -1.4681761 2009-01-05 0.0398953 -0.5440511 1.2939776 2009-01-06 -0.4254140 -1.4855994 4.1235297 2009-01-07 1.7265501 2.4139718 -0.6979418 2009-01-08 -1.2366452 -2.8319102 3.8638425 2009-01-09 0.1744760 -2.6099071 -2.8976411 2009-01-10 -0.0282893 -1.1921080 -4.3861945 2.6 Bibliotecas/pacotes O R permite que a gente importe comandos que não vêm por padrão no R. Em gerla esses comandos es~toa agrupados sob um pacote. PAra usar esses comandos, primeiro a gente instala o pacote, e depois carrega a biblioteca. # Exemplos #install.packages(&quot;data.table&quot;) #library(data.table) 2.7 importando dados Para importar dados, vamos usar a bilioteca “data.table” Então, instalem ela se ainda não instalaram (usando o comando install.packages(“data.table”)) E depois carreguem a biblioteca: library(data.table) Para importar, usaremos o comando fread do pacote data.table. # vamos importar uma base de dados de pib municipais de 2013, do IBGE # o arquivo está em formato RDS, que é um formato do R, e disponível no meu github. Para importá-lo direto no R, vamos ler o arquivo com a função url e depois import´-lo com a função readRDS. pib_cid &lt;- readRDS(url(&quot;https://github.com/mgaldino/book-regression/raw/main/dados/pib_cid.RDS&quot;)) # para visualizar os dados que forma importados, temos várias funções # glimpse, head e View library(dplyr) # para glimpse # glimpse(pib_cid) # head(pib_cid) # View(pib_cid) 2.8 Data wrangling Para manipulação, limpeza e processamento de dados, iremos utilizar o chamado “tidyverse”. library(tidyverse) # digamos que quero o pib total médio e o pib per capita médio # basta usar o comando summarise, que resume os dados e escolher a função mean. df &lt;- pib_cid %&gt;% summarise(pib_medio = mean(pib_total), pib_per_capita_medio = mean(pib_per_capita)) kable(df) ## Warning in attr(x, &quot;align&quot;): &#39;xfun::attr()&#39; is deprecated. ## Use &#39;xfun::attr2()&#39; instead. ## See help(&quot;Deprecated&quot;) ## Warning in attr(x, &quot;format&quot;): &#39;xfun::attr()&#39; is deprecated. ## Use &#39;xfun::attr2()&#39; instead. ## See help(&quot;Deprecated&quot;) pib_medio pib_per_capita_medio 957202.7 17388.86 # se eu quiser a soma dos pibs municipais df &lt;- pib_cid %&gt;% summarise(soma_pib = sum(pib_total)) %&gt;% head() kable(df) ## Warning in attr(x, &quot;align&quot;): &#39;xfun::attr()&#39; is deprecated. ## Use &#39;xfun::attr2()&#39; instead. ## See help(&quot;Deprecated&quot;) ## Warning in attr(x, &quot;format&quot;): &#39;xfun::attr()&#39; is deprecated. ## Use &#39;xfun::attr2()&#39; instead. ## See help(&quot;Deprecated&quot;) soma_pib 5331618957 # maior e menos pibs e pibs per capita entre municípios df &lt;- pib_cid %&gt;% summarise(pib_max = max(pib_total), pib_min = min(pib_total), pib_per_capita_max = max(pib_per_capita), pib_per_capita_min = min(pib_per_capita)) %&gt;% head() kable(df) ## Warning in attr(x, &quot;align&quot;): &#39;xfun::attr()&#39; is deprecated. ## Use &#39;xfun::attr2()&#39; instead. ## See help(&quot;Deprecated&quot;) ## Warning in attr(x, &quot;format&quot;): &#39;xfun::attr()&#39; is deprecated. ## Use &#39;xfun::attr2()&#39; instead. ## See help(&quot;Deprecated&quot;) pib_max pib_min pib_per_capita_max pib_per_capita_min 582079726 4198.94 717343.7 301.6 # se eu quiser apenas dos municípios od estado de SP? # basta filtrar pelo estado de SP, com o comando filter df &lt;- pib_cid %&gt;% filter(sigla_uf == &quot;SP&quot;) %&gt;% summarise(soma_pib = sum(pib_total)) %&gt;% head() kable(df) ## Warning in attr(x, &quot;align&quot;): &#39;xfun::attr()&#39; is deprecated. ## Use &#39;xfun::attr2()&#39; instead. ## See help(&quot;Deprecated&quot;) ## Warning in attr(x, &quot;format&quot;): &#39;xfun::attr()&#39; is deprecated. ## Use &#39;xfun::attr2()&#39; instead. ## See help(&quot;Deprecated&quot;) soma_pib 1715238417 df &lt;- pib_cid %&gt;% filter(sigla_uf == &quot;SP&quot;) %&gt;% summarise(pib_medio = mean(pib_total), pib_per_capita_medio = mean(pib_per_capita)) %&gt;% head() kable(df) ## Warning in attr(x, &quot;align&quot;): &#39;xfun::attr()&#39; is deprecated. ## Use &#39;xfun::attr2()&#39; instead. ## See help(&quot;Deprecated&quot;) ## Warning in attr(x, &quot;format&quot;): &#39;xfun::attr()&#39; is deprecated. ## Use &#39;xfun::attr2()&#39; instead. ## See help(&quot;Deprecated&quot;) pib_medio pib_per_capita_medio 2659284 24827.14 # se eu quiser esse cálculo por uf (or cada umas das ufs?) # Aí é melhor aguprar por uf # ideia é: split by, apply (function), combine (summarise?) df &lt;- pib_cid %&gt;% group_by(sigla_uf) %&gt;% summarise(pib_medio = mean(pib_total), pib_per_capita_medio = mean(pib_per_capita)) %&gt;% head() kable(df) ## Warning in attr(x, &quot;align&quot;): &#39;xfun::attr()&#39; is deprecated. ## Use &#39;xfun::attr2()&#39; instead. ## See help(&quot;Deprecated&quot;) ## Warning in attr(x, &quot;format&quot;): &#39;xfun::attr()&#39; is deprecated. ## Use &#39;xfun::attr2()&#39; instead. ## See help(&quot;Deprecated&quot;) sigla_uf pib_medio pib_per_capita_medio AC 521542.3 11450.979 AL 365515.0 7931.148 AM 1339536.0 8975.314 AP 797717.9 14947.829 BA 491233.3 8814.841 CE 592590.0 7157.360 # agora, quero criar uma nova variável, que é o pib estadual df &lt;- pib_cid %&gt;% group_by(sigla_uf) %&gt;% mutate(pib_uf = sum(pib_total)) df &lt;- head(df) kable(df) ## Warning in attr(x, &quot;align&quot;): &#39;xfun::attr()&#39; is deprecated. ## Use &#39;xfun::attr2()&#39; instead. ## See help(&quot;Deprecated&quot;) ## Warning in attr(x, &quot;format&quot;): &#39;xfun::attr()&#39; is deprecated. ## Use &#39;xfun::attr2()&#39; instead. ## See help(&quot;Deprecated&quot;) ano codigo_regiao nome_regiao codigo_uf sigla_uf nome_uf cod_municipio nome_munic nome_metro codigo_meso nome_meso codigo_micro nome_micro codigo_reg_geo_imediata nome_reg_geo_imediata mun_reg_geo_imediata codigo_reg_geo_intermediaria nome_reg_geo_intermediaria mun_reg_geo_intermediaria codigo_concentracao_urbana nome_concentracao_urbana tipo_concentracao_urbana codigo_arranjo_populacional nome_arranjo_populacional hierarquia_urbana hierarquia_urbana_principais codigo_regiao_rural nome_regiao_rural regiao_rural_classificacao amazonia_legal semiarido cidade_de_sao_paulo vab_agropecuaria vab_industria vab_servicos_exclusivo vab_adm_publica vab_total impostos pib_total pib_per_capita atividade_vab1 atividade_vab2 atividade_vab3 pib_uf 2013 1 Norte 11 RO Rondônia 1100015 Alta Floresta D’Oeste NA 1102 Leste Rondoniense 11006 Cacoal 110005 Cacoal do Entorno 1102 Ji-Paraná do Entorno NA NA NA NA NA Centro Local Centro Local 1101 Região Rural da Capital Regional de Porto Velho Região Rural de Capital Regional Sim Não Não 110850.84 20336.994 73025.07 120335.59 324548.50 16776.193 341324.69 13266.66 Administração, defesa, educação e saúde públicas e seguridade social Pecuária, inclusive apoio à pecuária Demais serviços 31121413 2013 1 Norte 11 RO Rondônia 1100023 Ariquemes NA 1102 Leste Rondoniense 11003 Ariquemes 110002 Ariquemes Polo 1101 Porto Velho do Entorno NA NA NA NA NA Centro Sub-regional B Centro Sub-regional 1101 Região Rural da Capital Regional de Porto Velho Região Rural de Capital Regional Sim Não Não 93249.75 354733.212 694832.17 466732.80 1609547.93 190304.571 1799852.51 17772.99 Administração, defesa, educação e saúde públicas e seguridade social Demais serviços Comércio e reparação de veículos automotores e motocicletas 31121413 2013 1 Norte 11 RO Rondônia 1100031 Cabixi NA 1102 Leste Rondoniense 11008 Colorado do Oeste 110006 Vilhena do Entorno 1102 Ji-Paraná do Entorno NA NA NA NA NA Centro Local Centro Local 1101 Região Rural da Capital Regional de Porto Velho Região Rural de Capital Regional Sim Não Não 38259.43 3412.205 17787.45 32838.64 92297.72 4066.819 96364.54 14836.73 Administração, defesa, educação e saúde públicas e seguridade social Pecuária, inclusive apoio à pecuária Agricultura, inclusive apoio à agricultura e a pós colheita 31121413 2013 1 Norte 11 RO Rondônia 1100049 Cacoal NA 1102 Leste Rondoniense 11006 Cacoal 110005 Cacoal Polo 1102 Ji-Paraná do Entorno NA NA NA NA NA Centro Sub-regional B Centro Sub-regional 5105 Região Rural do Centro Sub-regional de Vilhena (RO) e Cacoal (RO) Região Rural de Centro Sub-regional Sim Não Não 140658.88 140288.317 599519.83 395842.23 1276309.26 156944.241 1433253.51 16692.33 Administração, defesa, educação e saúde públicas e seguridade social Demais serviços Comércio e reparação de veículos automotores e motocicletas 31121413 2013 1 Norte 11 RO Rondônia 1100056 Cerejeiras NA 1102 Leste Rondoniense 11008 Colorado do Oeste 110006 Vilhena do Entorno 1102 Ji-Paraná do Entorno NA NA NA NA NA Centro de Zona B Centro de Zona 1101 Região Rural da Capital Regional de Porto Velho Região Rural de Capital Regional Sim Não Não 45153.64 19889.818 149569.44 83089.55 297702.45 55567.232 353269.68 19581.49 Administração, defesa, educação e saúde públicas e seguridade social Comércio e reparação de veículos automotores e motocicletas Demais serviços 31121413 2013 1 Norte 11 RO Rondônia 1100064 Colorado do Oeste NA 1102 Leste Rondoniense 11008 Colorado do Oeste 110006 Vilhena do Entorno 1102 Ji-Paraná do Entorno NA NA NA NA NA Centro Local Centro Local 1101 Região Rural da Capital Regional de Porto Velho Região Rural de Capital Regional Sim Não Não 51029.59 23179.131 66099.41 86090.50 226398.63 16368.611 242767.24 12650.72 Administração, defesa, educação e saúde públicas e seguridade social Demais serviços Pecuária, inclusive apoio à pecuária 31121413 Coisas estranhas. O maior pib per capita municipal deu muito alto. vamos ver qual município é? Vamos filtrar e depois selecionar apenas algumas colunas # agora, quero criar uma nova variável, que é o pib estadual df &lt;- pib_cid %&gt;% filter(pib_per_capita &gt; 700000) %&gt;% dplyr::select(sigla_uf, nome_munic, pib_per_capita, pib_total) %&gt;% head() kable(df) ## Warning in attr(x, &quot;align&quot;): &#39;xfun::attr()&#39; is deprecated. ## Use &#39;xfun::attr2()&#39; instead. ## See help(&quot;Deprecated&quot;) ## Warning in attr(x, &quot;format&quot;): &#39;xfun::attr()&#39; is deprecated. ## Use &#39;xfun::attr2()&#39; instead. ## See help(&quot;Deprecated&quot;) sigla_uf nome_munic pib_per_capita pib_total ES Presidente Kennedy 717343.7 7984035 Vamos entrar na Wiki do município ou perguntar pra chatGPT o que explica isso aí? Veremos que “faz sentido”, embora na verdade não faça. Exercício em sala de aula: veja os impostos desse município. 2.9 Visualização Para visualizarmos os dados com gráficos, utilizaremos a biblioteca ggplot2 # gráficos library(ggplot2) pib_cid %&gt;% ggplot(aes(y=pib_total, x=impostos)) + geom_point() A lgócia geral de um grtáfico com ggplot2 é como no exemplo acima. Primeiro passamos as variáveis por meio do comando ggplot, dentro de aes (de aesthetics), depois combinamos com o tipo de plot que queremos faze,r nesse caso, pontos, com geom_point. É possívle customizar o gráfico para ele ficar mais bonito. Vamos fazer isso agora. # gráficos mais bonitos pib_cid %&gt;% ggplot(aes(y=pib_total, x=impostos)) + geom_point() + scale_y_continuous(labels = scales::dollar) + theme_light() + theme(text=element_text(size=20)) + xlab(&quot;impostos municipais&quot;) + ggtitle(&quot;PIB municipal de 2013 x impostos municipais&quot;) Podemos usar vários temas feitos pela comunidade. Por exemplo, Barbie: # gráficos mais bonitos # install.packages(&quot;remotes&quot;) #remotes::install_github(&quot;MatthewBJane/theme_park&quot;) library(ThemePark) pib_cid %&gt;% ggplot(aes(y=pib_total, x=impostos)) + geom_point() + scale_y_continuous(labels = scales::dollar) + theme(text=element_text(size=20)) + theme_barbie() + xlab(&quot;impostos municipais&quot;) + ggtitle(&quot;PIB municipal de 2013 x impostos municipais&quot;) Vocês podem ver outros temas de filmes no github do autor do pacote: https://github.com/MatthewBJane/theme_park E, claro, há muito mais na internet. Para fazer outro tipo de gráfico, é só variar o geom. Por exemplo, um histograma do PIB per capita. # Histograma pib_cid %&gt;% ggplot(aes(x=pib_per_capita)) + geom_histogram() + theme_light() + theme(text=element_text(size=20)) + ggtitle(&quot;PIB per capita municipal&quot;) "],["revisão-de-estatística-e-probabilidade.html", "Capítulo 3 - Revisão de estatística e probabilidade 3.1 Variável Aleatória 3.2 Esperança matemática 3.3 Variância 3.4 Álgebra com Esperança, Variância e Covariância 3.5 Prova da identidade da variância 3.6 Distribuição de Probabilidade Conjunta 3.7 Probabilidade Condicional 3.8 Esperança Condicional 3.9 Projeção Linear", " Capítulo 3 - Revisão de estatística e probabilidade A média de um conjunto de valores é dada pela soma dos valores, dividido pelo número de observações. Matematicamente: \\(media = \\sum_{i=1}^n{x_i}/n\\) para observações \\(\\{ x_1, x_2, x_3, ..., x_n \\}\\) Em geral, as observações são uma amostra, e falamos de média amostral, \\(\\overline x\\). Ou seja: \\(\\overline x = \\sum_{i=1}^n{x_i}/n\\) Exercício 1. Vamos calcular, no R, a média das seguintes amostras: \\(\\{1,2,3,4,5,6,7,8,9,10\\}\\) \\(\\{5,5,5,5,5,5,5\\}\\) \\(\\{1,3,5,7,9,11\\}\\) \\(\\{-5,-4,-3,-2,-1,1,2,3,4,5\\}\\) Código no R x &lt;- c(1,2,3,4,5,6,7,8,9,10) (media_x &lt;- sum(x)/length(x)) ## [1] 5.5 x &lt;- c(5,5,5,5,5,5,5) (media_x &lt;- sum(x)/length(x)) ## [1] 5 x &lt;- c(1,3,5,7,9,11) (media_x &lt;- sum(x)/length(x)) ## [1] 6 x &lt;- c(-5,-4,-3,-2,-1,1,2,3,4,5) (media_x &lt;- sum(x)/length(x)) ## [1] 0 # ou podemos simplemsnte usar mean(x) mean(x) ## [1] 0 3.1 Variável Aleatória Uma variável aleatória (v.a.) mede numericamente resultados de eventos aleatórios. Por exemplo, um dado de \\(6\\) faces possui um espaço amostral de eventos possíveis dados pelos números \\(\\{1,2,3,4,5,6\\}\\). Ao atribuirmos uma probabilidade a cada resultado possível do espaço amostral, por exemplo \\(1/6\\), temos uma distribuição de probabilidade. Variáveis aleatórias podem ser discretas ou contínuas. Uma v.a. discreta pode assumir um número finito (contável) de valores. Já uma contínua pode assumir infinitos (não-contáveis) valores. Um conjunto é contável se ele for finito ou se puder ser estabelecida uma correspondência um para um com o conjunto (infinito) dos números naturais. 3.2 Esperança matemática A esperança de uma variável aleatória discreta \\(X\\), cuja probabilidade de massa de \\(x \\in X\\) é dada por \\(p(x)\\), é definida por: \\(\\sum(x*p(x))\\). A esperança de uma v.a. contínua X, cuja densidade é \\(f(x)\\), é definida por: \\(\\int f(x)*x\\,dx\\). Similarmente, para a mesma v.a. X acima, a esperança de uma função \\(h(X)\\) é dada por \\(\\int f(x)*h(x)\\,dx\\) e analogamente para o caso discreto. 3.3 Variância A variância de uma variável aleatória \\(X\\) é dada por: Definição 1. \\(Var(X) = \\mathbb{E}[(X - \\mathbb{E}[X])^2]\\). A Covariância de duas v.a. \\(X\\) e \\(Y\\) é definida como: \\(Cov(X,Y) = \\mathbb{E}[(X - \\mathbb{E}[X])*(Y - \\mathbb{E}[Y])]\\). Notem que \\(Cov(X,X) = Var(X)\\). A covariância é positiva quando ambos X e Y tendem a ter valores acima (ou abaixo) de sua média simultaneamente, enquanto ela é negativa quando uma v.a. tende a ter valores acima da sua média e a outra abaixo. 3.4 Álgebra com Esperança, Variância e Covariância Sejam \\(a\\) e \\(b\\) constantes. Linearidade da Esperança \\(\\mathbb{E}[aX + bY] =\\mathbb{E}[aX] + \\mathbb{E}[by] = a*\\mathbb{E}[X] + b*\\mathbb{E}[Y]\\) Exercício: verifique, com exemplos, que isso é verdade. Identidade da variância \\(Var(X) = \\mathbb{E}[(X - \\mathbb{E}[X])^2] = \\mathbb{E}[X^2] - \\mathbb{E}^2[X]\\) A prova será demonstrada mais adiante. Identidade da Covariância \\(Cov(X,Y) = \\mathbb{E}[X*Y] - \\mathbb{E}[X]*\\mathbb{E}[Y] = \\mathbb{E}[(X - \\mathbb{E}[X])*(Y - \\mathbb{E}[Y])]\\) Exercício para o leitor. Prove que isso é verdade. 4, Covariância é simétrica \\(Cov(X,Y) = Cov(Y,X)\\) Variância não é linear \\(Var(a*X + b) = a^2*Var(x)\\) Covariância não é linear \\(Cov(a*X + b,Y) = a*Cov(Y,X)\\) 3.5 Prova da identidade da variância Vamos mostrar que \\(\\mathbb{E}[(X - \\mathbb{E}[X])^2] = \\mathbb{E}[X^2] - \\mathbb{E}^2[X]\\) Começamos expandido o quadrado da esperança: \\(Var(X) = \\mathbb{E}[(X - \\mathbb{E}[X])^2] = \\mathbb{E}[(X - \\mathbb{E}[X]) * (X - \\mathbb{E}[X])]\\). Aplicando a regra do quadrado, temos: \\(\\mathbb{E}[(X - \\mathbb{E}[X]) * (X - \\mathbb{E}[X])] = \\mathbb{E}[(X^2 - 2* \\mathbb{E}[X]*X + \\mathbb{E}[X]^2)]\\) Pela propriedade da experança, sabemos que, sejam \\(A\\) e \\(B\\) duas v.a. independentes, então \\(\\mathbb{E}[A + B] = \\mathbb{E}[A] + \\mathbb{E}[B]\\). Então: \\(Var(X) = \\mathbb{E}[X^2] - \\mathbb{E}[2*\\mathbb{E}[X]*X] + \\mathbb{E}[\\mathbb{E}[X]^2]]\\) Outra propriedade da esperança é que, seja \\(a\\) uma constante e \\(X\\) uma v.a., então \\(\\mathbb{E}[a*X] = a*\\mathbb{E}[X]\\). \\(Var(X) = \\mathbb{E}[X^2] - 2*\\mathbb{E}[\\mathbb{E}[X]*X] + \\mathbb{E}[\\mathbb{E}[X]^2]]\\) Nós sabemos que \\(\\mathbb{E}[X]\\) é uma constante (é uma média da v.a.). E a média de uma constante é a própria constante. Portanto, \\(\\mathbb{E}[\\mathbb{E}[X]] = \\mathbb{E}[X]\\). E usaremos também que \\(\\mathbb{E}[a*X] = a*\\mathbb{E}[X]\\) e, por fim, o fato de que uma constante ao quadrado é em si uma constante. \\(Var(X) = \\mathbb{E}[X^2] - 2*\\mathbb{E}[X] * \\mathbb{E}[\\mathbb{E}[X]] + \\mathbb{E}[X]^2\\) \\(Var(X) = \\mathbb{E}[X^2] - 2*\\mathbb{E}[X] * \\mathbb{E}[X] + \\mathbb{E}[X]^2\\) \\(Var(X) = \\mathbb{E}[X^2] - 2*\\mathbb{E}[X]^2 + \\mathbb{E}[X]^2\\) \\(Var(X) = \\mathbb{E}[X^2] - \\mathbb{E}[X]^2\\) Como Queriamos Demonstrar (CQD). 3.6 Distribuição de Probabilidade Conjunta A Distribuição de probabilidade conjunta de \\(X\\) e \\(Y\\) (definida no mesmo espaço de probabilidade) é uma distribuição de probabilidade dos pares \\((x,y)\\) e descreve como os valores de \\(X\\) e \\(Y\\) variam conjuntamente. Cada uma das distribuições \\(X\\) e \\(Y\\) sozinhas são chamadas de distribuições marginais. Uma distribuição conjunta é como uma máquina que, de acordo com certas regras de probabilidade, retorna dois pares de valores. ex. 1. Roleta. Em um casino, um jogo comum é a roleta. Ela consiste normalmente de 32 números (0 a 31), e cada número tem uma cor (preto, vermelho ou verde). Ao girar a roleta, ela solta um número e uma cor. Portanto, podemos pensar que a roleta é uma distribuição conjunta de duas variáveis (números e cores). Ex. 2.: Considere um dado de 4 faces \\(( 1, 2, 3, 4 )\\). Seja \\(X\\) a soma dos números dos dois dados, e \\(Y\\) o maior valor dos dois dados. O espaço amostral é dado pela tabela abaixo. library(knitr) library(dplyr) library(kableExtra) #Definir o espaço amostral espaco_amostral &lt;- expand.grid(1:4, 1:4) espaco_amostral$X &lt;- espaco_amostral$Var1 + espaco_amostral$Var2 espaco_amostral$Y &lt;- pmax(espaco_amostral$Var1, espaco_amostral$Var2) # Criar a tabela kable(espaco_amostral, col.names = c(&quot;resultado do primeiro dado&quot;, &quot;resultado do segundo dado&quot;, &quot;X&quot;, &quot;Y&quot;), caption = &quot;Tabela representando a soma (X) and o maior valor (Y) do lançamento de dois dados de quatro lados&quot;) %&gt;% kable_styling(bootstrap_options = &quot;striped&quot;) ## Warning in attr(x, &quot;align&quot;): &#39;xfun::attr()&#39; is deprecated. ## Use &#39;xfun::attr2()&#39; instead. ## See help(&quot;Deprecated&quot;) ## Warning in attr(x, &quot;align&quot;): &#39;xfun::attr()&#39; is deprecated. ## Use &#39;xfun::attr2()&#39; instead. ## See help(&quot;Deprecated&quot;) (#tab:cap3 tab1)(#tab:cap3 tab1)Tabela representando a soma (X) and o maior valor (Y) do lançamento de dois dados de quatro lados resultado do primeiro dado resultado do segundo dado X Y 1 1 2 1 2 1 3 2 3 1 4 3 4 1 5 4 1 2 3 2 2 2 4 2 3 2 5 3 4 2 6 4 1 3 4 3 2 3 5 3 3 3 6 3 4 3 7 4 1 4 5 4 2 4 6 4 3 4 7 4 4 4 8 4 Se supusermos que todos os números possuem a mesma chance de sair quando jogamos os dados, então, a distribuição conjunta de \\(X\\) e \\(Y\\) pode ser dada por: # Definir os valores de (x, y) e P(X = x, Y = y) valores &lt;- c(&quot;(2, 1)&quot;, &quot;(3, 2)&quot;, &quot;(4, 2)&quot;, &quot;(4, 3)&quot;, &quot;(5, 3)&quot;, &quot;(5, 4)&quot;, &quot;(6, 3)&quot;, &quot;(6, 4)&quot;, &quot;(7, 4)&quot;, &quot;(8, 4)&quot;) probabilidades &lt;- c(0.0625, 0.1250, 0.0625, 0.1250, 0.1250, 0.1250, 0.0625, 0.1250, 0.1250, 0.0625) # Criar a tabela tabela &lt;- data.frame(&quot;(x, y)&quot; = valores, &quot;P(X = x, Y = y)&quot; = probabilidades) # Formatar a tabela kable(tabela, caption = &quot;Tabela representando a distribuição conjunta da soma (X) e o maior (Y) de dois lançamentos de um dado de quatro faces&quot;, col.names = c(&quot;$(X,Y)$&quot;, &quot;$P(X=x, Y=y)$&quot;)) %&gt;% kable_styling(bootstrap_options = &quot;striped&quot;) ## Warning in attr(x, &quot;align&quot;): &#39;xfun::attr()&#39; is deprecated. ## Use &#39;xfun::attr2()&#39; instead. ## See help(&quot;Deprecated&quot;) ## Warning in attr(x, &quot;align&quot;): &#39;xfun::attr()&#39; is deprecated. ## Use &#39;xfun::attr2()&#39; instead. ## See help(&quot;Deprecated&quot;) (#tab:cap3 dado)(#tab:cap3 dado)Tabela representando a distribuição conjunta da soma (X) e o maior (Y) de dois lançamentos de um dado de quatro faces \\((X,Y)\\) \\(P(X=x, Y=y)\\) (2, 1) 0.0625 (3, 2) 0.1250 (4, 2) 0.0625 (4, 3) 0.1250 (5, 3) 0.1250 (5, 4) 0.1250 (6, 3) 0.0625 (6, 4) 0.1250 (7, 4) 0.1250 (8, 4) 0.0625 3.7 Probabilidade Condicional Nós vimos o que é uma distribuição de probabilidade conjunta. É comum que nós tenhamos apenas uma observação parcial sobre uma das variáveis. Digamos, que \\(y = 2\\). De posse dessa informação, como podemos atualizar nossa tabela de probabilidades? Se o maior valor foi \\(2\\), então a soma dos dados \\(X\\) só pode ser 3 ou 4. Se a soma for 3, temos algo como \\((1,2)\\) ou \\((2,1)\\). Se a soma for 4, então deve ter saído \\((2,2)\\). Esse é o novo espaço amostral dado que \\(y=2\\).Todos os outros números não podem ocorrer e, portanto, possuem probabilidade zero. Logo, \\(P(X=1|Y=2) = 2/3\\) e \\(P(X=2|Y=2) = 1/3\\). Isso é o que chamamos de probabilidade condicional. No caso, a probabilidade condicional de \\(X\\), dado \\(Y=y\\). A probabilidade condicional é em si uma distribuição de probabilidade. Do mesmo jeito que temos as distribuições de probabilidade de \\(X\\), de \\(Y\\) e de \\(X,Y\\), também temos a de \\(X|y=y\\) (e, claro, a de \\(Y|X=x\\)). A tabela abaixo apresenta essa distribuição de probabilidade condicional para o caso de \\(Y=2\\). # Definir os valores de (x, y) e P(X = x, Y = y) valores &lt;- c(&quot;(2, 1)&quot;, &quot;(3, 2)&quot;, &quot;(4, 2)&quot;, &quot;(4, 3)&quot;, &quot;(5, 3)&quot;, &quot;(5, 4)&quot;, &quot;(6, 3)&quot;, &quot;(6, 4)&quot;, &quot;(7, 4)&quot;, &quot;(8, 4)&quot;) probabilidades &lt;- c(0, 2/3, 1/3, 0, 0, 0, 0, 0, 0, 0) # Criar a tabela tabela &lt;- data.frame(&quot;(x, y)&quot; = valores, &quot;P(X = x| Y = 2)&quot; = probabilidades) # Formatar a tabela kable(tabela, caption = &quot;Tabela representando a distribuição conjunta da soma (X) e o maior (Y) de dois lançamentos de um dado de quatro faces&quot;, col.names = c(&quot;$(X,Y)$&quot;, &quot;$P(X=x| Y=2)$&quot;)) %&gt;% kable_styling(bootstrap_options = &quot;striped&quot;) ## Warning in attr(x, &quot;align&quot;): &#39;xfun::attr()&#39; is deprecated. ## Use &#39;xfun::attr2()&#39; instead. ## See help(&quot;Deprecated&quot;) ## Warning in attr(x, &quot;align&quot;): &#39;xfun::attr()&#39; is deprecated. ## Use &#39;xfun::attr2()&#39; instead. ## See help(&quot;Deprecated&quot;) (#tab:cap3 prob-cond)(#tab:cap3 prob-cond)Tabela representando a distribuição conjunta da soma (X) e o maior (Y) de dois lançamentos de um dado de quatro faces \\((X,Y)\\) \\(P(X=x&amp;amp;#124; Y=2)\\) (2, 1) 0.0000000 (3, 2) 0.6666667 (4, 2) 0.3333333 (4, 3) 0.0000000 (5, 3) 0.0000000 (5, 4) 0.0000000 (6, 3) 0.0000000 (6, 4) 0.0000000 (7, 4) 0.0000000 (8, 4) 0.0000000 3.8 Esperança Condicional Se a probabilidade condicional é uma distribuição de probabilidade no seu próprio direito, então podemos calcular a esperança dessa distribuição, exatamente como fazíamos antes. No caso, falamos de esperança condicional. Qual o valor médio de \\(X\\), quando \\(Y=2\\)? \\(3*(2/3) + 4*(1/3)\\) \\(2 + 4/3 = 6/3 + 4/3 = 10/3 = 3.333\\) A notação matemática para a esperança condicional, nesse caso, é: \\(E[X|Y=2]\\). 3.9 Projeção Linear A projeção linear é uma das formas de interpretar o que a regressão linear está fazendo. Então, vamos entender o que é a projeção linear e como ela se conectar com regressão. Antes disso, vamos garantir que temos uma intepretação geométrica da soma e subtração de vetores, para dpois entender a projeção linear. 3.9.1 Adição de vetores Considere os vetores \\(\\vec{u} = (2,1)\\) e \\(\\vec{v} = (1,2)\\). Sabemos algebricamente que a soma de \\(\\vec{u} + \\vec{v} = (2+1, 1+2) = (3,3)\\). Vejamos isso graficamente. Agora, passemos à subtração. Primeiro, veja que \\(-\\vec{v}\\) é o vetor \\(\\vec{v}\\) em que mudamos o sentido. Algebricamente, sabemos que \\(\\vec{u} + \\vec{v} = (2-1, 1-2) = (1,-1)\\). Graficamente, fica como abaixo. 3.9.2 Projeção linear y &lt;- c(2, 2.5) # projeção de y em u proj_coeff &lt;- sum(y * u) / sum(u * u) proj_y_on_u &lt;- proj_coeff * u r &lt;- y - proj_y_on_u # vetores unitários para marcar ângulo reto u_hat &lt;- u / sqrt(sum(u^2)) n_hat &lt;- c(-u_hat[2], u_hat[1]) # perpendicular a u_hat (rotação 90°) # cantoneira (marcador de ângulo reto) no ponto da projeção A &lt;- proj_y_on_u eps &lt;- 0.25 B &lt;- A + eps * u_hat C &lt;- A + eps * n_hat p3 &lt;- base_plot(list(u, y, proj_y_on_u)) + seg(u, col = &quot;gray40&quot;) + seg(y, col = &quot;#1f77b4&quot;) + seg(proj_y_on_u, col = &quot;#2ca02c&quot;) + # resíduo r: da projeção até y geom_segment(aes(x = proj_y_on_u[1], y = proj_y_on_u[2], xend = y[1], yend = y[2]), arrow = arrow_style, linewidth = 1, color = &quot;#d62728&quot;) + # marcador de ângulo reto em proj_u(y) geom_segment(aes(x = A[1], y = A[2], xend = B[1], yend = B[2])) + geom_segment(aes(x = A[1], y = A[2], xend = C[1], yend = C[2])) + annotate(&quot;text&quot;, x = y[1] + 0.2, y = y[2], label = &quot;y&quot;) + annotate(&quot;text&quot;, x = u[1] + 0.2, y = u[2], label = &quot;u&quot;) + annotate(&quot;text&quot;, x = proj_y_on_u[1] + 0.2, y = proj_y_on_u[2], label = TeX(&quot;$\\\\proj_u (y)$&quot;)) + annotate(&quot;text&quot;, x = (y[1] + proj_y_on_u[1]) / 2, y = (y[2] + proj_y_on_u[2]) / 2 + 0.25, label = TeX(&quot;$r\\\\,=\\\\,y- \\\\proj_u (y)\\\\;\\\\perp\\\\;u$&quot;)) + labs(title = &quot;Projeção de y em u e resíduo perpendicular&quot;) p3 A fórmula da projeção é: \\(proj_u (y) = \\frac{y \\cdot u}{u \\cdot u} u\\). "],["introdução-à-simulação.html", "Capítulo 4 Introdução à Simulação 4.1 Objetivos 4.2 Introdução 4.3 DGP — Processo Gerador de Dados 4.4 Simulação de Monte Carlo 4.5 Configuração 4.6 Simular 4.7 Resumir 4.8 LGN e TCL 4.9 Cobertura de um IC", " Capítulo 4 Introdução à Simulação 4.1 Objetivos Entender o que é um Processo Gerador de Dados (DGP) e como especificá-lo em R. Gerar amstras aleatórias de distribuições (rnorm, runif, rbinom, rpois, …) e fixar semente com set.seed(). Compreender simulação de Monte Carlo: por que, quando e como. Realizar aplicações de simulação, como diferenciar LLN (Lei dos Grandes Números) e CLT (Teorema Central do Limite) via simulação; e entender cobertura de intervalos de confiança (IC) e avaliar coberturas para níveis 80/90/95/99% com gráficos. 4.2 Introdução Simulação é uma ferramenta extremamente importante em probabilidade e estatística. Dois são seus principais usos. De um lado, é possível usar um modelo de probabilidade para recriar, artificialmente, no computador, amostras desse modelo, quantas vezes quisermos. Tudo se passa como se tivéssemos o poder de criar um mundo, totalmente controlado por nós, e podemos simular esse mundo e verificar o que acontece. Isso é útil quando não podemos ou não queremos verificar matematicamente o que deveria acontecer com base no modelo matemático de probabilidade. De outro lado, é possível usar simulações para aproximar quantidades matemáticas que não podemos ou não queremos calcular analiticamente. Por calcular analiticamente, é fazendo a conta nós mesmos. Um exemplo simples disso é, em vez de calcular os resultados de alguma análise combinatória usando as fórmulas de combinação e simularmos os valores e contar quantas combinações resultam. No geral usamos simulação para o primeiro tipo de uso. Mas, aqui no curso, será útil que vocês verifiquem resultados matemáticos por meio de simulações. Assim, se escrevemos que \\(\\sum_{i=1}^{10} i = 55\\), vocês podem verificar com código no R essa soma, por exemplo, rodando: sum(1:10). Sempre que vocês tiverem dúvida de algum passo matemático, podem fazer uma simulação para entender melhor o que está acontecendo. Isso é encorajado, para melhorar a intuição do que está acontecendo e lhe assegurar que de fato você está entendendo o que está acontecendo. Podemos usar simulações para aproximar quantidades. Um exemplo clássico em probabilidade é a fórmula de Stirling, dada por \\(n! \\sim \\sqrt{2\\pi}n^{n + 1/2}*e^{-n}\\). Então, por exemplo, \\(10! = 10*9*8*7*6*5*4*3*2*1\\) com os computadores modernos pode ser calculada diretamente para números não tão grandes. No caso, \\(10! = 3628800\\). Ou pode ser aproximada pela fórmula de Stirling: Exemplo 4.1 # especificando semente, para simulação ser reproduzível set.seed(2) # número de amostras stirling_aprox &lt;- function(n) { sqrt(2*pi)*n^(n+1/2)*exp(-n) } print(stirling_aprox(10)) [1] 3598696 # razão da aproximação para o valor correto stirling_aprox(10)/3628800 [1] 0.991704 # erro percentual 1 - stirling_aprox(10)/3628800 # 0,8% [1] 0.00829596 Como se vê, a fórmula de Stirling é bem precisa para aproximar fatorial e muito mais fácil de computar. 4.3 DGP — Processo Gerador de Dados Definição (intuitiva): um DGP descreve como os dados são produzidos na população. Tipicamente contém um componente determinístico e outro estocástico. Origem do termo “estocástico” Estocástico possui origem grega, stókhos. Referia-se à capacidade de adivinhar ou conjecturar. Em latim, o termo aproximadamente equivalente é alea, como na famosa frase de Júlio Cesar, “Alea iacta est”, quando decidiu atravessar o Rubicão. Comumente traduzido por “a sorte está lançada”, mas aparentemente seria melhor algo como “os dados estão lançados”, sendo os dados aleatórios. Em resumo, temos duas palavras com significados similares (estocástico e aleatório). O inglês random aparentemente vem do francês randon, querendo dizer impetuoso, desorganizado, e utilizado pelo seu derivado randonné, que tem a ver com trilha ou caminhada desorganizada, caótica, bagunçada. Portanto, nada de usar “randomizar” quando podemos utilizar aleatorizar com o mesmo significado. Fonte: math.stackexchange.com Por hora, vamos trabalhar com DGPs apenas estocásticos. Um DGP estocástico é tipicamente definido por uma distribuição de probabilidade (Normal, Bernoulli, Poisson, Exponencial etc.) e os parâmetros que govenram a distribuição: média/variância (Normal), probabilidade (Bernoulli), taxa (Poisson/Exponencial). Às vezes a distribuição de probabilidade pode ser reparametrizada (por exemplo, especificar a Normal em termos do desvio-padrao em vez da variância, ou o inverso da variância, chamado de precisão). É preciso definir também a relação entre os dados, se de Independência/Identicamente distribuídos (i.i.d.) ou se há presença de dependência (séries temporais, agrupamentos espaciais etc.). 4.4 Simulação de Monte Carlo A simulação de Monte Carlo é um método computacional que se utiliza de amostras repetidas de um dado DGP para nos permitir calcular determinadas quantidades númericas. O nome vem do principado de Mônaco, com seus cassinos e jogos de azar, e foi criado nos anos 40 em Los Alamos, para auxiliar nos cálculos necessários para construção da bomba atômica. Em estatística, as quantidade calculadas são chamadas de “estatísticas”. Estatística Uma estatística é qualquer quantidade calculada a partir de uma amostra e utilizada para um propósito estatístico. Exemplos são a média (amostral), variância, desvio-padrão, mediana. Mas também estatística T, Z, F etc. Quando calculamos uma estatística para estimar um parâmetro de um modelo, chamamos essa estatística de estimativa. E o método de estimador. Para realizar uma simulação ou experimento de Monte Carlo, especificamos o DGP, tamanho amostral, definimos a semente para garantir a reprodutibilidade e então o resumo e visualização da distribuição empírica da estatística. Pergunta: Por que usamos set.seed() antes de simular? 4.4.1 Simulando um dado Comecemos lembrando que probabilidades podem ser interpretadas como frequências relativas de longo prazo. Portanto, a probabilidade de um evento pode ser aproximada por simulações, na medida em que aproximamos a frequência relativa com que o fenômeno acontece em nossas simulações. Vamos dar um exemplo simples desse tipo de aplicação. Suponha que quero simular a probabilidade do número 6 sair em um dado de seis lados. Exemplo 4.1.1 # especificando semente, para simulação ser reproduzível set.seed(234) # número de amostras n &lt;- 10000 # 1000 amostras de uma lançamento de dado de 6 lados resultado &lt;- sample(1:6, n, TRUE) # frequência relativade 6 é dada por número de 6 / total de amostras prob_6 &lt;- sum(resultado == 6)/n # 16,89% # 1/6 = 16.6666 Podemos também ver como a aproximação converge para o verdadeiro valor à medida que \\(n\\) cresce. # especificando semente, para simulação ser reproduzível set.seed(234) # número de amostras vec_amostra &lt;- c(100, 1000, 10000, 100000, 1000000) # lista vazia para armazenar os resultados das simulações resultado_lista &lt;- list() # vetor vazio para armazenar a frequência relativa de 6 vec_prob6 &lt;- numeric() set.seed(234) # loop sobre os tamanhos das amostrar for ( i in 1:length(vec_amostra)) { # n amostras de uma lançamento de dado de 6 lados resultado_lista[[i]] &lt;- sample(1:6, vec_amostra[i], TRUE) # frequência relativade 6 é dada por número de 6 / total de amostras vec_prob6[i] &lt;- sum(resultado_lista[[i]] == 6)/vec_amostra[i] } print(vec_prob6) [1] 0.150000 0.189000 0.164700 0.169250 0.166257 Se supusermos que todos os números possuem a mesma chance de sair quando jogamos os dados, então, a distribuição conjunta de \\(X\\) e \\(Y\\) pode ser dada por: library(knitr) library(kableExtra) # Definir os valores de (x, y) e P(X = x, Y = y) valores &lt;- c(&quot;(2, 1)&quot;, &quot;(3, 2)&quot;, &quot;(4, 2)&quot;, &quot;(4, 3)&quot;, &quot;(5, 3)&quot;, &quot;(5, 4)&quot;, &quot;(6, 3)&quot;, &quot;(6, 4)&quot;, &quot;(7, 4)&quot;, &quot;(8, 4)&quot;) probabilidades &lt;- c(0.0625, 0.1250, 0.0625, 0.1250, 0.1250, 0.1250, 0.0625, 0.1250, 0.1250, 0.0625) # Criar a tabela tabela &lt;- data.frame(&quot;(x, y)&quot; = valores, &quot;P(X = x, Y = y)&quot; = probabilidades) # Formatar a tabela kable(tabela, caption = &quot;Tabela 2.26: Tabela representando a distribuição conjunta da soma (X) e o maior (Y) de dois lançamentos de um dado de quatro faces&quot;, col.names = c(&quot;$(X,Y)$&quot;, &quot;$P(X=x, Y=y)$&quot;)) %&gt;% kable_styling(bootstrap_options = &quot;striped&quot;) (#tab:cap4 tab1)(#tab:cap4 tab1)Tabela 2.26: Tabela representando a distribuição conjunta da soma (X) e o maior (Y) de dois lançamentos de um dado de quatro faces \\((X,Y)\\) \\(P(X=x, Y=y)\\) (2, 1) 0.0625 (3, 2) 0.1250 (4, 2) 0.0625 (4, 3) 0.1250 (5, 3) 0.1250 (5, 4) 0.1250 (6, 3) 0.0625 (6, 4) 0.1250 (7, 4) 0.1250 (8, 4) 0.0625 como podemos ver, à medida que \\(n\\) cresce, a simulação converge para o verdadeiro valor do parâmetro, embora isso não seja linear. Por isso que é importante variar a configuração para ter certeza de que a simulação está próxima do verdadeiro valor do parâmetro. De maneira geral, uma simulação envolve os seguintes passos. 4.5 Configuração Definir o modelo de probabilidade, variáveis aleatórias e eventos a serem modelados. O modelo de probabilidade codifica todas as suposições do modelo. No exemplo acima, de que todos os lados têm a mesma probabilidade de sair, que existem apenas seis possibilidades, numeradas de \\(1-6\\) e assim por diante. 4.6 Simular Vamos considerar o exemplo dos dados novamente. Vamos usar a função sample para simular lançamento de um dado de quatro faces. Para “jogar o dado” uma vez, sorteio um número entre 1 e 4. X &lt;- sample(1:4, size=1) Como quero a frequência de longo prazo, preciso repetir esse processo (de maneira independente a cada jogada) \\(n\\) vezes. # número de jogadas/simulações n &lt;- 1000 # vetor X, para armazenar o resultado de cada uma das n jogadas X &lt;- numeric() # simulando n vezes for( i in 1:n){ X[i] &lt;- sample(1:4, size=1) } # visualizando as primeiras 20 jogadas head(X, 20) [1] 4 4 1 4 2 3 2 4 2 2 3 4 4 1 4 2 4 2 3 3 4.7 Resumir Após realizada a simulação, queremos não olhar todos os números simulados, mas resumir a simulação. Por exemplo, obtendo a minha distribuição de probabilidade, ou a esperança. # prob X = 1 sum(X==1)/n [1] 0.262 # prob X = 2 sum(X==2)/n [1] 0.26 # prob X = 3 sum(X==3)/n [1] 0.232 # prob X = 2 sum(X==4)/n [1] 0.246 ## resumo geral summary(X) Min. 1st Qu. Median Mean 3rd Qu. Max. 1.000 1.000 2.000 2.462 3.000 4.000 Questão: O que faz o código replicate(10000, { … })? 4.8 LGN e TCL Qual a diferença entre a Lei dos Grandes Números e o Teorema Central do Limite? Uma forma de entender esses dois resultados fundamentais é por meio de simulações. Vamos simular o LGN e depois o TCL library(knitr) library(ggplot2) library(tidyverse) set.seed(123) n_values &lt;- c(5, 10, 30, 50, 100, 200, 300, 400, 500, 1000, 2000, 3000, 4000, 5000, 10000, 20000, 30000, 40000, 50000) s_means &lt;- numeric() for(i in 1:length(n_values)) { s_means[i] &lt;- mean(rnorm(n_values[i], mean = 100, sd = 15)) } df &lt;- data.frame(n = n_values, sample_mean = s_means) kable(df) n sample_mean 5 102.90355 10 101.97687 30 100.36523 50 101.25004 100 98.73293 200 100.60105 300 99.51731 400 101.08235 500 100.19364 1000 100.07918 2000 99.81436 3000 99.99797 4000 100.08209 5000 99.88526 10000 99.66418 20000 99.99947 30000 100.17037 40000 100.07525 50000 100.01786 df %&gt;% ggplot(aes(x=n, y=sample_mean)) + geom_point() + geom_line() + theme_minimal() Pergunta: A LGN diz que a média amostral iguala a média populacional quando \\(n\\) é grande, ou apenas que se aproxima da média populacional? Definição do TCL: Se você coletar muitas amostras independentes de tamanho \\(n\\) de uma população com qualquer distribuição de probabilidade, mas com variância finita, então a distribuição da médioa amostral será aproximadamente normal quando \\(n\\) é grande. Exemplo de uma simulação com distribuição oginal da variável aleatória uniforme: set.seed(123) means &lt;- replicate(10000, mean(runif(36, min = 0, max = 1))) hist(means, main = &quot;Distribution of sample means&quot;, xlab = &quot;Mean&quot;, breaks = 30) Qual a média e o desvio-padrão da distribuição da média amostral? Podemos calcular aproximadamente a partir de nossa simulação. A média da simulação é 0.5000232 e o desvio-padrão é 0.048084. Vejam que a média amostral é próxima da média populacional e o desvio-padrão amostral é próximo do desvio-padrão populacional dividido pela raiz-quadrada de \\(n\\). Lembrando que a variância de uma uniforme é dada por \\(\\frac{1}{12}(a - b)^2\\), em que \\(a\\) é o maior valor e \\(b\\) o menor. Logo, em nosso caso, a variância é \\(\\frac{1}{12}(1 - 0)^2 = 1/12\\). E o desvio-padrão é a raiz quadrada disso, ou \\(0.2886751\\). Como \\(n = 36\\), a raiz-quadrada é \\(6\\), ou seja, igual a \\(0.2886751/6 = 0.04811252\\). 4.9 Cobertura de um IC Um Intervalo de Confiança (IC) de 95% possui uma cobertura nominal de 95%. O que isso quer dizer? Quer dizer que se repetirmos o procedimento infinitas vezes, em 95% delas o IC conterá o verdadeiro valor do parâmetro. Vamos verificar isso com uma simulação. set.seed(123) n_sims &lt;- 10000 n &lt;- 30 true_mean &lt;- 50 true_sd &lt;- 10 cover &lt;- replicate(n_sims, { x &lt;- rnorm(n, mean = true_mean, sd = true_sd) se &lt;- sd(x)/sqrt(n) ci_lower &lt;- mean(x) - 1.96*se ci_upper &lt;- mean(x) + 1.96*se (true_mean &gt;= ci_lower) &amp; (true_mean &lt;= ci_upper) }) mean(cover) # proportion of CIs containing true mean [1] 0.9461 Nessa simulação, nós rodamos 10.000 vezes a simulação e calculamos o intervalo de confiança para a média em cada iteração. Ao final, comoutamos o percentual de vezes em que o CI continha o verdadeiro valor do parâmetro. Veja que nesse exemplo não foi exatamente 95%, mas 94.61%. Cmo sempre, a simulação nos dá uma resposta aproximada, não exata. "],["o-modelo-de-regressão.html", "Capítulo 5 O Modelo de Regressão 5.1 Modelo de Regressão - Teoria 5.2 CEF 5.3 CEF e voto 5.4 Algumas Propriedades da CEF 5.5 Regressão Linear 5.6 Logaritmos 5.7 Prevendo a partir da esperança condicional 5.8 Preditores lineares", " Capítulo 5 O Modelo de Regressão “Prediction is hard. Specially about the future”. Fonte desconhecida. 5.1 Modelo de Regressão - Teoria O modelo de regressão (não confundir com regressão linear) é uma forma bem ampla de modelar os dados para prever uma variável de interesse, usualmente designada pela letra \\(Y\\). Se eu quero prever os votos de candidatas em uma eleição, a votação de cada candidata é minha variável de interesse, \\(Y\\). Digamos que eu tenho uma amostra da intenção de votos das candidatas, obtidas por meio de uma pesquisa eleitoral. Então, a regressão é uma forma de modelar os dados para prever justamente essa variável \\(Y\\). Antes de entrarmos em Regressão, vale a pena repassar a nomenclatura básica sobre \\(Y\\) e \\(X\\). Table 5.1: Table 5.2: Nomenclaturas para Variáveis em Regressão Linear VD VI Variável Resposta Variável Preditora Variável Explicada Variável Explicativa Variável Predita Variável de Entrada Variável de Saída Variável Exógena Variável Regredida Regressor Regressando Covariável Uma das primeiras distinções que temos de fazer é sobre previsão e explicação (causal). Quando queremos prever, estamos interessados em saber quais os provaveis valores de variáveis no futuro, a parte de informações sobre a própria variável e outras no passado. Nesse sentido, é preciso algum tipo de suposição de que o futuro se assemelha ao passado de algum modo. Esse tipo de suposição usualmente toma a forma de um modelo probabilísitco, mas não apenas. Quando estamos interessados em explicações causais, temos dois tipos de perguntas de pesquisa possíveis. Uma sobre a chamada causa dos efeitos e outra sobre o efeito das causas (Gelman &amp; Imbens, 2013). A causa dos efeitos são perguntas do tipo: o que causal a II Grande Guerra? Ou qual a causa da eleição de Trump ou Bolsonaro? O que explica a desigualdade de renda no Brasil? São todas perguntas em que queremos explicar um efeito, isto é, identificar as causas de um fenômeno (efeito). Já o efeito das causas ão perguntas do tipo: qual o efeito da vacina de covid-19 sobre a mortalidade por Covid-19? Qual o efeito de checagem de notícias sobre a crença de pessoas em desinformação? Qual o efeito da magnitude eleitoral sobre fragmentação partidária? E assim por diante. Aqui, estamos interessados em entender o efeito causal de uma variável sobre outra, sem pretender esgotar todas as explicações de causa possíveis. A maior parte dos métodos quantitativos existentes são bons para responder perguntas de previsão e de causa dos efeitos. Grosso modo, não há método quantitativo para estimação do efeito das causas, exceto realizar uma série de estudos independentes sobre várias causas dos efeitos, olhando uma causa distinta do mesmo efeito por vez e esperar que isso gere um conhecimento combinado sobre essas múltiplas causas. Mas não há, contudo, uma metodologia bem definida de como combinar esses estudos independentes em um único conhecimento do efeito conjunto das causas. Assim, nosso curso será dedicado apenas a modelos de previsão e modelos de causa dos efeitos, que é o que temos de metodologias já desenvolvidas e consolidadas. Começamos por essa explicação porque uma perspectiva mais antiga, e ainda comum nas ciências sociais, é que modelos de regressão múltiplas permitem estimar o efeito de várias causas. Isso raramente é o caso e não adotaremos essa perspecitva aqui 1. Há muitas formas de apresentar ou motivar regressão linear. O método mais tradicional é pensar que a regressão linear é uma reta que é ajustada aos pontos observados. Porém, não tomaremos esse caminho aqui. Nós iremos considerar que a regressão linear é uma forma de aproximar a chamada “Conditional Regression Function” (CEF, na sigla em inglês). O objetivo é entender “as far as possible with the available data how the conditional distribution of some response \\(y\\) varies across subpopulations determined by the possible values of the predictor or predictors” (Cook and Weisberg, apud Berk, p. 4). 5.2 CEF Para introduzir a CEF, iremos trabalhar com um banco de dados de eleição. O código abaixo mostrar como importar e processar os dados. O dado original foi retirado do portal de dados abertos do TSE, com os dados de votação nominal nas eleições de 2020 para o estado de São Paulo. library(here) library(data.table) library(tidyverse) # Importando os dados desejados # dados &lt;- fread(here(&quot;dados\\\\votacao_secao_2020_SP\\\\&quot;, &quot;votacao_secao_2020_SP.csv&quot;), encoding = &quot;Latin-1&quot;) # dados_finais &lt;- dados %&gt;% # dplyr::filter(DS_CARGO == &quot;Prefeito&quot; &amp; ANO_ELEICAO == 2020 &amp; NM_TIPO_ELEICAO == &quot;Eleição Ordinária&quot;) %&gt;% # group_by(DS_ELEICAO, NR_TURNO, NM_MUNICIPIO, # NR_ZONA, NR_SECAO, NR_VOTAVEL) %&gt;% # summarise(votos_nominais = sum(QT_VOTOS)) # saveRDS(dados_finais, file=here(&quot;dados\\\\&quot;, &quot;dados_finais.rds&quot;)) #salvei os dados e posso importar o que fiz. # o arquivo dados_finais.rds está no respositório em: # https://github.com/mgaldino/book-regression/tree/main/dados dados_finais &lt;- readRDS(here(&quot;dados&quot;, &quot;dados_finais.rds&quot;)) # Identificar os municípios que possuem segundo turno municipios_segundo_turno &lt;- dados_finais %&gt;% ungroup() %&gt;% filter(NR_TURNO == 2) %&gt;% dplyr::select(NM_MUNICIPIO) %&gt;% distinct() # Filtrar os dados para incluir apenas os municípios que têm segundo turno dados_filtrados &lt;- dados_finais %&gt;% ungroup() %&gt;% inner_join(municipios_segundo_turno, by = &quot;NM_MUNICIPIO&quot;) dados_turno &lt;- dados_filtrados %&gt;% filter(!NR_VOTAVEL %in% c(95, 96)) %&gt;% # nulos e brancos fora pivot_wider( id_cols = c(DS_ELEICAO, NM_MUNICIPIO, NR_ZONA, NR_SECAO, NR_VOTAVEL), names_from = NR_TURNO, values_from = votos_nominais, names_prefix = &quot;votos_turno_&quot; ) %&gt;% rename(votos_1turno = votos_turno_1, votos_2turno = votos_turno_2) dados_turno1 &lt;- dados_turno %&gt;% group_by(NR_VOTAVEL, NM_MUNICIPIO) %&gt;% mutate(voto_total_candidato_2t = sum(votos_2turno, na.rm=T), voto_total_candidato_1t = sum(votos_1turno, na.rm=T)) %&gt;% ungroup() %&gt;% group_by(NM_MUNICIPIO, NR_ZONA, NR_SECAO) %&gt;% mutate(votos_validos_1t = sum(votos_1turno), votos_validos_2t = sum(votos_2turno, na.rm=T)) %&gt;% filter(voto_total_candidato_2t &gt; 0) %&gt;% group_by(NM_MUNICIPIO, NR_ZONA, NR_SECAO, NR_VOTAVEL) %&gt;% mutate(perc_validos_1t = votos_1turno/votos_validos_1t, perc_validos_2t = votos_2turno/votos_validos_2t) %&gt;% ungroup() # salvando dados que iremos trabalhar em sala de aula efetivamente. saveRDS(dados_turno1, file=here(&quot;Dados//&quot;, &quot;dados_turno1.rds&quot;)) O que constitui uma boa previsão? Tradicionalmente, empregamos a noção de Erro Quadrátco Médio (EQM) para quantificar boas previsões. Quanto menor o EQM, melhor uma previsão. Se o objetivo é, portanto, fazer previsões que minimizem o EQM, iremos apresertar e mostrar que a Função de Esperança Condicional (CEF, na sigla em inglês) é o melhor preditor global possível. Vamos dizer, em outras palavras, porque esse resultado é verdadeiramente incrível. A CEF é o melhor preditor possível dentre todos que existam ou possam vir a existir, entendendo melhor como ter o menor EQM. Por isso que a CEF é o ponto de partida de qualquer preditor que exista, seja uma regressão simples ou algoritmos de aprendizens de máquinas como “random forest” ou mesmo algorítimos de deep learning de redes neurais por traz dos recentes avanços na inteligência artificial. Mesmo os algorítmos mais avançados de inteleigência artificial, como os Large Language Models, que estão na base de ferramentas como ChatGPT, não podem ter desempenho melhor que a função de experança condicional, CEF, ao fazer uma previsão. Naturalmente, se esse é o caso, a próxima pergunta que todos nós iremos fazer é: por que não aprender apenas a usar a CEF, que é o melhor preditor possível, e ser feliz para sempre? Porque a natureza não nos diz qual é a CEF. Nós nunca sabemos qual a verdadeira função de esperança condicional. Então tentamos aproximar o mais bem possível a CEF, a partir de simplificações da realidade. Em particular, nosso curso pode ser pensado em torno das seguintes perguntas: como aproximar a CEF por meio de regressão linear (combinação lineares de preditores)? Quais as propridades dessa aproximação? Em que condições ela é uma boa aproximação e em que sentido (quantitativo e preciso) podemos falar de boa aproximação? Mais para o final do curso faremos a conexão entre a CEF, modelos preditivos e modelos causais. 5.3 CEF e voto library(ggplot2) library(tidyr) p1 &lt;- dados_turno1 %&gt;% ggplot(aes(x=perc_validos_1t , y = perc_validos_2t )) + geom_point() + theme_bw(base_size = 22) + scale_x_continuous(labels = scales::percent) + scale_y_continuous(labels = scales::percent) print(p1) (#fig:voto_turnos_todos_municipios cap5)voto O gráfico agrupando todos os munícipios de São Paulo sem distinção mostra uma correlação positiva, mas muita variabilidade. Vamos ver por município. p2 &lt;- dados_turno1 %&gt;% ggplot(aes(x=perc_validos_1t , y = perc_validos_2t )) + geom_point() + theme_bw(base_size = 22) + facet_wrap(~ NM_MUNICIPIO) + scale_x_continuous(labels = scales::percent) + scale_y_continuous(labels = scales::percent) print(p2) (#fig:voto_turnos_por_municipios)voto_por_municipio Os gráficos fazem mais sentido e a variabilidade diminuiu no geral, mas alguns municípios estão com padrões estranhos. Vamos colorir os pontos por candidato? p3 &lt;- dados_turno1 %&gt;% ggplot(aes(x=perc_validos_1t , y = perc_validos_2t, colour = NR_VOTAVEL)) + geom_point() + theme_bw(base_size = 22) + facet_wrap(~ NM_MUNICIPIO) + scale_x_continuous(labels = scales::percent) + scale_y_continuous(labels = scales::percent) + theme(legend.position=&quot;none&quot;) print(p3) (#fig:voto_turnos_por_municipios_candidatos)voto_por_municipio Municípios como Diadema, Sorocaba, Franca e Taubaté, que não estavam fazendo muito sentido, agora passaram a fazer. E o que é a Esperana Condicional? Qual o valor esperado do \\(Y\\) para um davo valor de \\(X\\). No gráfico, é dado pela linha vermelha. 5.4 Algumas Propriedades da CEF A CEF é simplesmente uma função de \\(X_i\\) que resulta na esperança condficional, ou seja \\(\\mathbb{E}[Y_i|X_i]\\). É como se eu escrevesse \\(f(X_i) = \\mathbb{E}[Y_i|X_i]\\). Como \\(X_i\\) pode ser pensado como aleatório, então a CEF também é aleatória. Nós podemos descrever os dados da seguinte forma: \\(Y_i = \\mathbb{E}[Y_i|X_i] + e_i\\). Ou seja, a Esperança condicional mais um termo de erro. Nós veremos que é possível mostrar que \\(\\mathbb{E}[e_i|X_i] = 0\\), ou seja, independência na média. 5.4.1 CEF é a melhor aproximação aos dados Suponha que estamos interessados em achar a melhor função de \\(X\\), \\(m(x)\\) que aproximada os dados \\(Y\\). Qual seria essa função? Se nós utilizamos o EQM para computar quão bem estamos aproximando os dados, então: \\((Y_i - m(X_i))^2 = ((Y_i - \\mathbb{E}[Y_i|X_i]) + (\\mathbb{E}[Y_i|X_i] - m(X_i)))^2\\) \\((Y_i - m(X_i))^2 = (Y_i - \\mathbb{E}[Y_i|X_i])^2 + 2(Y_i - \\mathbb{E}[Y_i|X_i])(\\mathbb{E}[Y_i|X_i] - m(X_i) + (\\mathbb{E}[Y_i|X_i] - m(X_i))^2\\) Nós queremos minimizar a esperança do erro quadrático, isto é, achar \\(m(X_i)\\) que torna o EQM mínimo. Logo, sabemos que qulquer que seja o \\(m(X_i)\\) que minimiza o EQM, não impacta o primeiro termo da equação acima, de forma que podemos desconsiderá-lo. \\(Y_i - \\mathbb{E}[Y_i|X_i]\\) é \\(e_i\\). Como a esperança de \\(e_i\\) é zero, então todo o termo é zero. De forma que só sobra a esperança do último termo. E o melhor que se pode fazer é zerar o termo, ou seja, \\(m(X_i) = \\mathbb{E}[Y_i|X_i]\\). 5.5 Regressão Linear Como podemos ver nos gráficos, a esperança condicional é em geral não-linear. A ideia da regressão linear é aproximar a CEF por meio de uma reta. Vamos plotar a reta de regressão linear nos gráficos, juntamente com a CEF. # Criar o gráfico grafico_cef_reg &lt;- ggplot() + geom_point(data = dados_binned, aes(x = perc_validos_1t, y = perc_validos_2t), alpha = 0.3, color = &quot;gray&quot;) + geom_point(data = cef_data, aes(x = bin_center, y = perc_validos_2t_mean), color = &quot;red&quot;, size = 2) + geom_line(data = cef_data, aes(x = bin_center, y = perc_validos_2t_mean), color = &quot;blue&quot;) + geom_smooth(data = dados_binned, aes(x = perc_validos_1t, y = perc_validos_2t), colour = &quot;black&quot;,linewidth=1, method=&quot;lm&quot;, se=FALSE) + facet_wrap(~NM_MUNICIPIO) + labs(title = &quot;CEF e Reta de Regressão Linear&quot;, x = &quot;Percentual de Votos Válidos no 1º Turno&quot;, y = &quot;Percentual Médio de Votos Válidos no 2º Turno&quot;) + theme_minimal() + scale_x_continuous(limits = c(0, 1), labels = scales::percent) + scale_y_continuous(limits = c(0, 1), labels = scales::percent) print(grafico_cef_reg) (#fig:CEF_voto_turnos_regressao)voto_por_municipio 5.6 Logaritmos É muito comum a utilização de logaritmos em regressões, de modo que vale uma digresssão sobre a correta interpretação de logaritmos. A primeira razão é que a diferença de logaritmos é aproximadamente igual à diferença entre porcentagens. Vamos ilustrar isso por meio de um exemplo, e então mostaremos matematicamente que isso sempre é o caso. 5.6.1 Logartimos como diferença de porcentagens De acodo com dados da PNADc de 2017, quarto trimstre, o salário por hora dos homens é em média \\(14.72\\) reais, e o das mulheres \\(12.57\\) reais. A diferença percentual no salário é, portanto, \\(100*(14.72 - 12.57)/12.57\\) ou \\(17,1%\\). Se em vez de calcular a diferença percentual, usar a aproximação \\(log(14.72) - log(12.57) = 0.1578941\\) ou 15,8%, tenho um valor próximo do verdadeiro valor de 17,1%. O caso geral pode ser visto do seguinte modo. Sejam dois números positivos \\(a\\) e \\(b\\), com \\(a &gt; b\\). A diferença percentual \\(p\\) entre \\(a\\) e \\(b\\) é dada por: \\[ 100*(a - b)/b = p \\] Que é o mesmo que: \\[ a/b = 1 + p/100 \\] Se passarmos o logaritmo natural de ambos os lados, temos: \\[ log(a/b) = log(1 + p/100) \\] \\[ log(a) - log(b) = log(1 + p/100) \\] Iremos usar a aproximação \\(log( 1 + x) \\approx x\\) para \\(x\\) pequeno. Logo: \\[ log(a) - log(b) \\approx p/100 \\] Numericamente, como o erro dessa aproximação varia para diferentes valores de \\(p\\)? Exercício em sala: Vamos fazer uma simulação para verificar como o erro da aproximação funciona. Ou seja, vamos variar a diferença percentual entre dois números \\(a\\) e \\(b\\) e calcular o erro de aproximação pela fórmula com logaritmo. dif_perc &lt;- function(a,b){ stopifnot(a &gt; b) 100*(a - b)/b } aprox_dif_perc &lt;- function(a,b){ stopifnot(a &gt; b) 100*(log(a) - log(b)) } erro_aprox_perc &lt;- function(a,b) { dif_perc(a,b) - aprox_dif_perc(a,b) } b &lt;- 100 a &lt;- seq(100.1, 120, by=.1) vec_erro &lt;- numeric() true_p &lt;- numeric() for ( i in 1:length(a)) { vec_erro[i] &lt;- erro_aprox_perc(a[i],b) true_p[i] &lt;- dif_perc(a[i],b) } df &lt;- data.frame(erro = vec_erro, p = true_p ) library(ggplot2) library(tidyverse) df %&gt;% ggplot(aes(x=p, y=erro)) + geom_point() + geom_hline(yintercept = .1, colour= &quot;red&quot;) + labs(x = &quot;Diferença Percentual Real&quot;, y = &quot;Erro de Aproximação (p.p.)&quot;, title = &quot;Erro na Aproximação da Dif Per&quot;) + theme_minimal() # Verificação do intervalo onde o erro é menor que 0.1 pp df$within_tolerance &lt;- abs(df$erro) &lt; 0.1 max_p_within_tolerance &lt;- max(df$p[df$within_tolerance]) print(paste(&quot;Máximo p com erro &lt; 0.1 pp:&quot;, round(max_p_within_tolerance, 2))) ## [1] &quot;Máximo p com erro &lt; 0.1 pp: 4.5&quot; A segunda razão é que se aplicarmos essa lógica para médias, estamos aproximando a diferença percentual na média geométrica. 5.6.2 Logaritmos e média geométrica A média geométrica de um conjunto de observações \\((x_1, x_2, \\cdots, x_n)\\) é dada por: \\(MG = \\sqrt[n]{(x_1 \\cdot x_2 \\cdot x_n)}\\). Se considerarmos variáveis aleatórias, \\(X\\) e \\(Y\\), ambas positivas, e suas respectivas transformações com logaritmo, \\(log(X)\\) e \\(log(Y)\\), definimos a média geométrica \\(\\theta_x = \\exp(\\mathbb{E}[log(X)])\\) e \\(\\theta_y = \\exp(\\mathbb{E}[log(Y)])\\) e a diferença percentual como: \\(p = 100\\frac{(\\theta_y - \\theta_x)}{\\theta_x}\\) A diferença na esperança da transformações do log é: \\(100(\\mathbb{E}[log Y] - \\mathbb{E}[log X]) = 100 (log \\theta_y - log \\theta_x) \\approx p\\) Em paavras: A diferença na média de variáveis transformadas (pelo log) é aproximadamente a diferença percentual nas médias geométricas. Isso é relevante por causa do seguinte. Suponha que temos o logaritmo do salário de homens e mulheres: \\(\\mathbb{E}[Y|homem] =\\mu_1\\) \\(\\mathbb{E}[Y|mulher] =\\mu_2\\) Podemos dizer que a diferença \\(\\mu_2 - \\mu_1\\) é aproximadamente a diferença percentual na média geométrica. 5.7 Prevendo a partir da esperança condicional No gráfico abaixo, as esperanças condicionais para os dois valores de \\(X\\) são dadas pelos triângulos vermelhos. Como vemos, há uma grande variabilidade nos dados de salário em torno da esperança condicional. Isso significa que se só tenho a variável gênero, prever \\(Y\\) a partir da esperança condicional \\(\\mathbb{E}[Y|X]\\) resulta em um erro grande. De fato, o erro que cometemos pode ser calculado por meio do Erro Quadrático Médio, que é definido como a soma dos erros ao quadrado (para não cancelar). No R, isso pode ser calculado como: # # df_erro &lt;- df %&gt;% # group_by(genero) %&gt;% # mutate(cond_exp = mean(log_salario)) %&gt;% # ungroup() %&gt;% # mutate(erro = log_salario - cond_exp) # # df_erro %&gt;% # select(log_salario, genero, cond_exp, erro) %&gt;% # head() %&gt;% # kable() # # df_erro %&gt;% # summarise(eq = round(sum(erro),4), # eqm = sum(erro^2)) %&gt;% # kable() Como iremos mostrar depois, não existe nenhuma outra forma melhor de prever \\(Y\\), se só tivermos informação de \\(Y\\) e \\(X\\). Ou seja, esse é o menor erro quadrático médio possível. Claro que, com mais variávels, podemos condicionais em mais informação e melhorar a previsão. Por exemplo, se temos a variável raça, além de gênero, podemos condicionar nas duas variáveis. Vemos que o EQM diminuiu ao condicionar em mais variáveis. Para reforçar, até aqui estamos falando da população. Embora no exemplo tenhamos dados da PNAD, que é uma amostra, e em teoria deveríamos trabalhar com o censo, estou simplificando e assumindo que a PNAD representa a população. Fazendo essa suposição, o erro que é cometido pela esperança condicional para prever \\(Y\\) é o menor erro (quantificado pelo EQM) possível (iremos demonstrar isso no próximo capítulo). 5.8 Preditores lineares Como vimos, uma boa aproximação para a CEF é um preditor linear. Se temos um preditor \\(x\\) e queremos usar esse preditor para prever \\(y\\), o modelo linear simples pode ser escrito da seguinte forma: \\[ \\hat{y_i} = \\alpha + \\beta x_i \\] Vejam que eu usei \\(x_i\\), mas não \\(y_i\\). Em vez disso, utilizei \\(\\hat{y}_i\\). Este último valor é a minha previsão do \\(y\\) dado o valor do meu preditor \\(x\\) para cada observação \\(i\\). Vejam que, uma vez que nós saibamos os valores de \\(\\alpha\\) e \\(\\beta\\), temos efetivamente uma reta (linear) que relaciona \\(x_i\\) e \\(\\hat{y}_i\\). Um critério para tal é minimizar a diferença, para cada observação, entre \\(y\\) e \\(\\hat{y}\\), isto é: \\[ \\min_{\\mathbf{\\alpha, \\beta}}\\sum_i^n (y_i - \\hat{y}_i)^2 \\] A notação fica mais simples com a esperança matemática: \\[ \\min_{\\mathbf{\\alpha, \\beta}} \\mathbb{E}[(Y - \\hat{Y})^2] \\] Por essa razão, chamamos de: preditor linear de erro quadrático médio mínimo. Por hora, basta saber que se minimizarmos a soma (ou a esperança) obteremos os valores de \\(\\alpha\\) e \\(\\beta\\). Quando falamos de uma amostra (em vez da população), iremos indicar esses valores por \\(\\hat{\\alpha}\\) e \\(\\hat{\\beta}\\). Essa previsão, por mais que nós estejamos tentando minimizar o erro entre o valor do \\(y\\) e previsto \\(\\hat{y}\\), em geral não será perfeito. Assim, para cada previsão \\(\\hat{y}_i\\) cometo um erro \\(\\epsilon_i\\) que é a diferença entre o observado \\(y\\) e o precisto \\(\\hat{y}\\), ou seja: \\[ \\epsilon_i = y_i - \\hat{y}_i \\] Se eu substituir \\(\\hat{y}_i\\) pos sua equação, temos: \\[ \\epsilon_i = y_i - \\alpha - \\beta x_i \\] Rearranjando: \\[ y_i = \\alpha + \\beta x_i + \\epsilon_i \\] Esse é chamado de modelo de regressão linear. Em resumo, o preditor linear é \\(\\hat{y}_i = \\alpha + \\beta x_i\\) e o modelo de regressão lienar é \\(y_i = \\alpha + \\beta x_i + \\epsilon_i\\). No primeiro caso, tenho uma previsão que é diferente do observado. No segundo caso, acoplo a previsão ao erro e obtenho exatamente o observado. Vale notar que minimizar a soma dos quadrados dos resíduos é o mesmo que minimizar a a raiz quadrada dessa soma dividida por \\(n\\), ou seja: \\[ \\min_{\\mathbf{\\alpha, \\beta}}\\sum_i^n (y_i - \\hat{y}_i)^2 = \\min_{\\mathbf{\\alpha, \\beta}} \\sqrt{\\frac{1}{n}\\sum_i^n (y_i - \\hat{y}_i)^2} \\] Essa soma com a raiz quadrada é chamada de erro raiz-quadrático médio (Root-Mean-Squared-Error ou RMSE em inglês). Vamos calcular no R um modelo de regressão linear e depois o RMSE de nossos dados. Para rodar uma regressão no R, usamos a função lm(). Ela recebe a fórmula na forma \\(y \\sim x\\), em que \\(y\\) é a variável resposta e \\(x\\) o preditor. Não é preciso escrever os coeficientes \\(\\alpha\\) e \\(\\beta\\). O R vai calcular eles pra gente e teremos estimativas amostrais \\(\\hat{\\alpha}\\) e \\(\\hat{\\beta}\\). # vamos ficar só com o candidato mais votado dados_filtrados &lt;- dados_turno1 %&gt;% group_by(NM_MUNICIPIO) %&gt;% # calcula o valor máximo de votos no 2º turno por município mutate(max_votos_2t = max(voto_total_candidato_2t, na.rm = TRUE)) %&gt;% # mantém apenas os candidatos que atingiram esse valor filter(voto_total_candidato_2t == max_votos_2t) %&gt;% ungroup() %&gt;% dplyr::select(-max_votos_2t) reg &lt;- lm(perc_validos_2t ~ perc_validos_1t, data = dados_filtrados) reg ## ## Call: ## lm(formula = perc_validos_2t ~ perc_validos_1t, data = dados_filtrados) ## ## Coefficients: ## (Intercept) perc_validos_1t ## 0.3624 0.6597 O resultado do modelo mostra que o intecepto (\\(\\hat{\\alpha}\\)) é \\(0.3624\\) e a inclinação (o \\(\\hat{\\beta}\\)) é 0.6597. O R não reporta o nome dos coeficientes \\(\\hat{\\alpha}\\) ou \\(\\hat{\\beta}\\) mas intercepto para o caso do \\(\\hat{\\alpha}\\) e o nome do preditor para o caso do \\(\\hat{\\beta}\\). E interpretamos esse resultado do seguinte modo: quando o preditor é zero, ou seja, quando o voto no primeiro turno do vencedor da eleição é zero naquela seção eleitoral, a previsão do modelo é que no segundo turno o candidato mais votado tem 36,24% dos votos na seção. E cada ponto percentual de voto na seção se traduz em \\(.01*0.6597 = 0.006597\\) pontos percentuais adicionais, ou seja, 1 ponto pecentual de voto na seção eleitoral se traduz em 0,6597 pontos percentuais. Em outras palavras, o candidato vencedor que teve 1% de voto numa seção eleitoral deve ter (é a previsão do modelo) \\(0.3624 + .01*0.6597 = 0.368997\\) ou aproximadamente 37% dos votos na seção. Similarmente, se esse candidato teve 10% dos votos naquela seção no primeiro turno, então a previsão é que tenha: \\(0.3624 + .1*0.6597 = 0.42837\\) ou aproximadamente 43% dos votos. # vamos ficar só com o candidato mais votado dados_filtrados %&gt;% ggplot(aes(y=perc_validos_2t, x = perc_validos_1t)) + geom_point() + geom_abline(intercept = 0.3624, slope = 0.6597, colour = &quot;red&quot; ) + annotate(&quot;text&quot;, label=&quot;intercepto&quot;, x=.05, y= .33, colour = &quot;red&quot;) (#fig:reg_reta_cap5)voto_por_municipio E vamos obter o RMSE. Para isso, usaremos a função resid(): # vamos ficar só com o candidato mais votado residuos &lt;- resid(reg) sqrt(mean(residuos^2)) ## [1] 0.08419337 Esse número implica que em média cometemos um erro (“resíduo médio”) de .08 pontos percentuais. Ou seja, em média nosso modelo faz previsões diferentes do observado da ordem de .08 pontos percentuais. A maior parte do curso será dedicadas a modelos preditivos, e apenas pontualmente falaremos de modelos causais↩︎ "],["cef-1.html", "Capítulo 6 - CEF 6.1 Erro da CEF 6.2 Simulando para entender a CEF 6.3 Propriedades da CEF 6.4 Regressão Linear e a CEF 6.5 Linearidade e Causalidade 6.6 Modelo só com intercepto 6.7 Variância da Regressão 6.8 Variância condicional 6.9 Efeito Marginal 6.10 CEF linear com preditores não-lineares", " Capítulo 6 - CEF Em geral, iremos simplificar a notação usando variáveis com uma letra. Assim, a variável dependente, Percentual do voto no segundo turno, será dado por \\(Y\\), como constuma ser feito. Já os preditores, como percentual de voto no primeiro turno diferença entre priemeiro e segundo colocado, por \\(X_1, X_2\\) etc. O subscrito indica que é uma variável diferente. Assim, a esperança condicional pode ser reescrita para muitas variáveis preditoras como: \\[\\begin{equation} \\mathbb{E}[Y| X_1 = x_1, X_2 = x_2, ..., X_k=x_k] = m(x_1, x_2, ..., x_k) \\end{equation}\\] Como vimos, a essa esperança damos o nome de Função de Esperança Condicional (Conditional Expectation Function, em inglês). Às vezes é útil pensar na CEF como uma função das variáveis aleatórias \\(X_1, X_2, ..., X_k\\), de modo que escrevemos simplesmente: \\[ \\mathbb{E}[Y| X_1, X_2, ..., X_k] \\] Se tivermos apenas um único preditor, \\(X\\), então a CEF é dada por: \\[ \\mathbb{E}[Y| X] = m(X) \\] 6.1 Erro da CEF Quando estamos fazendo previsões sobre \\(Y\\) a partir da CEF, cometemos um erro. Esse erro (para o caso de uma variável) é definido por: \\[ e = Y - m(X) \\] Se rearranjarmos, isso nos leva à seguinte equação: \\[ Y = m(X) + e \\] Notem que o erro \\(e\\) depende de \\(X\\) e \\(Y\\) e, portanto, é também uma variável aleatória. E uma propriedade (não um pressuspoto) da CEF é que o erro \\(e\\) tem esperança condiconal a \\(X\\) igual a zero. 6.2 Simulando para entender a CEF Vamos fazer simulações no R para entender o erro da CEF. Suponha que \\(Y = X^2 + U\\), em que \\(U \\sim norm(0,1)\\) e \\(X \\sim norm(0,1)\\). set.seed(234) n &lt;- 1000 x &lt;- rnorm(n) u &lt;- rnorm(n) y &lt;- x^2 + u m1 &lt;- mean(y) m2 &lt;- median(y) erro1 &lt;- y - m1 erro2 &lt;- y - m2 print(sum(erro1^2)) ## [1] 2987.139 print(sum(erro2^2)) ## [1] 3062.869 Nós imprimimos os dois erros quadráticos médios calculados, um usando a média de \\(Y\\) como minha estimativa, e outro usando a mediana. A média teve um desempenho melhor. De maneira geral, é possível mostrar que a média é é a melhor estimativa não condicional possível. 6.3 Propriedades da CEF Vejam que: \\[ \\mathbb{E}[e] = \\mathbb{E}[Y - m(X)] \\] e \\[ \\mathbb{E}[e|X] = \\mathbb{E}[Y - m(X)|X] \\] Pela propriedade de lineraridade da esperança, temos então: \\[ \\mathbb{E}[e|X] = \\mathbb{E}[Y|X] - \\mathbb{E}[m(X)|X] \\] Do que segue: \\[ \\mathbb{E}[e|X] = m(X) - \\mathbb{E}[m(X)|X] \\] 6.3.1 A Esperança do Erro Condicional a X é zero Existe um teorema, que não irei demonstrar, chamando de teorema do condicionamento, que diz que em situações como \\(\\mathbb{E}[m(X)|X]\\), isso é igual e \\(m(X)\\). O que torna nossa equação igual a zero. Ou seja: \\[ \\mathbb{E}[e|X] = m(X) - m(X) = 0 \\] 6.3.2 A esperança (não-condicional) do erro é zero. E existe um outro teorema, chamdo de lei das esperanças iteradas (Law of Iterated Expectations) que diz que a esperança da esperança condicional é a esperança não-condicional. Ou seja, \\(\\mathbb{E}[\\mathbb{E}[Y\\|X]] = \\mathbb{E}[Y]\\). Utilizando esse fato, temos que a esperança não-condicional do erro também é zero. \\[ \\mathbb{E}[e] = \\mathbb{E}[\\mathbb{E}[e|X]] = \\mathbb{E}[0] = 0 \\] 6.3.2.1 Intuição da LIE Como a LIE é muito importante em muitas demonstração de estatística, vale a penda dar pelo menos a intuição de sua validade. Para isso, vamos usar a lei da probabilidade total. Digamos que tenho um dado de 6 faces, dado \\(A\\), e outro de 4 faces, o dado \\(B\\). E \\(Y\\) é o valor que sai de jogar um dado e \\(X\\) é qual dado eu joguei. Queremos mostrar que: \\[ \\mathbb{E}[\\mathbb{E}[Y|X]] = \\mathbb{E}[Y] \\] A média das médias condicionais é dado pela média quando escolho o dado A (vezes sua probabilidade) mais a média quando escolho o dado B (vezes sua probabilidade). Se \\(X = A\\), o dado de 6 faces, então \\(\\mathbb{E}[Y|X=A]]\\), isto é, a média do valor do dado é \\((1+2+3+4+5+6)/6 = 3.5\\) Se \\(X = B\\), o dado de 4 faces, então \\(\\mathbb{E}[Y|X=A]]\\), isto é, a média do valor do dado é \\((1+2+3+4)/4 = 2.5\\) Se eu escolher cada dado aleatoriamente, isto é, com probabilidade 50%, então o valor médio de \\(Y\\), dado por \\(\\mathbb{E}[y]\\) é simplesmente \\(3.5*.5 + 3.5*.5 = 3\\). . No caso dos salários condicional ao gênero, efetivamente o que estamos dizendo é o seguinte: \\[ \\mathbb{E}[log(salário)|gênero = homem]*P[gênero = homem] + \\mathbb{E}[log(salário)|gênero = mulher]*P[gênero = mulher] \\] 6.3.3 O erro da CEF é não-correlacionado com X Por fim, uma última propriedade que não iremos demonstrar é que o erro da CEF é não-correlacionado com qualquer função de \\(X\\). Formalmente isso é formulado como \\(\\mathbb{E}[h(x)*e] = 0\\). Em particular, se \\(h(x)=x\\), então \\(\\mathbb{E}[x*e] = 0\\). Esse resultado implica que o erro é não-correlacionado com \\(X\\) (ou qualquer função de \\(X\\)). Dizemos que o erro é independente na média de \\(X\\), ou seja, \\(\\mathbb{E}[e|X]=0\\) e isso implica que o erro não é correlacionado com x. Vejam que já sabemos que \\(\\mathbb{E}[e]=0\\), portanto, \\(\\mathbb{E}[e|X]=0\\) é uma forma de dizer que a média do erro condicional a \\(X\\) é a mesma coisa que a média do erro sem condicionar em \\(X\\). Em outras palavras, a informação de \\(X\\) não muda minha estimativa da média do erro. Voltemos para o exemplo dos dados. Agora, suponha que tenho dois dados de \\(6\\) faces. Nesse caso, saber que escolhi o dado \\(A\\) não muda a médiade \\(Y\\), pois será igualmente \\(3.5\\). É fácil ver que não há correlação entre \\(X\\) e \\(Y\\) nesse caso. Aqui tem uma intuição de porque uma variável ser independente na média significa que não é correlacionada. Suponha que eu tenho uma medida da altura de pessoas, alguma medida do vocabulário delas e suas idades. Condicional à idade, saber a altura de uma pessoa não diz nada sobre o vocabulário dela (e vice-versa). Isso significa que não estão correlacionadas. Nós vimos que o log do salário (por hora) médio não condicional \\(\\mathbb{E}[log(salário)]\\) é 2.15. A tabela abaixo quebra esse valor por gênero, juntamente com a frequência relativa de categoria: # table2 &lt;- df %&gt;% # mutate(total_n = sum(V1028), # log_renda = log(renda)) %&gt;% # group_by(genero) %&gt;% # summarise(salario = round(weighted.mean(salario, w=V1028),2), freq=sum(V1028)/total_n) # # table2 %&gt;% # head(10) %&gt;% # kable() Vejam que: \\[ \\mathbb{E}[log(salário)] = \\mathbb{E}[log(salário)|homem]*p(homem) + \\mathbb{E}[log(salário)|mulher]*p(mulher) \\] \\[ x*p(x) + y*p(y) = x1*y1 = 2.15 \\] Como vimos, é sempre verdade que (para o caso de \\(X\\) assumir dois valores), que: \\[ \\mathbb{E}[\\mathbb{E}[Y|X]] = \\mathbb{E}[Y|X=x_{1}]*p(X_1) + \\mathbb{E}[Y|X =x_2]*p(x_2) \\] Então, voltando ao nosso caso do erro, temos o seguinte (novamente para o caso particular de X assumindo dois valores). \\[ \\mathbb{E}[e]*\\mathbb{E}[X] = \\mathbb{E}[e*\\mathbb{E}[X]] = \\mathbb{E}[e*\\mathbb{E}[e|X]] = \\mathbb{E}[X*Y|X=x_{1}]*p(X_1) + \\mathbb{E}[X*Y|X =x_2]*p(x_2) \\] ## A esperança condicional é o melhor preditor Nós já antecipamos que a esperança conidcional é o melhor preditor de \\(Y\\) no sentido de minimizar o EQM. vamos agora mostrar isso. Anteriormente definidmos a esperança condicional \\(\\mathbb{E}[Y|X]\\) como \\(m(x)\\). Vamos por um momento esquecer essa definição e assumir que \\(m(x)\\) é valor que minimizar o EQM. Iremos provar que nesse caso, \\(m(x) = \\mathbb{E}[Y|X]\\) \\[ EQM = \\mathbb{E}[(Y - m(X))^2] \\] Somando e subtraindo \\(\\mathbb{E}[Y|X]\\) não altera a equação. Portanto \\[ EQM = \\mathbb{E}[(Y - \\mathbb{E}[Y|X] + \\mathbb{E}[Y|X] - m(X))^2] \\] Chamando por um momento \\(Y - \\mathbb{E}[Y|X] = a\\) e \\(\\mathbb{E}[Y|X] - m(X) = b\\), podemos expandir o quadrado para: \\[ EQM = \\mathbb{E}[a^2 + 2*a*b + b^2] \\] Substituindo os conteúdos e \\(a\\) e \\(b\\) de volta, temos: \\[ EQM = \\mathbb{E}[(Y - \\mathbb{E}[Y|X])^2 + 2*(Y - \\mathbb{E}[Y|X])*(\\mathbb{E}[Y|X] - m(X)) + (\\mathbb{E}[Y|X] - m(X))^2] \\] Qualquer se seja o valort de \\(m(x)\\) não altera o primeiro termo. Então, se quero achar o menor \\(m(x)\\) posso desprezá-lo. O segundo termo pode ser reescrito como \\(h(x)*e\\), em que \\(h(x) = 2*(Y - \\mathbb{E}[Y|X])\\) e \\(e = \\mathbb{E}[Y|X] - m(X)\\). E já vimos que essa esperança é zero. Portanto, o que realmente importa é escolher \\(m(x)\\) que torne \\(\\mathbb{E}[Y|X] - m(X)\\) mínimo. E o valor que minimizar o EQM é, portanto, \\(m(x) = \\mathbb{E}[Y|X]\\). Como Queríamos Demonstrar. 6.4 Regressão Linear e a CEF O modelo de regressão que iremos rodar com nossos dados pode ser conectado com a CEF em três maneiras diferentes (pelo menos). 6.4.1 Mínimos Quadrados Ordinários O modelo de regreessão linear supõe que queremos achar a melhor reta que se ajusta aos nossos dados. Se tivermos apenas um preditor, \\(X\\) e uma variável a ser predita \\(Y\\), a equação da reta de regressão (populacional) é: \\[ Y = \\alpha + \\beta*X + e \\] Essa reta é chamada de reta de regressão populacional ou até função de regressão populacional. Eu posso ter infinitas combinações de valores de \\(\\alpha\\) e \\(\\beta\\) formando infinitas retas de regressões. Porém, só uma delas me dará a menor soma dos erros quadráticos. Estou interesado em achar quais são esses valores de \\(\\alpha\\) e \\(\\beta\\). Antes de determinar a fórmula para achar esses valores, vamso relembrar como interpretar uma equação da reta. 6.4.1.1 Equação da Reta Na nossa equação da reta, \\(\\alpha\\) é o intercepto, isto é, o ponto onde a reta cruza o eixo \\(y\\) e \\(\\beta\\) é a inclinção ou coeficiente angular da reta. Se nós simularmos dados e plotarmos um scatter plot, veremos que podemos ajustar muitas retas aos dados. set.seed(1234) n &lt;- 1000 x &lt;- rnorm(n) u &lt;- rnorm(n) y &lt;- 2 + 1.3*x + u df &lt;- data.frame(y=y, x=x) df %&gt;% ggplot(aes(x, y)) + geom_point() + geom_smooth(se=F, method=&quot;lm&quot;) + geom_abline(slope= .5, intercept = 1, colour=&quot;red&quot;) + geom_abline(slope= 3, intercept = 3, colour=&quot;green&quot;, size=1) + geom_abline(slope= 0, intercept = 2, colour=&quot;grey&quot;, size=1) No gráfico ??, vemos que a reta em azul é melhor do que a verde e vermelha. Para achar os valores da reta azul (que aqui sabemos ser \\(\\alpha =2\\) e \\(\\beta = 1.3\\)), podemos novamente minimizar o EQM. A derivação requer calcular a derivada ou utilização de álgebra linerar, requisitos matemáticos para além do curso. Para um preditor, a fórmula do \\(\\beta\\) é dada por \\(COV(Y,X)/Var(X)\\) e a constante, \\(\\alpha\\) é dada por \\(\\mathbb{E}[(Y)] - \\beta*\\mathbb{E}[X]\\). Expressando a mesma fórmula com somatório (em vez de esperança), temos: \\[ \\alpha = \\bar{Y} - \\beta*\\bar{X} \\] \\[ \\beta = \\sum\\limits_{i=1}^n{}(y_i - \\bar{y})(x_i - \\bar{x})/\\sum\\limits_{i=1}^n{}(x_i - \\bar{x})^2 \\] Se nós aplicarmos essa fórmula para os dados do gráfico acima, iremos recuperar um valor aproximado da verdadeira reta de regressão (aproximado porque no fundo estamos simulando uma amostra. Mas se tivéssemos infinitos valores, teríamos exatamente os parâmetros populacionais). df %&gt;% summarise(cov_yx = cov(y,x), var_x = var(x), beta = cov_yx/var_x, alpha = mean(y) - beta*mean(x)) %&gt;% kable(digits=3) ## Warning in attr(x, &quot;align&quot;): &#39;xfun::attr()&#39; is deprecated. ## Use &#39;xfun::attr2()&#39; instead. ## See help(&quot;Deprecated&quot;) ## Warning in attr(x, &quot;format&quot;): &#39;xfun::attr()&#39; is deprecated. ## Use &#39;xfun::attr2()&#39; instead. ## See help(&quot;Deprecated&quot;) cov_yx var_x beta alpha 1.349 0.995 1.356 2.016 Vejam que é a mesma estimativa se eu usar a função “lm” do R, que estima uma modelo de regressão linear. lm(y ~x, df) ## ## Call: ## lm(formula = y ~ x, data = df) ## ## Coefficients: ## (Intercept) x ## 2.016 1.356 Agora podemos justificar porque faz sentido ajustar uma reta de regressão para aproximar a CEF. 6.4.2 Suponha que a CEF é Linear (Justificativa I) Se a CEF for de fato linear, então devemos usar a reta de regressão para estimar a CEF. É possível demonstrar esse resultado, mas não vou fazê-lo. Ao leitor interessado, recomendo, por exemplo, o livro Mostly Harmless Econometrics (p. 37), a quem estou seguindo de perto. Quando a CEF será linear? Um dos casos é quando a distribuição conjunta de probabilidade de \\(Y\\) e \\(X\\) for normal (normal bivariada, no caso de duas variáveis, ou multivariada, no caso de muitas variáveis). Foi o que Galton observou em seu estudo sobre a altura de pais e filhos, por exemplo, já que a altura é aproximadamente normal. O outro caso conhecido é o chamado “modelo saturado”. Em um modelo de regressão linear saturado, existe um parâmetro para cada possível valor que os preditores podem assumir. Voltaremos a discutir modelos saturados no futuro e, portanto, postergarei a discussão desse caso. Assim, fora esses dois cenários, não temos muito motivo para supor que a CEF é linear, posto que é improvável a priori que a CEF seja linear em geral. O que nos leva à segunda justificativa. 6.4.3 É o melhor preditor linear do \\(Y\\) (justificativa II) Outra justificativa para a CEF linear é que o modelo linear de regressão gera a melhor previsão possível (no sentido de menor Erro Quadrático Médio) para minha variável dependente, dentre todos os modelos lineares possíveis. Ou seja, se eu minimizar o erro entre o \\(Y\\) observado e minha previsão dada pelo modelo linear \\(\\alpha + \\beta*x\\), em que \\(\\alpha\\) e \\(\\beta\\) são definidos pelas fórmulas derivadas de MQO, terei o menor erro possível. Formalmente estou minimizando \\(\\mathbb{E}[Y - m(x)]\\). 6.4.4 A reta de regressão é a aproximação com o menor erro quadrático médio (justificativa III) Justificativa II e III são parecidas, mas distintas. No primeiro caso estou pensando em termos de previsão. No terceiro, em termos de aproximação. Ou seja, se a CEF for não-linear, e quiser aproximá-la por meio de uma reta, a melhor aproximação possível é por meio da fórmula de regressão. Formalmente, estou minimizando \\(\\mathbb{E}[\\mathbb{E}[Y|X] - m(x)]\\). Matematicamente fica claro que estou falando de coisas distintas (na justificativa II, minimizo a diferença entre \\(Y\\) e \\(m(x)\\), e na justificativa III minimizo a diferença entre \\(\\mathbb{E}[Y|X]\\) e \\(m(x)\\)). Em palavras, aqui estou aproximando a CEf (a esperança condicional), antes eu estava prevendo minha variável dependente. 6.5 Linearidade e Causalidade Toda a discussão de lineraridade é importante pelo motivo óbvio de que estaremos estimando modelos lineares, então é bom ter justificativas para usar modelos lineares para além do fato de que usamos porque aprendemos a usar ou porque é o que sabemos fazer. Mas além disso, estamos muitaz vezes interessados em modelo causais e a pergunta natural é: Sob que condições a CEF nos diz o efeito causal de \\(x\\) sobre \\(y\\)? Voltaremos a esse ponto em um capítulo futuro, uma vez que tenhamos aprendido a estimar modelos de regressão linear para fins de previsão. 6.6 Modelo só com intercepto O exemplo mais simples de modelo de regressão linear é o modelo apenas com intercepto, isto é, sem nenhum preditor \\(X\\). Nesse caso, \\(m(x) = E[Y] = \\mu\\), a média não condicional de \\(Y\\). Escrevendo a equação de regressão, temos: \\[ Y = \\mu + e \\] Com \\(\\mathbb{E}[e] = 0\\). 6.7 Variância da Regressão A variância não condicional do erro da CEF é dada por: \\[ \\sigma^2 = var[e] = \\mathbb{E}[(e - \\mathbb{E}[e])^2] = \\mathbb{E}[e^2] \\] A variância da regressão mede a porção da variância que não é “explicada” ou predita pela esperança condicional, já que é definida pela variância do erro \\(e\\). Além disso, ela depende dos preditores. Se temos preditores diferentes, a varância da regressão será diferente. 6.7.1 Chuva na Jamaica Uma propriedade da variância da regressão é que adicionar preditores não piora a variância e quase sempre melhora (reduz). É uma relação não-monotônica, isto é, a variância com mais preditores é sempre menor ou igual que a variância com menos preditores (se a com mais preditores incluir os mesmos preditores da com menos regressores). Por isso que uma professor meu dizia: se você mediu chuva na Jamaica, pode colocar essa variável como preditora da regressão que isso irá reduzir a variância não explicada. 6.8 Variância condicional Sabemos que a esperança condicional é o melhor preditor que existe. Porém, ainda assim pode ser uma previsão ruim, como vimos no caso dos salários condicional ao gênero. Nesses casos, é útil olhar também para a variância condicional. \\[ \\sigma^2(x) = var[Y|X=x] = \\mathbb{E}[(Y - \\mathbb{E}[Y|X=x])^2] = \\mathbb{E}[e^2|X=x] \\] A variância condicional do erro (ou de \\(Y\\)) em geral depende de \\(X\\). Quando não depende, e portanto é uma constante, dizemos que o erro é homocedástico. Quando depende, que é o caso geral, dizemos que o erro é heterocedástico. 6.9 Efeito Marginal A regressão pode ser intepretada como efeito marginal. Se estivéssemos usando cálculo diferencial, o efeito marginal poderia ser definito como a derivada parcial de um preditor \\(x\\) com relação à variável dependente \\(y\\). Sem cálculo, podemos pensar como o coeficiente angular da variável, ou seja, quanto um aumento em x prediz um aumento em y. Quando temos vários regressoes, eles são mantidos constantes. É o que em economia se chama de ceteris paribus. Isso significa que a intepretação do efeito marginal só mantém constante os regressores incluído na regressão. 6.10 CEF linear com preditores não-lineares Uma aspecto importante que deve fcar cláro que é o modelo de regressão linear que usamos para aproximar a CEF é linear nos parâmetros, mas pode ser não-linear nas variáveis. Assim, a seguinte equação de regressão é linear nos parâmetros: \\[ y = a + b*x + c*x^2 + e \\] Porém, essa outra equação não é linear nos parâmetros: \\[ y = a + b*x + c*x^d + e \\] Vejam que o parâmetro \\(d\\) entra exponenciado e, portanto, não linearmente. "],["estimação.html", "Capítulo 7 - Estimação 7.1 Plug-in estimators 7.2 Simulando de uma Normal Bivariada 7.3 Suposições amostrais de MQO 7.4 Modelo de Média amostral 7.5 Modelo de Regressão Linear com preditores 7.6 Variância do estimador 7.7 Estimando um modelo de regressão real 7.8 Limitações de MQO 7.9 Distrações 7.10 Demonstração Alternativa que estimador é não-viesado.", " Capítulo 7 - Estimação Nós já vimos que é possível mostrar que o preditor linear ótimo com um único regressor possui intercepto \\(\\alpha = \\mathbb{E}[Y] - \\beta*\\mathbb{E}[X]\\) e inclinação \\(\\beta = Cov(X,Y)/Var(X)\\). Isso significa que, se nós considerarmos o modelo de regressão \\(y_i = \\alpha + \\beta*x_i + e_i\\), e usarmos essas fórmulas para calcular os valores de \\(\\alpha\\) e \\(\\beta\\) em uma população, obteríamos uma reta ajustada que é o melhor preditor linear. Até o momento, estávamos trabalhando com a população. Porém, na prática estaremos sempre lidando com uma amostra, de forma que precisamos entender como funciona a estimação de uma regressão a partir de uma amostra. 7.1 Plug-in estimators Se tivermos uma amostra, e não a população, é razoável pensar que uma boa estimativa para os valores populacionais de \\(\\alpha\\) e \\(\\beta\\) são justamente essas fórmulas, calculadas para os dados amostrais. Esse estimador dos parâmetros populacionais nós chamamos de plug-in estimates, pois sem maiores teorias, assumimos que o que vale para a população vale para a amostra. Para diferenciar as estimativas amostrais dos valores populacionais, é comum usarmos \\(\\hat{\\beta}\\) em vez de \\(\\beta\\), ou então letras latinas \\(b\\) em vez das gregas \\(\\beta\\). Vou usar letras gregas com o “chapéu”, mas outros textos utilizam letras latinas . Ou seja, \\(\\hat{\\beta} = \\frac{\\sum{(x_i - \\bar{x})(y_i - \\bar{y})}}{n\\hat{\\sigma_x}^2}\\) No caso da variância amostral de \\(x\\), também é comum designá-la por \\(S_x^2\\). 7.2 Simulando de uma Normal Bivariada Uma distribuição de probabilidade de \\(X\\) e \\(Y\\) é Normal bivariada quando \\(aX + bY\\) é Normal também, para quaisquer \\(a\\) e \\(b\\). Para nós, importa aqui que \\(X\\) e \\(Y\\) são normais cada uma, e a conjunta deles também é Normal. E diferentemente da Normal univariada, preciso representar os parâmetros por meio de vetores. A média, que antes era um escalar, agora será representada por um vetor: \\(\\mu = (\\mu_x, \\mu_y)\\). Mesmo que \\(\\mu_x = \\mu_y\\), ainda assim preciso representar por um vetor. E a variância também não consegue especificartudo que preciso saber sobre a variação nos dados. Eu preciso saber, a variância de \\(X\\), dada por \\(\\sigma_x\\), a variância de \\(Y\\), \\(\\sigma_x\\) e a covariância entre \\(X\\) e \\(Y\\) e \\(Y\\) e \\(X\\), dadas respectivamente por \\(\\mathbb{Cov}[X,Y]\\) e \\(\\mathbb{Cov}[Y,X]\\), que obviamente serão iguais. E nós representamos essas quatro informações por meio de uma matriz, chamada de matriz de covariância ou matriz de variância-covariância: \\[ \\Sigma = \\begin{bmatrix} \\sigma_x^2 &amp; \\mathbb{Cov}[Y,X] \\\\ \\mathbb{Cov}[X,Y] &amp; \\sigma_y^2 \\end{bmatrix} \\] Essa matriz precisa ser positiva-definida, que é uma característica de matrizes, mas que basicamente tem a ver com o fato de que não existe variância negativa e dadas as variâncias de \\(X\\) e \\(Y\\), existem limites para o que a covariância pode ser. Como escolher os valores para essa matriz não é nada intuitivo, vamos trabalhar com a correlação, que temos uma noção melhor. Para isso, utilizaremos a seguinte relação: \\[ \\Sigma = \\begin{bmatrix} \\sigma_x^2 &amp; \\rho \\cdot \\sigma_x \\cdot \\sigma_y \\\\ \\rho \\cdot \\sigma_x\\cdot \\sigma_y &amp; \\sigma_y^2 \\end{bmatrix} \\] Ou seja, a matriz de variância-covariância pode ser parametrizada apenas em função dos desvios-padrões e da correlação entre as duas variáveis. library(ggplot2) library(tidyverse) # vetor de médias vetor_media &lt;- c(0,0) sigma_x &lt;- 2 sigma_y &lt;- 2 rho &lt;- .6 matriz_var_cov &lt;- matrix(c(sigma_x^2, rho*sigma_x*sigma_y, rho*sigma_x*sigma_y, sigma_y^2 ), byrow=T, nrow=2) matriz_var_cov %&gt;% kable() ## Warning in attr(x, &quot;align&quot;): &#39;xfun::attr()&#39; is deprecated. ## Use &#39;xfun::attr2()&#39; instead. ## See help(&quot;Deprecated&quot;) ## Warning in attr(x, &quot;format&quot;): &#39;xfun::attr()&#39; is deprecated. ## Use &#39;xfun::attr2()&#39; instead. ## See help(&quot;Deprecated&quot;) 4.0 2.4 2.4 4.0 set.seed(345) norm_bivariada &lt;- MASS::mvrnorm(n = 10000, mu = vetor_media, Sigma = matriz_var_cov) bivariada_tibble1 &lt;- as_tibble(norm_bivariada, .name_repair = &quot;universal&quot;) %&gt;% rename(x = &#39;...1&#39;, y = &#39;...2&#39;) bivariada_tibble1 %&gt;% ggplot(aes(x)) + geom_density() bivariada_tibble1 %&gt;% ggplot(aes(y)) + geom_density() bivariada_tibble1 %&gt;% ggplot(aes(x, y)) + geom_density_2d() bivariada_tibble1 %&gt;% ggplot(aes(x, y)) + geom_point() Vamos usar esses dados para fazer uma regressão. reg1 &lt;- lm( y ~x , data = bivariada_tibble1) summary(reg1) ## ## Call: ## lm(formula = y ~ x, data = bivariada_tibble1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.7942 -1.0639 -0.0067 1.0740 6.4187 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.014913 0.015907 -0.937 0.349 ## x 0.588264 0.008073 72.870 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.591 on 9998 degrees of freedom ## Multiple R-squared: 0.3469, Adjusted R-squared: 0.3468 ## F-statistic: 5310 on 1 and 9998 DF, p-value: &lt; 2.2e-16 A fórmula do \\(\\beta\\) nós sabemos que depende da covariância entre as variáveis e o desvio-padrão de \\(X\\). Vamos então simular os dados novamente, mudando as correlações e variâncias, para entender como isso impacta a regressão a partir da fórmula da inclinação da reta. library(ggplot2) library(tidyverse) # vetor de médias vetor_media &lt;- c(0,0) # cov menor, dp o mesmo sigma_x &lt;- 2 sigma_y &lt;- 2 rho &lt;- .3 matriz_var_cov &lt;- matrix(c(sigma_x^2, rho*sigma_x*sigma_y, rho*sigma_x*sigma_y, sigma_y^2 ), byrow=T, nrow=2) matriz_var_cov %&gt;% kable() ## Warning in attr(x, &quot;align&quot;): &#39;xfun::attr()&#39; is deprecated. ## Use &#39;xfun::attr2()&#39; instead. ## See help(&quot;Deprecated&quot;) ## Warning in attr(x, &quot;format&quot;): &#39;xfun::attr()&#39; is deprecated. ## Use &#39;xfun::attr2()&#39; instead. ## See help(&quot;Deprecated&quot;) 4.0 1.2 1.2 4.0 set.seed(345) norm_bivariada &lt;- MASS::mvrnorm(n = 10000, mu = vetor_media, Sigma = matriz_var_cov) bivariada_tibble2 &lt;- as_tibble(norm_bivariada, .name_repair = &quot;universal&quot;) %&gt;% rename(x = &#39;...1&#39;, y = &#39;...2&#39;) bivariada_tibble2 %&gt;% ggplot(aes(x, y)) + geom_point() # cov igual, dp de x maior rho &lt;- .3 sigma_x &lt;- 4 matriz_var_cov &lt;- matrix(c(sigma_x^2, rho*sigma_x*sigma_y, rho*sigma_x*sigma_y, sigma_y^2 ), byrow=T, nrow=2) matriz_var_cov %&gt;% kable() ## Warning in attr(x, &quot;align&quot;): &#39;xfun::attr()&#39; is deprecated. ## Use &#39;xfun::attr2()&#39; instead. ## See help(&quot;Deprecated&quot;) ## Warning in attr(x, &quot;align&quot;): &#39;xfun::attr()&#39; is deprecated. ## Use &#39;xfun::attr2()&#39; instead. ## See help(&quot;Deprecated&quot;) 16.0 2.4 2.4 4.0 set.seed(345) norm_bivariada &lt;- MASS::mvrnorm(n = 10000, mu = vetor_media, Sigma = matriz_var_cov) bivariada_tibble3 &lt;- as_tibble(norm_bivariada, .name_repair = &quot;universal&quot;) %&gt;% rename(x = &#39;...1&#39;, y = &#39;...2&#39;) bivariada_tibble3 %&gt;% ggplot(aes(x, y)) + geom_point() # cov maor, dp de x maior rho &lt;- .15 sigma_x &lt;- 4 sigma_y &lt;- 2 matriz_var_cov &lt;- matrix(c(sigma_x^2, rho*sigma_x*sigma_y, rho*sigma_x*sigma_y, sigma_y^2 ), byrow=T, nrow=2) matriz_var_cov %&gt;% kable() ## Warning in attr(x, &quot;align&quot;): &#39;xfun::attr()&#39; is deprecated. ## Use &#39;xfun::attr2()&#39; instead. ## See help(&quot;Deprecated&quot;) ## Warning in attr(x, &quot;align&quot;): &#39;xfun::attr()&#39; is deprecated. ## Use &#39;xfun::attr2()&#39; instead. ## See help(&quot;Deprecated&quot;) 16.0 1.2 1.2 4.0 set.seed(345) norm_bivariada &lt;- MASS::mvrnorm(n = 10000, mu = vetor_media, Sigma = matriz_var_cov) bivariada_tibble4 &lt;- as_tibble(norm_bivariada, .name_repair = &quot;universal&quot;) %&gt;% rename(x = &#39;...1&#39;, y = &#39;...2&#39;) bivariada_tibble4 %&gt;% ggplot(aes(x, y)) + geom_point() O R mostra os resultados da regressão desse jeito, e vamos aprender a interpretá-lo, mas vamos também apresentar em um formato mais amigável e, caso vocês utilizem o rmarkdown, poderão utilizar em trabalhos acadêmicos. Para isso, utilizaremos o pacote stargazer reg2 &lt;- lm( y ~x , data = bivariada_tibble2) reg3 &lt;- lm( y ~x , data = bivariada_tibble3) reg4 &lt;- lm( y ~x , data = bivariada_tibble4) library(&quot;stargazer&quot;) stargazer::stargazer(list(reg1, reg2, reg3, reg4), type = &quot;html&quot;, style = &quot;ajps&quot;, title = &quot;Regressão linear&quot;, omit.stat = &quot;f&quot;, column.labels=c(&quot;cov = 2.4, s_x = 2&quot;, &quot;cov = 1.2, s_x = 2&quot;, &quot;cov = 2.4, s_x = 4&quot;, &quot;cov = 1.2, s_x = 4&quot; )) Regressão linear y cov = 2.4, s= 2 cov = 1.2, s= 2 cov = 2.4, s= 4 cov = 1.2, s= 4 Model 1 Model 2 Model 3 Model 4 x 0.588*** 0.284*** 0.148*** 0.073*** (0.008) (0.010) (0.005) (0.005) Constant -0.015 -0.019 0.014 0.014 (0.016) (0.019) (0.019) (0.020) N 10000 10000 10000 10000 R-squared 0.347 0.081 0.085 0.021 Adj. R-squared 0.347 0.081 0.085 0.021 Residual Std. Error (df = 9998) 1.591 1.892 1.904 1.974 p &lt; .01; p &lt; .05; p &lt; .1 7.3 Suposições amostrais de MQO Quando temos uma amostra, é comum supormos que os dados são independentes e identicamente distribuídos, i.i.d. Esse tipo de amostra às vezes é chamada de amostra aleatória simples. Isso é tipicamente válido em dados de corte transversal (cross-section) como uma pesquisa de opinião (survey) de intenção de voto, amostra pequena (mil, dois mil) de alunos do Brasil a partir de dados doe Censo Escolar e assim por diante. O ponto é que os dados precisam ser dispersos. Se você coletar alunos de uma mesma escola, eles não tenderão a ser independentes. Se amostrar eleitores de um mesmo bairro, tampouco haverá independência. Há formas de lidar com isso, e aprenderemos mais tarde. Por enquanto, tratemos do caso mais simples. A suposição de que dados são i.i.d significa que, em uma amostra com duas variáveis, \\(X\\) e \\(Y\\), o par \\(x_i, y_i\\) é independente de \\(x_j, y_j\\), com \\(i \\neq j\\) e identicamente distribuídos, isto é, com a mesma distribuição. Essa suposição é importante para calcular a variância dos nossos estimadores, não para a estimativa pontual deles. 7.4 Modelo de Média amostral Como vimos, o modelo de regressão mais simples é aquele sem preditores, em que \\(Y = \\mu + e\\). O estimador de mínimos quadrados para \\(\\hat{\\mu}\\) é a média amostral, \\(\\bar{y}\\). Notem que \\(\\mathbb{E}[Y]\\) é a esperança populacional e \\(\\mu\\) é o valor da esperança, já que a esperança do erro \\(e\\) é zero. Vamos então calcular a média de nosso estimador \\(\\bar{y}\\) e sua variância. \\[ \\mathbb{E}[\\bar{y}] = \\mathbb{E}[\\sum{y_i}/n] = (1/n) \\cdot \\sum{\\mathbb{E}[y_i]} = (1/n)\\cdot \\mu \\cdot n = \\mu \\] Portanto, o valor esperando do estimador de mínimos quadrados (a média amostral) é igual à média populacional. Quando isso acontece, isto é, quando a esperança de um estimador é igual ao parâmetro populacional dizemos que o estimador é não-viesado. Definição 7.4. Um estimador \\(\\hat{\\theta}\\) é não-viesado quando \\(\\mathbb{E}[\\hat{\\theta}] = \\theta\\). Agora, vamos calcular a variância da média amostral sob a suposição de que a amostra é i.i.d. Considerando \\(y_i = \\mu + e_i\\), temos: \\[ \\bar{y} = \\sum{y_i}/n = \\sum{(\\mu + e_i)}/n = \\sum{\\mu}/n + \\sum{e_i}/n = \\mu + \\sum{e_i}/n \\] Rearranjando, temos: \\(\\bar{y} - \\mu = \\sum{e_i}/n\\) A variância então pode ser calculada como: \\[ \\mathbb{Var}[\\bar{y}] = \\mathbb{E}[(\\bar{y} - \\mu)^2] \\] \\[ = \\mathbb{E}[( \\sum\\limits_{i=1}^n {e_i}/n)( \\sum\\limits_{j=1}^n{e_j}/n)] = (1/n^2) \\sum\\limits_{i=1}^n{} \\sum\\limits_{j=1}^n{}\\mathbb{E}[e_ie_j] \\] Se nós somarmos o somatório indexado no \\(j\\), o primeiro erro fica constante (indexado no i) e vamos passar por todo os erros de \\(1\\) até \\(n\\). Eventualmente, vamos passar por \\(i\\) e todos os outros diferentes de \\(i\\). Sabendo que os erros são i.i.d, isso significa que erros diferentes são não correlacionados e possuem esperança zero, ou seja, \\(\\mathbb{E}[e_ie_j] = 0, i \\neq j\\), e erros iguais, por serem da mesma distribuição, têm \\(\\mathbb{E}[e_ie_i] = \\sigma^2\\). Então o resultado do último somatório é uma soma de zeros, exceto para o caso em que \\(i = j\\), em que dá \\(\\sigma^2\\). De forma que a equação fica: \\[ (1/n^2) \\sum\\limits_{i=1}^n{}\\sigma^2 = (1/n^2)n\\sigma^2 = \\sigma^2/n \\] Ou seja, a variância de nosso estimador é igual à variância populacional dividida pelo tamanho da amostra. O desvio-padrão (raiz quadrada da variância) amostral do estimador também é chamada de erro padrão. 7.5 Modelo de Regressão Linear com preditores Tendo mostrado que o estimador do modelo com intercepto é não-viesado e calculado a variância amostral para o caso mais simples, sem preditor, vamos agora fazer o mesmo cálculo com um preditor. Vamos assumir que o modelo de regressão linear é uma boa aproximação para CEF. Portanto, vamos supor que: O modelo linear \\(y_i = \\alpha + \\beta x_i + e_i\\) é adequado. \\(\\mathbb{E}[e|X] = 0\\), isto é, o erro é não correlacionado com x. Lembrando que a fórmula da inclinação em uma amostra é: \\(\\hat{\\beta} = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2}\\). Pra simplificar a notação, escreveremos: \\(\\hat{\\beta} = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{nS_x^2}\\) Vamos utlizar o fato de que \\(\\sum_{i=1}^n (x_i - \\bar{x}) = 0\\). Veja que isso é sempre verdade. Para verificar: \\[\\begin{align} \\sum_{i=1}^n (x_i - \\bar{x})\\\\ &amp;= (x_1 - \\bar{x}) + (x_2 - \\bar{x}) + \\ldots + (x_n - \\bar{x}) = \\\\ &amp;= x_1 + x_2 + \\ldots + x_n - n \\bar{x} = \\\\ &amp;= \\sum_{i=1}^n x_i - n \\bar{x} = \\\\ &amp;= n \\bar{x} - n \\bar{x} = 0 \\end{align}\\] Agora, vamos substituir \\(y_i = \\alpha + \\beta x_i + e_i\\) na fórmula do \\(\\hat{\\beta}\\), e depois usar \\(x_i = (x_i - \\bar{x}) + \\bar{x}\\) para obter: \\[\\begin{align} \\hat{\\beta}\\\\ &amp;= \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{nS_x^2} \\\\ &amp;= \\frac{\\sum_{i=1}^n (x_i - \\bar{x})( \\alpha + \\beta x_i + e_i - \\bar{y})}{nS_x^2} \\\\ &amp;= \\frac{\\sum_{i=1}^n (x_i - \\bar{x})( \\alpha) + \\sum_{i=1}^n (x_i - \\bar{x})(\\beta x_i) + \\sum_{i=1}^n (x_i - \\bar{x})e_i + \\sum_{i=1}^n -(x_i - \\bar{x})\\bar{y})}{nS_x^2} \\\\ &amp;= \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(\\beta x_i) + \\sum_{i=1}^n (x_i - \\bar{x})e_i}{nS_x^2} \\\\ &amp;= \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(\\beta ((x_i - \\bar{x}) + \\bar{x}) ) + \\sum_{i=1}^n (x_i - \\bar{x})e_i}{nS_x^2} \\\\ &amp;= \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(\\beta(x_i - \\bar{x}) + \\beta\\bar{x}) ) + \\sum_{i=1}^n (x_i - \\bar{x})e_i}{nS_x^2} \\\\ &amp;= \\frac{\\beta\\sum_{i=1}^n (x_i - \\bar{x})^2 + \\beta\\bar{x}\\sum_{i=1}^n (x_i - \\bar{x}) + \\sum_{i=1}^n (x_i - \\bar{x})e_i}{nS_x^2} \\\\ &amp;= \\frac{\\beta\\sum_{i=1}^n (x_i - \\bar{x})^2 + \\sum_{i=1}^n (x_i - \\bar{x})e_i}{nS_x^2} \\\\ &amp;= \\beta + \\frac{\\sum_{i=1}^n (x_i - \\bar{x})e_i}{nS_x^2} = \\\\ \\end{align}\\] Agora, podemos computar a esperança condicional. \\[\\begin{align} \\mathbb{E}[\\hat{\\beta}|X]\\\\ &amp;= \\mathbb{E}[\\beta + \\frac{\\sum_{i=1}^n (x_i - \\bar{x})e_i}{nS_x^2}|X] \\\\ &amp;= \\beta + \\mathbb{E}[\\frac{\\sum_{i=1}^n (x_i - \\bar{x})e_i}{nS_x^2}|X] \\\\ &amp;= \\beta + \\sum_{i=1}^n (x_i - \\bar{x})\\frac{\\mathbb{E}[e_i|X]}{nS_x^2} \\\\ &amp;= \\beta \\end{align}\\] Veja que podemos usar a LIE para mostrar que a esperança não condicional de \\(\\hat{\\beta}\\) é \\(\\beta\\). A mesma demonstração pode ser feita com uma parametrização alternativa para o modelo de regressão linear em torno da média de \\(x\\). Ao final do capítulo ela é apresentada. 7.6 Variância do estimador Podemos similarmente calcular a variância do nosso estimador, e mostrar que ela é a variância do erro dividida pela variância de \\(x\\) vezes o tamanho da amostra, \\(n\\). Vamos começar com a variância condicional ao \\(X\\). \\[\\begin{align} \\mathbb{Var}[\\hat{\\beta}|X]\\\\ &amp;= \\mathbb{Var}[\\frac{\\beta + n^{-1}\\sum_{i=1}^n({x_i - \\bar{x}})e_i}{S_x^2}|X] \\\\ &amp;= \\mathbb{Var}[\\frac{n^{-1}\\sum_{i=1}^n({x_i - \\bar{x}}) e_i}{S_x^2}|X] \\text{, somar uma constante não altera a variância} \\\\ &amp;= \\frac{1}{S_x^4}\\mathbb{Var}[n^{-1}\\sum_{i=1}^n({x_i - \\bar{x}}) e_i|X] \\text{, multiplicação por uma constante sai da variância ao quadrado} \\\\ \\end{align}\\] É possível mostrar que dá para tirar para fora da esperança \\(n^{-1}\\sum_{i=1}^n({x_i - \\bar{x}})\\). Isso não é exatamente trivial e vou pular a derivação de que isso está certo. E se suposermos homocedasticidade, \\(\\mathbb{Var}[e_i|X] = \\sigma^2\\), então: \\[\\begin{align} \\frac{1}{S_x^4}\\mathbb{Var}[n^{-1}\\sum_{i=1}^n({x_i - \\bar{x}}) e_i|X] \\\\ &amp;= \\frac{1}{n^2S_x^4}\\sum_{i=1}^n({x_i - \\bar{x})^2}\\mathbb{Var}[e_i|X] \\\\ &amp;= \\frac{1}{nS_x^2}\\mathbb{Var}[e_i|X] = \\frac{\\sigma^2}{nS_x^2} \\text{, supondo homocedasticidade} \\end{align}\\] Ou seja, a variância de nosso estimador é a variância do erro dividida pela variância de \\(x\\) multiplicada pelo tamanho da amostra. Isso significa que quando mais disperso o \\(Y\\), maior a variância do meu estimador, quanto maior a amostra, menor a variância e, por fim, quanto mais espalhado o \\(X\\), menor a variância do estimador. Por fim, o erro padrão de uma estimativa é apenas o desvio padrão da variância. Portanto: \\[ se(\\hat{\\beta}) = \\frac{\\sigma}{\\sqrt{nS^2_x}} \\] 7.7 Estimando um modelo de regressão real Vamos começar com um exemplo, importando dados do site Base de Dados, que possui muitas bases de dados públicas, já tratadas. Para tanto, precisaremos criar um projeto no Google cloud, conforme os passos aqui: https://basedosdados.github.io/mais/access_data_packages/ Depois, só escolher quais dos dados iremos olhar https://basedosdados.org/ Escolhi mexer com dados do censo escolar # instalando a biblioteca # install.packages(&#39;basedosdados&#39;) # carregando a biblioteca na sessão library(basedosdados) library(bigrquery) # para importar os dadosdiretamente no R, precisamos criar um id para acessar a base de dados set_billing_id(&quot;aula-reg-manoel&quot;) #bq_auth() # se autenticação falhar # importando dados knitr::include_graphics(here(&quot;imagens&quot;, &quot;autenticacao base dados.jpg&quot;)) query &lt;- &quot;SELECT * FROM `basedosdados.br_inep_censo_escolar.escola` where sigla_uf = &#39;AL&#39; and id_municipio = &#39;2704302&#39; and ano = 2019&quot; escola &lt;- basedosdados::read_sql(query) # query &lt;- &quot;SELECT count(*) as contagem, id_municipio FROM `basedosdados.br_inep_censo_escolar.escola` where sigla_uf = &#39;AL&#39; and ano = 2019 group by id_municipio&quot; # escola &lt;- read_sql(query) query &lt;- &quot;SELECT * FROM `basedosdados.br_inep_censo_escolar.dicionario`&quot; dicionario &lt;- basedosdados::read_sql(query) query &lt;- &quot;SELECT * FROM `basedosdados.br_inep_censo_escolar.matricula` where sigla_uf = &#39;AL&#39; and id_municipio = &#39;2704302&#39; and ano = 2019&quot; aluno &lt;- basedosdados::read_sql(query) # dicionário de gênero e raça # dicionario %&gt;% # dplyr::filter(id_tabela == &quot;matricula&quot;, nome_coluna %in% c(&quot;sexo&quot;, &quot;raca_cor&quot;)) aluno_gen &lt;- aluno %&gt;% dplyr::filter(regular == 1) %&gt;% group_by(id_escola, sexo) %&gt;% summarise(num_aluno_gen = n()) %&gt;% mutate(sexo = gsub(&quot;1&quot;, &quot;Male&quot;, sexo), sexo = gsub(&quot;2&quot;, &quot;Female&quot;, sexo)) %&gt;% mutate(total = sum(num_aluno_gen), percent = num_aluno_gen/total) %&gt;% filter(sexo == &quot;Female&quot;) %&gt;% rename(percent_female = percent) %&gt;% dplyr::select(id_escola, percent_female) aluno_raca &lt;- aluno %&gt;% dplyr::filter(regular == 1) %&gt;% group_by(id_escola, raca_cor) %&gt;% summarise(num_aluno_raca = n()) %&gt;% mutate(raca_cor = gsub(&quot;1&quot;, &quot;Branca&quot;, raca_cor), raca_cor = gsub(&quot;2&quot;, &quot;Preta&quot;, raca_cor), raca_cor = gsub(&quot;3&quot;, &quot;Parda&quot;, raca_cor)) %&gt;% mutate(total = sum(num_aluno_raca), percent = num_aluno_raca/total) %&gt;% filter(raca_cor %in% c(&quot;Branca&quot;, &quot;0&quot;, &quot;Preta&quot;, &quot;Parda&quot;)) %&gt;% mutate(raca_cor = gsub(&quot;0&quot;,&quot;não_declarado&quot;, raca_cor)) %&gt;% dplyr::select(id_escola, raca_cor, percent) %&gt;% pivot_wider(names_from = raca_cor , values_from = percent ) aluno_escola &lt;- aluno_gen %&gt;% inner_join(aluno_raca, by = &quot;id_escola&quot;) %&gt;% inner_join(escola, by = &quot;id_escola&quot;) aluno_escola_reg &lt;- aluno_escola %&gt;% ungroup() %&gt;% mutate(negra = Preta + Parda) %&gt;% dplyr::select(negra, percent_female, tipo_localizacao, agua_potavel, esgoto_rede_publica, lixo_servico_coleta, area_verde, biblioteca, quantidade_profissional_psicologo) %&gt;% filter(across(everything(), ~!is.na(.))) %&gt;% mutate_if(bit64::is.integer64, as.factor) reg &lt;- lm(negra ~ percent_female + tipo_localizacao + agua_potavel + esgoto_rede_publica + lixo_servico_coleta + area_verde + biblioteca + quantidade_profissional_psicologo, data= aluno_escola_reg) summary(reg) ## ## Call: ## lm(formula = negra ~ percent_female + tipo_localizacao + agua_potavel + ## esgoto_rede_publica + lixo_servico_coleta + area_verde + ## biblioteca + quantidade_profissional_psicologo, data = aluno_escola_reg) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.46731 -0.16217 -0.01934 0.17534 0.55176 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.070584 0.204310 0.345 0.72991 ## percent_female 0.400443 0.256195 1.563 0.11880 ## tipo_localizacao2 -0.251352 0.224599 -1.119 0.26373 ## agua_potavel1 0.067306 0.052684 1.278 0.20212 ## esgoto_rede_publica1 -0.069037 0.022951 -3.008 0.00279 ** ## lixo_servico_coleta1 0.144155 0.158554 0.909 0.36378 ## area_verde1 0.047138 0.025681 1.836 0.06714 . ## biblioteca1 0.009462 0.022531 0.420 0.67473 ## quantidade_profissional_psicologo1 -0.069969 0.033593 -2.083 0.03787 * ## quantidade_profissional_psicologo2 -0.120828 0.092870 -1.301 0.19396 ## quantidade_profissional_psicologo3 -0.306991 0.130754 -2.348 0.01935 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2222 on 418 degrees of freedom ## Multiple R-squared: 0.07237, Adjusted R-squared: 0.05017 ## F-statistic: 3.261 on 10 and 418 DF, p-value: 0.0004498 7.8 Limitações de MQO Os estimadores de MQO, nossos \\(\\hat{\\beta}\\) e \\(\\hat{\\alpha}\\), são não-viesados e conseguimos derivar a variância amostral deles. Porém, não há muito mais que possamos dizer sobre eles a partir apenas das suposições que fizemos, ou seja, de que o modelo linear (nos parâmetros) é adequado, e que a espereança do erro, condicional ao preditor, é zero (e assumimos, em certo momento, homecedasticidade). O que não podemos dizer a mais que gostaríamos de poder fazer? Uma aspecto importante da inferência é, por exemplo, calcular o intervalo de confiança. Para isso, contudo, precisaremos de mais suposições do que o que fizemos até aqui. Portanto, tanto cálculos de intervalo de confiança como o p-valor e condução de testes de hipótese estão vedados com as suposições de MQO. 7.9 Distrações Quando nós imprimimos o resultado de uma regressão no R, além dos coeficientes e o erro padrão, há uma série de outras informações apresentadas. Muitas delas são apenas distrações, mas como existem e muita gente ainda as utiliza (às vezes com bom uso), vou falar rapidamente de uma delas, o que já é possível com nosso conhecimento atual. 7.9.1 R-quadrado Uma das estatísticas apresentadas é o chamado \\(R^2\\), também denominado coeficiente de determinação, que pode ser definido de várias maneiras distintas e uma delas é a razão entre a variância dos valores preditos \\(\\hat{Y}\\) e a variância dos dados observados, \\(Y\\): \\[ R^2 \\equiv \\frac{\\hat{\\sigma}^2_{\\hat{y}}}{\\hat{\\sigma}^2_y} \\] Uma definição alternartiva é que o R-quadrado é a razão entre a covariância de \\(y\\) e \\(\\hat{y}\\) e a variância de \\(Y\\): \\[ R^2 \\equiv \\frac{\\mathbb{Cov}[Y,\\hat{Y}]}{\\hat{\\sigma}^2_y} \\] Por fim, o R-quadrado pode ser escrito como o quadrado da covariância amosrtral entre \\(X\\) e \\(Y\\) dividido pelo produto dos desvios-padrãos amostrais de \\(X\\) e \\(Y\\). \\[ R^2 \\equiv \\left(\\frac{\\hat{\\mathbb{Cov}}[X,Y]}{\\hat{\\sigma}_x \\cdot \\hat{\\sigma}_y}\\right)^2 \\] Essa última parametrização é o que permite interpretar o R-quadrado como a correlação (entre \\(X\\) e \\(Y\\)) ao quadrado. É fácil ver também que uma regressão de \\(X\\) sobre \\(Y\\) gera o mesmo R-quadrado que uma regressão de \\(Y\\) sobre \\(X\\). 7.9.2 R-quadrado ajustado O estimador \\(\\hat{\\sigma^2}\\) é viesado com relação ao parâmetro \\(\\sigma^2\\). Então, o R-quadrado ajustado usa \\(\\frac{n \\cdot \\hat{\\sigma^2}}{n-2}\\) como estimador de \\(\\sigma^2\\). Uma interpretação dada é que adicionar preditores no modelo em geral reduz a variância “não-explicada” \\(\\sigma^2\\) (lembrem-se do “chuva na Jamaica”). E isso induz um viés da estimativa do \\(R^2\\), que é corrigida pelo desconto no número de preditores + 1. 7.9.3 Limites do R-quadrado Pela primeira parametrização, é fácil ver que o \\(R^2\\) jamais será negativo, pois a variância é sempre não-negativa. Além disso, jamais será mais que 1, pois a variância dos valores preditors será no máximo igual à variância observada, quando as previsões forem perfeitas. 7.9.4 Para que serve? Por muito tempo foi utilizada como uma medida do ajuste do modelo e, portanto, quanto maior o R-quadrado, melhor o modelo (ou quanto maior o R-quadrado ajustado). Hoje em dia, contudo, sabemos que é uma péssima medida de ajuste do modelo. O modelo pode estar correto e produzir um R-quadrado pequeno, a depender da variância de \\(Y\\). O modelo pode estar totalmente errado e o R-quadrado ser próximo de 1. Quando o verdadeiro modelo é não-linear e a variância de \\(X\\) é muito grande, isso pode acontecer se a reta de regressão ajustada não tiver \\(\\beta = 0\\). Há também quem use o \\(R^2\\) como medida de capacidade preditiva do modelo. Porém, quanto mais espalhado o \\(X\\), melhor o \\(R^2\\). A simulação abaixo mostra isso: set.seed(123) x1 &lt;- rnorm(1000, 0, 10) x2 &lt;- rnorm(1000, 0, 2) y1 &lt;- 2 + 3*x1 + rnorm(1000, 0, 2) y2 &lt;- 2 + 3*x2 + rnorm(1000, 0, 2) summary(lm(y1 ~x1)) ## ## Call: ## lm(formula = y1 ~ x1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.651 -1.279 -0.062 1.318 6.839 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.960390 0.061904 31.67 &lt;2e-16 *** ## x1 2.996186 0.006245 479.81 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.957 on 998 degrees of freedom ## Multiple R-squared: 0.9957, Adjusted R-squared: 0.9957 ## F-statistic: 2.302e+05 on 1 and 998 DF, p-value: &lt; 2.2e-16 summary(lm(y2~x2)) ## ## Call: ## lm(formula = y2 ~ x2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -6.2418 -1.2558 0.0156 1.3276 5.8205 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.98227 0.06285 31.54 &lt;2e-16 *** ## x2 2.99309 0.03111 96.21 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.986 on 998 degrees of freedom ## Multiple R-squared: 0.9027, Adjusted R-squared: 0.9026 ## F-statistic: 9256 on 1 and 998 DF, p-value: &lt; 2.2e-16 Não é possível comparar o \\(R^2\\) entre amostras distintas, como a simulação acima também mostra. O modelo é o mesmo, mas os dados são distintos e produzem \\(R^2\\) distintos. Não faz sentido comparar o \\(R^2\\) entre transformações da nossa variável dependente (com e sem log, por exemplo). Quando a amostra é a mesma e o \\(y\\) é o mesmo, é possível comparar modelos com o \\(R^2\\) e um R-quadrado maior significa um Erro Quadrático Médio menor. Mas nesse caso, é melhor usar direto o EQM. É fácil ver, por tudo isso, que o \\(R^2\\) não é uma medida da “variância explicada”, com ou sem aspas. Eu não consigo entender o que é “explicar”, assim com aspas, e como é distinto de sem aspas. Portanto, melhor seria não usar e deixar de lado. Mas como é usado e os softwares mostram, tenho de perder tempo falando sobre ele para vocês. 7.10 Demonstração Alternativa que estimador é não-viesado. Para isso, vamos usar nosso truque matemático de adicionar e subtrair Lembremos que a fórmula para o \\(\\hat{\\beta}\\) é \\(\\mathbb{Cov}[X,Y]/\\mathbb{Var}[X]\\). Notem que: \\[ \\hat{\\beta} = \\frac{\\sum({x_i - \\bar{x}}) \\cdot ({y_i - \\bar{y}})}{\\sum({x_i - \\bar{x}})^2} = \\frac{\\sum({x_i - \\bar{x}}) \\cdot ({y_i - \\bar{y}})}{n \\cdot S^2_x} = \\frac{n^{-1}\\sum({x_i - \\bar{x}}) \\cdot ({y_i - \\bar{y}})}{\\cdot S^2_x} \\] Vamos então usar essa última parametrização para reescrever minha equação do \\(\\hat{\\beta}\\) e então mostrar que ele é não-viesado e calcular sua variância. Substituindo nossa equação de regressão na fórmula, temos: \\[ \\hat{\\beta} = \\frac{n^{-1}\\sum({x_i - \\bar{x}}) \\cdot ({y_i - \\bar{y}})}{S^2_x} = \\\\ \\frac{\\sum{(x_i - \\bar{x}) \\cdot y_i - (x_i - \\bar{x})\\bar{y}}}{n \\cdot S^2_x} = \\\\ \\frac{n^{-1}\\sum({x_i - \\bar{x}}) \\cdot y_i - \\sum({x_i -\\bar{x}}) \\cdot\\bar{y}}{S^2_x} \\] Vamos agora usar o fato de que, para qualquer variável \\(z\\), a diferença média de \\(z\\) para a média amostral de \\(\\bar{z}\\) é zero, isto é, \\(n^{-1}\\sum\\limits_{i=1}^n{}z_i - \\bar{z} = 0\\) Por fim, segue-se disso que, para qualquer constante \\(w\\) que não varia com \\(i\\), podemos escrever: \\(n^{-1} \\cdot \\sum\\limits_{i=1}^n{}(z_i - \\bar{z})w = 0\\) Portanto, \\(n^{-1} \\cdot \\sum({x_i -\\bar{x}})\\bar{y} = 0\\). Logo, \\[ \\hat{\\beta} = \\frac{n^{-1}\\sum({x - \\bar{x}})y_i}{S^2_x} \\] Lembremos que, \\(y_i = \\alpha + \\beta x_i + e_i\\). Se eu adicionar e subtrair \\(\\beta\\bar{x}\\) da equação, não altero ela. Utilizando esse truque e rearranjando, temos o seguinte: \\[ y_i = \\alpha + \\beta X_i + e_i = \\\\ \\alpha + \\beta\\bar{x} + \\beta X_i - \\beta\\bar{x} + e_i = \\\\ \\alpha + \\beta\\bar{x} + \\beta(x_i - \\bar{x}) + e_i \\] Substituindo na equação anterior, temos: \\[ \\hat{\\beta} = \\frac{n^{-1}\\sum({x_i - \\bar{x}})(\\alpha + \\beta\\bar{x} + \\beta(x_i - \\bar{x}) + e_i)}{S^2_x} \\] \\[ \\hat{\\beta} = \\frac{ n^{-1} \\cdot (\\alpha + \\beta)\\bar{x}\\sum({x_i - \\bar{x}}) + n^{-1}\\sum({x_i - \\bar{x}}) \\cdot [\\beta \\cdot (x_i - \\bar{x}) + e_i]}{S^2_x} \\] \\[ \\hat{\\beta} = \\frac{ n^{-1} \\cdot (\\alpha + \\beta) \\cdot \\bar{x}\\sum({x_i - \\bar{x}}) + n^{-1} \\beta\\sum(x_i - \\bar{x})^2 + n^{-1} \\sum({x_i - \\bar{x}})e_i} {S^2_x} \\] A primeira soma é uma constante, vezes o somatório de \\(x_i\\) menos sua média, o que é zero. Posso então eliminar da equação, resultando em: \\[ \\hat{\\beta} = \\frac{\\beta \\cdot n^{-1} \\cdot\\sum({x_i - \\bar{x}})^2 + n^{-1}\\sum({x_i - \\bar{x}})e_i}{S^2_x} \\] Agora, a primeira soma é a variância amostral de \\(x\\) multiplicada por \\(\\beta\\). Reescrevendo a equação de outro modo, temos: \\[ \\hat{\\beta} = \\frac{\\beta \\cdot S^2_x}{S^2_x} + \\frac{n^{-1}\\sum({x_i - \\bar{x}})e_i}{S^2_x} \\] Simplificando, chegamos a: \\[ \\hat{\\beta} = \\beta + \\frac{n^{-1}\\sum({x_i - \\bar{x}})e_i}{S^2_x} \\] Portanto, nós mostramos que o estimador \\(\\hat{\\beta}\\) pode ser decomposto no parâmetro populacional \\(\\beta\\) mais uma soma ponderada do termo de erro. Uma vez que nossa amostra é i.i.d., sabemos que é uma média ponderada dos erros não-correlacionados. Podemos agora mostrar que o estimador é não-viesado. Essa interpretação dos erros como uma média ponderada é útil para nós aqui por causa do seguinte. Se eu chamar meus pesos (a ponderação) de \\(a_i\\), posso pensar que a esperança da média ponderada de erros não-correlacionados pode ser pensada como: \\[ \\mathbb{E}[ \\sum{a_ie_i}] = \\mathbb{E}[a_1 \\cdot e_1 + a_2 \\cdot e_2 + ... + a_n \\cdot e_n] = a_1 \\cdot \\mathbb{E}[e_1] + a_2 \\cdot \\mathbb{E}[ e_2] + ... + a_n \\cdot \\mathbb{E}[ e_n] \\] \\[ \\mathbb{E}[ \\sum{a_ie_i}] = \\sum{a_i} \\cdot \\mathbb{E}[e_i] \\] Primeiro, lembremos nossa suposição de que \\(\\mathbb{E}[e|X] = 0\\). Isso significa que, para qualquer valor de \\(x\\) que eu condicionar, a esperança do erro é zero. Portanto, se a esperança do erro é sempre zero, temos: \\[ n^{-1}\\sum({x_i - \\bar{x}}) \\cdot \\mathbb{E}[e] = 0 \\] \\[ \\mathbb{E}[\\hat{\\beta}] = \\beta + \\mathbb{E}[ \\frac{ \\sum({x_i - \\bar{x}})e_i}{n \\cdot S^2}] = \\\\ \\beta +\\frac{\\sum({x_i - \\bar{x}})}{S^2_x} \\cdot \\mathbb{E}[e] = \\beta \\] "],["causalidade.html", "Capítulo 8 - Causalidade 8.1 Resultados Potenciais 8.2 Modelo estrutural e modelo de regressão 8.3 Simulação de Resultados Potenciais e Causalidade 8.4 Referências", " Capítulo 8 - Causalidade Até o momento estivemos estudando regressão em um sentido puramente preditivo (ou descritivo). Mas com frequência nosso interesse está em utilizar regressão para inferir causalidade. E uma regressão pode ser interpretada causalmente quando a CEF for causal. Obviamente, essa resposta é insatisfatória, pois a próxima pergunta é: quando a CEF é causal? A resposta é: A CEF é causal quando ela descreve diferenças nos resultados ponteciais (potencial outcomes) em uma dada população alvo. Vamos então definir causalidade a partir da noção de resultados potenciais, para em seguida ver como podemos utilizar regressão para fazer inferência causal. 8.1 Resultados Potenciais Digamos que estamos interessados em estimar o efeito causal de pertencer à base do governo sobre o percentual de votos de um deputado que será conforme indicado pelo líder do governo. Donald Rubin, em uma série de artigos nos anos 70, introduziu a noção de resultados potenciais a partir da noção mais familiar de contrafactual. Imagine um deputado que está na oposição e ele passa a fazer parte do governo. Se nós pudéssemos observar uma série de votações tanto no mundo factual em que é do governo, quando no contrafactual em que não é do governo, teríamos o efeito causal para aquele deputado de fazer parte do governo sobre seu percentual de votação em acordo com a indicação do líder do governo. Porém, não é possível observar ambos os mundos, o factual e o contrafactual ao mesmo tempo, o que é conhecido como o problema fundamental da inferência causal. Se pudéssemos observar duas terras paralelas, exatamente iguais, exceto por esse único fato, conseguiríamos calcular o efeito causal. Seja \\(Y_{i}(1)\\) o percentual de votação potencial do deputado \\(i\\) quando ele é da base do governo (indicado pelo índice “1”) e seja \\(Y_{i}(0)\\) o percentual de votação potencial do deputado \\(i\\) quando ele não é da base do governo (indicado pelo índice “0”). \\(Y_{i}(1) - Y_{i}(0)\\) é o efeito causal para \\(i\\) de fazer parte da base do governo e é o que gostaríamos de saber para cada deputado. Se eu pudesse observar essa diferença para cada deputado, poderia calcular o Efeito Causal Médio, \\(\\mathbb{E}[Y_{i}(1) - Y_{i}(0)]\\), em inglês, Average Causal Effect (ACE). Seja \\(B_i\\) uma variável que indica se um deputado é ou não da base do governo (\\(1\\) se é, \\(0\\) caso contrário). O mundo factual (observado) pode ser descrito em termos dos resultados potenciais pela eqquação abaixo. É importante enfatizar que essa equação conecta os resultados observados com os resultados potenciais. Vale notar também a diferença de notação entre um resultado observado (indexado apenas o indivíduo \\(i\\)) e um potencial (indexado para o indivíduo \\(i\\) e para o tratamento potencialmente recebido). \\[\\begin{equation} \\tag{8.1} Y_i = Y_{i}(0) + (Y_{i}(1) - Y_{i}(0))*B_i \\end{equation}\\] A equação nos lembra também que nós só podemos observar um dos resultados potenciais, associado a ser ou não da base do governo, mas não ambos ao mesmo tempo. Veja que se para um indivíduo \\(i\\) ele é da base do governo, isto é, \\(B_i = 1\\), então tenho para este individuo em particularque \\(Y_i = Y_{i}(1)\\), isto é, o resultado potencial coincide com o resultado observado. Essa igualdade é na verdade uma suposição, chamada de consistência. Consistência: Essa suposição implica que o resultado potencial de um indivíduo quando recebe um tratmento é igual ao observado na realidade. Em outras palavras, consistência significa que o que você observa é o que você supunha que aconteceria se a pessoa recebesse o tratamento. Para imaginar que consistência falhasse, imagine quando um deputado \\(i\\) distante ideologicamente do governo vira da base do governo ele, buscando reduzir dissonância cognitiva, muda sua ideologia para algo mais proximo do governo. Já um deputado mais alinhado não precisa fazer isso. Então, na verdade temos dois “tratamentos” acontecendo, e não um, e a hipótese de consistência não será verificada. 8.1.1 Decomposição do Efeito Causal A equação 8.1 conecta resultados observados com resultados potenciais. Isso é particularmente útil quando quisermos estimar o efeito causal a prtir de dados observacionais. Digamos que tenho dados sobre deputados que fazem parte da base do governo e que não fazem. Nós iremos mostrar agora porque correlação (associação) não implica causalidade. Digamos que queremos computar a média percentual de votação conforme indicação do governo para deputados da base e fora da base. Isso é dado pela equação 8.2 abaixo. \\[\\begin{equation} \\tag{8.2} \\mathbb{E}[Y_{i}|B_i = 1] - \\mathbb{E}[Y_{i}| B_i = 0] \\end{equation}\\] Veja que por enquanto estamos tratando apenas de dados observados e, portanto, não podemos falar de dados potenciais. Contudo, se a suposição de consistência for válida, podemos utilizar a equação 8.1 para conectar o factual e o contrafactual. \\[\\begin{equation} \\begin{aligned} \\mathbb{E}[Y_{i}|B_i = 1] = \\mathbb{E}([Y_{i}(0) + (Y_{i}(1) - Y_{i}(0))*B_i)|B_i=1] \\\\ \\mathbb{E}[Y_{i}|B_i = 1] = \\mathbb{E}[(Y_{i}(0) + (Y_{i}(1) - Y_{i}(0))*1)|B_i=1] \\\\ \\mathbb{E}[Y_{i}|B_i = 1] = \\mathbb{E}[Y_{i}(1)|B_i=1] \\end{aligned} \\end{equation}\\] E, similarmente, \\[\\begin{equation} \\mathbb{E}[Y_{i}|B_i = 0] = \\mathbb{E}[Y_{i}(0)|B_i=0] \\end{equation}\\] De modo que a equação 8.2 pode ser reescrita do seguinte modo (supondo consistência): \\[\\begin{equation} \\mathbb{E}[Y_{i}|B_i = 1] - \\mathbb{E}[Y_{i}| B_i = 0] = \\mathbb{E}[Y_{i}(1)|B_i=1] - \\mathbb{E}[Y_{i}(0)| B_i = 0] \\end{equation}\\] Agora, podemos utilizar aquele truque de subtrair e somar a mesma coisa que não altera a equação. Em particular, somar um resultado pottencial contrafactual, isto é, o que em média um deputado que faz parte da base do governo faria se não estivesse no governo (denotado por \\(Y_{i}(0)|B_i = 1\\)). Em vermelho está o truqe matemático de subtrair e somar a mesma coisa sem alterar a equação. \\[\\begin{equation} \\begin{aligned} \\mathbb{E}[Y_{i}|B_i = 1] - \\mathbb{E}[Y_{i}| B_i = 0] = \\\\ \\mathbb{E}[Y_{i}(1)|B_i=1] \\color{red}{- \\mathbb{E}[Y_{i}(0)|B_i = 1] + \\mathbb{E}[Y_{i}(0)|B_i = 1]} - \\mathbb{E}[Y_{i}(0)| B_i = 0] \\end{aligned} \\end{equation}\\] Rearranjando a equação e utilizando as propriedades do operador esperança ,temos: \\[\\begin{equation} \\tag{8.3} \\begin{aligned} \\mathbb{E}[Y_{i}|B_i = 1] - \\mathbb{E}[Y_{i}(0)|B_i=0] = \\\\ \\color{green}{\\mathbb{E}[Y_{i}(1) - Y_{i}(0)|B_i = 1]} + \\color{blue}{\\mathbb{E}[Y_{i}(0)|B_i = 1] - \\mathbb{E}[Y_{i}(0)| B_i = 0]} \\end{aligned} \\end{equation}\\] A equação 8.3 apresenta em verde o que chamamos de efeito Médio do Tratamento entre as pessoas Tratadas (em inglês, Average effect of the Treatment on the Treated, ATT) e em azul o viés de seleção. O ATT é o efeito médio porque estamos calculando a média (esperança), do efeito do Tratamento por causa do termo \\(Y_{i}(1) - Y_{i}(0)\\), e entre as pessoas Tratadas, isto é, que pertencem à base do governo, por que condiciona em \\(B_i = 1\\). E a parte em azul é o viés de seleção porque reflete a diferença média (no percentual de votos seguindo o governo) entre as pessoas que escolheram ser da base do governo \\(|B_i = 1\\) e as que escolheram não ser \\(|B_i = 0\\) no mundo de resultados potenciais em que ambos não foram tratados (\\(Y_{i}(0)\\)). Ou seja, se pudéssemos observar o mundo contrafactual em que deputados da base estão fora do governo e computar o percentual médio de voto deles seguindo o governo, e comparar com o observado para os deputados fora da base, se houver diferença, isso é o viés de seleção. Quando o viés de seleção é diferente de zero, então a nossa comparação ingênua da diferença média de comportamento entre governo e oposição não recupera o efeito causal médio. Por exemplo, se deputados de esquerda possuem maior chance de fazer parte do governo Lula e, pelo fato de serem mais próximos ideologicamente, tendem a votar com o governo por esse motivo, então teriam um comportamento médio diferente da oposição, mesmo se não estivessem na base do governo. Nesse exemplo, a diferença média entre governo e oposição superestima o efeito causal médio de ser da base do governo. Esse resultado permite visualizar também porque, em um contexto experimental, podemos garantir que o viés de seleção é zero em média. Se cada pessoa é atribuída ao tratamento ou controle aleatoriamente (exemplo fictício, se der cara, é da base do governo, se der coroa, não é da base), então não há diferença média entre um grupo e outro. Obviamente, temos de supor que há compliance perfeito (isto é, um Bolsonarista alocado aleatorimnte para compor o governo deveria aceitar fazê-lo e, similarmente, um petista alocado na oposição deveria aceitar isso e serem ambos tratados assim pelo governo). Quando o compliance não é perfeito, há formas de tratar isso, mas se torna mais complicado. Em estudos observacionais, o mecanismo de alocação de pessoas entre tratamento e controle não está sob controle do pesquisador e, portanto, não podemos garantir que o viés de seleção é zero. Em um contexto dos assim chamados experimentos naturais ou quase experimentos, a aleatorização existe, mas não é feita pelo pesquisador (caso de uma política governamental usada segundo alguma regra aleatória, como no caso do draft do Vietnam) ou a aleatorização não existe, mas é plausível supor que, para um certo subconjunto dos dados, a alocação entre tratamento e controle é como se fosse aleatório (exemplo, quando da introdução da urna eletrônica de acordo com tamanho da população, cidades com um eleitor acima do ponto de corte versus com um abaixo possui diferença aleatória em relação a, por exemplo, percentual de votos nulos. Então é possível estimar o efeito causal da urna eletrônica sobre percentual de votos nulos nesse subgrupo de cidades próximas do ponto de corte). E em contextos mais gerais de estudos observacionais, para além de contextos quasi-experimentais, o efeito causal só pode ser recuperado (identificado) se a suposição de Conditional Independence Assumption (CIA) for válida. Essa suposição diz que, condicional a variáveis de controle, não há relação entre a alocação de tratamento e os resultados potenciais. No nosso exemplo, isso significa que, condicional a, por exemplo, a ideologia do deputado, não há correlação entre alocação para tratamento e controle e resultados potenciais. Ou seja, o viés de seleção seria zero em média. Se houver alguma variável que causa quem éselecionado (ou se seleciona) entre tratamento e controle e tmbém causa os resultados potenciais, o viés de seleção não é zero e o efeito causal não é estimável. Por exemplo, digamos que a execução de emendas parlamentares fosse diferente entre governo e oposição e que essa variável também influencia a votação. Ao não controlar para essa variável, a suposição de CIA não é atendida. 8.2 Modelo estrutural e modelo de regressão A distinção entre CEF e CEF causal é um ponto muito importante mas no geral negligenciado. Chen &amp; Pearl (2013), ao avaliaram livros de econometria, notaram que a maioria não fazia a distinção adequada entre modelo de regressão e modelo estrutural de regressão. Como a compreensão desse ponto é crítica para qualquer análise causal com regressão e frequentemente não feita, quero insistir nesse aspecto. Vamos recapitular o que nós aprendemos sobre regressão e sobre causalidade. O erro da regressão (na população) é definido como a diferença entre o \\(Y\\) observado e a esperança condicional \\(\\mathbb{E}[Y|X]\\). Uma consequência dessa definição é que o erro é não correlacionado com o regressor \\(X\\), sejam os dados observacionais ou não. A regressão linear pode ser jusitifcada de três modos distintos: 1. se a CEF for linear (como em um modelo saturado); 2. se queremos minimizar o erro quadrático médio das previsões; 3. é a melhor aproximação linear que existe para a CEF, seja linear ou não. Em estudos observacionais, a análise de regressão linear identifica o modelo causal se o viés de seleção for zero. Isso requer que a suposição de indepenência (ou independência condicional) seja verdade, isto é, que a correlação entre o erro e os regressores seja zero. Ou seja, é uma suposição do modelo (e não uma propriedade ou consequência da definição do erro). Para a suposição ser plausível, precisamos ter um modelo estrutural (como o de Resultados Potenciais) em que seja plausível (é uma suposição não testável) a independência condicional. Muitos livros textos de regressão não diferenciam as propriedades da CEF e as suposições de um modelo estrutural (causal). Vamos ilustrar, por meio de um exemplo, o ponto de como é possível o erro da CEF ser não correlacionado com o regressor e o modelo estrutural ter erro e regressor correlacionados (e portanto a causalidade não identificada se não controlarmos para variável que garanta independência condicional. 8.2.1 Exemplo de CEF e CEF estrutural Suponha que estou interessado em estimar o efeito causal de uma variável, que vou designar por tratamento. Meus dados são observacionais e há viés de seleção nos dados. Uma outra variável, \\(X\\), causa tanto o tratamento quando minha variável dependente, \\(Y\\). Porém, condicional a \\(X\\), o tratamento é independente dos resultados potenciais, ou seja, é válida a suposição de independência condicional. Vou mostrar que a o erro da CEF é independente dos regressores e, por outro lado, uma regressão sem controlar para \\(X\\) tem resíduo correlacionado com o regressor e, portanto, não identifica o efeito causal. Por fim, uma regressão que controle para \\(X\\) identifica o efeito causal. 8.3 Simulação de Resultados Potenciais e Causalidade Vamos rodar uma simulação para ilustrar o ponto. ## potential outcomes n1 &lt;- 5000 tau &lt;- 2 # ATE Average Treatment Effect (ATE) or Average Causal Effect (ACE) alpha &lt;- 1 eta_0 &lt;- rnorm(n1) eta_1 &lt;- rnorm(n1) ## n &lt;- n1+n1 x &lt;- rnorm(n1) gamma &lt;- 2 nu_0 &lt;- rnorm(n1) nu_1 &lt;- rnorm(n1) eta_0 &lt;- gamma*x + nu_0 eta_1 &lt;- gamma*x + nu_1 treatment &lt;- rep(1:0, each=n1) # potential outcomes y_1 &lt;- alpha + tau*treatment[1:n1] + eta_1 y_0 &lt;- alpha + tau*treatment[(n1+1):n] + eta_0 # ATT ou ACE mean(y_1 - y_0) ## [1] 1.994582 amostra &lt;- sample(1:length(treatment), size = 1000) df &lt;- data.frame(y = c(y_1, y_0)[amostra], x=c(x, x)[amostra], treatment = treatment[amostra]) reg &lt;- lm(y ~ 1, data=df) summary(reg) ## ## Call: ## lm(formula = y ~ 1, data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -7.7216 -1.6394 0.0018 1.5643 7.5036 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.93528 0.07762 24.93 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.454 on 999 degrees of freedom reg1 &lt;- lm(y ~ x, data=df) summary(reg1) ## ## Call: ## lm(formula = y ~ x, data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.8776 -1.0204 -0.0698 1.0136 4.4301 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.91234 0.04484 42.65 &lt;2e-16 *** ## x 1.99457 0.04465 44.67 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.418 on 998 degrees of freedom ## Multiple R-squared: 0.6666, Adjusted R-squared: 0.6662 ## F-statistic: 1995 on 1 and 998 DF, p-value: &lt; 2.2e-16 ## Ignorability apenas com CIA (x) library(arm) x &lt;- rnorm(n) treatment &lt;- ifelse(x &gt; 0, 1,0) gamma &lt;- 2 tau &lt;- -2 # ATE Average Treatment Effect (ATE) or Average Causal Effect (ACE) alpha &lt;- 1 nu &lt;- rnorm(n) eta &lt;- gamma*x + nu y &lt;- alpha + tau*treatment + eta df &lt;- data.frame(y=y, x=x, treatment = treatment) reg3 &lt;- lm(y ~ treatment, data=df) summary(reg3) ## ## Call: ## lm(formula = y ~ treatment, data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -7.5315 -1.0682 -0.0096 1.0557 7.1952 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.57690 0.02206 -26.15 &lt;2e-16 *** ## treatment 1.21599 0.03147 38.63 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.573 on 9998 degrees of freedom ## Multiple R-squared: 0.1299, Adjusted R-squared: 0.1298 ## F-statistic: 1493 on 1 and 9998 DF, p-value: &lt; 2.2e-16 reg4 &lt;- lm(y ~ treatment + x, data=df) summary(reg4) ## ## Call: ## lm(formula = y ~ treatment + x, data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.9749 -0.6685 -0.0045 0.6599 3.7366 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.00922 0.01872 53.91 &lt;2e-16 *** ## treatment -1.99776 0.03234 -61.77 &lt;2e-16 *** ## x 2.01140 0.01608 125.12 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.9823 on 9997 degrees of freedom ## Multiple R-squared: 0.6609, Adjusted R-squared: 0.6608 ## F-statistic: 9743 on 2 and 9997 DF, p-value: &lt; 2.2e-16 library(tidyverse) library(ggplot2) df %&gt;% group_by(treatment) %&gt;% mutate(uncond_y = mean(y), erro = (y - uncond_y), sqs = sum(erro)^2) %&gt;% ggplot(aes(x, erro )) + geom_point() df %&gt;% mutate(bol_x = ifelse(x &gt; 0 ,1,0)) %&gt;% group_by(treatment, bol_x) %&gt;% mutate(cond_y = mean(y)) %&gt;% ungroup() %&gt;% mutate(erro = (y - cond_y), sqs = sum(erro)^2) %&gt;% ggplot(aes(x, erro )) + geom_point() # -5 ocorre 10%,02 10% etc. até 5, 10% das vezes x &lt;- sample(1:11, size=n, replace=T) - 6 treatment &lt;- ifelse(x &gt; 0, 1,0) gamma &lt;- 2 tau &lt;- -2 # ATE Average Treatment Effect (ATE) or Average Causal Effect (ACE) alpha &lt;- 1 nu &lt;- rnorm(n) eta &lt;- gamma*x + nu y &lt;- alpha + tau*treatment + eta df %&gt;% mutate(bol_x = ifelse(x &gt; 0 ,1,0)) %&gt;% group_by(treatment, bol_x) %&gt;% mutate(cond_y = alpha + .5*tau + gamma*x ) %&gt;% ungroup() %&gt;% mutate(erro = (y - cond_y), sqs = sum(erro)^2) %&gt;% ggplot(aes(x, erro )) + geom_point() df %&gt;% mutate(bol_x = ifelse(x &gt; 0 ,1,0), cond_y = alpha + tau*treatment + gamma*x, erro = (y - cond_y), sqs = sum(erro)^2) %&gt;% ggplot(aes(y=erro, x=x )) + geom_point() + geom_smooth(method=&quot;lm&quot;) df %&gt;% mutate(bol_x = ifelse(x &gt; 0 ,1,0), cond_y = alpha + tau*treatment + gamma*x, erro = (y - cond_y), sqs = sum(erro)^2) %&gt;% summarise(cor(erro, x)) ## cor(erro, x) ## 1 0.01257919 8.4 Referências Chen, B., &amp; Pearl, J. (2013). Regression and causation: a critical examination of six econometrics textbooks. Real-World Economics Review, Issue, (65), 2-20. "],["mle.html", "Capítulo 9 - MLE 9.1 Estimação por Máxima Verossimilhança 9.2 Teste de hipótese 9.3 regressão", " Capítulo 9 - MLE 9.1 Estimação por Máxima Verossimilhança Além do método de Mínimos Quadrados Ordinários, podemos estimar os coeficientes de regressão pelo método de Máxima Verossimilhança, que é uma tradução ruim do inglês de Maximum Likelihood. Para dar uma intuição do método de MLE, vamos considerar um caso mais simples. Suponha que coletamos uma amostra aleatória simples de \\(n\\) observações, em que perguntamos a intenção de voto para segundo turno de uma eleição, e estamos interessados em estimar a proporção de votos válidos na candidata \\(A\\). A proporção (na população) é dada pelo número de sucessos dividido pelo total de observações, \\(p\\). Quero portanto estimar \\(p\\) a partir da minha amostra. Podemos modelar esse dado como se cada observação, que pode ser \\(1\\) se vota na candidata \\(A\\), e \\(0\\) caso contrário, como repetições de uma Bernoulli. Cada observação \\(i\\) é \\(1\\) (chamadada de “sucesso”) com probabilidade \\(p\\), e \\(0\\) (chamada de “fracasso”) com probabilidade \\(1-p\\). Seha \\(x\\) o número de sucesso, \\(n-x\\) o número de fracassos e a resposta de cada pessoa dada por \\(x_i = {1,0}\\). Suponha que observo a primeira pessoa, então, \\(Pr(x_i = 1) = p\\) e analogamente, \\(Pr(x_i = 0) = 1 -p\\) A probabilidade de obter \\(x\\) sucessos em \\(n\\) observações é a probabilidade conjunta, dada por: \\[ p^{x_1}(1-p)^{1 - x_1}\\cdot p^{x_2}(1-p)^{1 - x_2} ... \\cdot p^{x_n}(1-p)^{1 - x_n} \\] \\[ \\prod_{i=1}^n p^{x_i}(1-p)^{1 - x_i} = p^{\\sum_1^n x_i}(1-p)^{\\sum_1^n (1 - x_i)} = p^x(1-p)^{n-x} \\] Note que há várias maneiras disso acontecer, um sucesso na pimeira observação, ou na segunda etc. Portanto, preciso multiplicar a probabilidade pelas possibilidades disso acontecer, que é dada por \\({n\\choose x}\\). Então, a rigor deveria colocar \\({n\\choose x}\\) na fórmula, ou seja, \\(Pr(X = x) = {n\\choose x}p^x(1-p)^{n-x}\\). Essa fórmula, com \\(n\\) e \\(x\\) observados é a verossmilhança. E a ideia é escolher um estimador de \\(p\\) que maximize essa função, para os dados observados (\\(n\\) e \\(x\\)). É possível mostrar que a função é máxima quando \\(p = x/n\\). Ou seja, a verossimilhança é maximizada. Esse é o estimador de máxima verossimilhaça para uma proporção de uma distribuição binomial. Para uma regressão, é algo similar. Eu vou supor uma distribuição de probabilidade para os dados \\(y\\) (Normal, por exemplo) e, dada uma amostra, derivo um estimador para os parÂmetros \\(\\alpha\\) e \\(\\beta\\). Em relação ao estimador de MQO, na prática estamos acrescentando uma suposição ao nosso modelo de regressão: Os erros \\(e_i \\sim N(0, \\sigma^2)\\) e são independentes de \\(X\\). Os erros são independentes entre observações. Uma consequência dessas suposições é que a própria variável resposta \\(Y\\) torna-se independente entre observações, condicional aos preditores. Podemos então computar a probabilidade condicional de observar os dados de nossa amostra \\(y_i\\), condicional aos \\(x_i\\) e aos parâmetros da equação de regressão, \\(\\alpha\\) e \\(\\beta_1\\) e a variância \\(\\sigma^2\\). Como as observações são independentes, a probabilidade conjunta é o produto das probabilidades marginais. A verossimilhança é uma função dos parâmetros para essa probabilidade conjunta. Então, meu objetivo é escolher valores dos coeficientes que maximizem a probabilidade de observar os dados. Com a suposição de normalidade dos erros acima, é possível mostrar que os estimadores de Máxima Verossimilhança são iguais aos de MQO. A vantagem de estimar por Máxima Verossimilhança é que agora temos uma distribuição amostral para nossos estimadores e, portanto, podemos fazer inferências sobre ele. É possível mostrar que: \\[ \\hat{\\beta_1} \\sim N(\\beta_1, \\frac{\\sigma^2}{n \\cdot s_x}) \\] Notem que a variância é supostamente conhecida, o que na prática raramente será verificado. Voltaremos a isso na aula de inferência. 9.2 Teste de hipótese Nesse caso, o típico teste de hipótese que é feito em um modelo de regressão é testar a hipótese nula de que \\(\\beta_1 = 0\\). Sob a suposição de que isso é verdade, \\(\\hat{\\beta_1} \\sim N(0, \\frac{\\sigma^2}{n \\cdot s_x})\\). E se quisermos cometer o erro de rejeitar a hipótese nula (isto é, rejeitar que \\(\\beta_1 = 0\\)) no máximo 5% das vezes, se eu colhesse novas amostras repetidas vezes, então posso calcular se rejeito ou não minha hipótese nula ao nível de 5% de confiança. Vamos ver um exemplo a partir de dados de um survey no Reino Unido sobre o Brexit. ## Warning in attr(x, &quot;align&quot;): &#39;xfun::attr()&#39; is deprecated. ## Use &#39;xfun::attr2()&#39; instead. ## See help(&quot;Deprecated&quot;) ## Warning in attr(x, &quot;format&quot;): &#39;xfun::attr()&#39; is deprecated. ## Use &#39;xfun::attr2()&#39; instead. ## See help(&quot;Deprecated&quot;) variable_id item_text vote vote: intenção de voto no referendo do Brexit: ‘sair’, ‘ficar’, ‘não sabe’, ‘não votará’ leave leave: identifica quem vai votar ‘sair’ (1) ou ‘ficar’ (0). Não sabe ou não irá votar é NA education education: 1 = sem qualificação, 2 = ensino médio, 3 = mais que ensino médio, 4 = ensino superior, 5 = pós-graduação, NA = sem resposta age idade 9.3 regressão reg &lt;- lm(leave ~ age, data=bes, na.action = na.omit ) stargazer::stargazer(reg, type = &quot;html&quot;, style = &quot;ajps&quot;, title = &quot;Regressão linear - Brexit&quot;, omit.stat = &quot;f&quot;) Regressão linear - Brexit leave age 0.007*** (0.0002) Constant 0.119*** (0.009) N 28044 R-squared 0.058 Adj. R-squared 0.058 Residual Std. Error 0.485 (df = 28042) p &lt; .01; p &lt; .05; p &lt; .1 Nós vemos que pessoas mais velhas tenderiam a votar 1 (sair). 20*.007*100 # 20 anos ## [1] 14 40*.007*100 # 40 anos ## [1] 28 80*.007*100 # 40 anos ## [1] 56 O p-valor é a probabilidade de eu observar dados tão ou mais extremos do que o observado, sob a suposição de que a hipótese nula é verdadeira. O erro padrão é \\(.0002\\), ou seja, supondo normalidade (MLE), o p-valor é aproximadamente zero: z &lt;- coef(reg)[2]/summary(reg)$coefficients[2 , 2] print(z) ## age ## 41.44094 p_valor &lt;- 1 - pnorm(z) print(p_valor) ## age ## 0 hist(rnorm(10000)) E se o p-valor é menor que o nível de significância (5%), então rejeitamos a hipótese nula de que o coeficiente é zero. O problema desse tipo de teste de hipótese é que, à medida que o número de observaçoes cresce, sempre vamso rejeitar a hipótese nula. É muito raro que em ciências sociais o coeficiente de uma variável seja exatamente zero. Nesse sentido, é melhor utilizar o intervalo de confiança para quantificar a incerteza. Aqui, podemos calculá-lo do seguinte modo: \\(\\hat{\\beta_1} -1.96 \\cdot se; \\hat{\\beta_1} + 1.96 \\cdot se = [0.007 - 1.96 \\cdot 0.002; 0.007 + 1.96 \\cdot 0.002] = [0.003; 0.01]\\). 40*.007*100 # 40 anos estimativa pontual ## [1] 28 40*.003*100 # 40 anos limite inferior ## [1] 12 40*.01*100 # 40 anos limite superior ## [1] 40 Na prática podemos calcular de cabeça multiplicando por \\(2\\) o erro-padrão e vendo se o coeficiente cruza o zero quando somo ou subtraio. "],["checagem.html", "Capítulo 10 - Checagem 10.1 Resíduos 10.2 Modelo no R 10.3 Normalidade dos resíduos 10.4 Generalização", " Capítulo 10 - Checagem Antes de proceder com a parte de inferência estatística, vamos apresentar como realizar checagem do modelo. A razão é que, em geral, a teoria de inferência depende da suposição do modelo ser correta. Então, faz sentido primeiro checar se o modelo satisfaz os pressupostos, e depois fazer inferência. Vamos então apresentar os principais testes e checagem que devemos fazer com nosso modelo de regressão linear. Aqui vale uma comentário sobre inferência Bayesiana, que não estamos utilizando em nosso curso, mas que eu particularmente utilizo em minha pesquisa aplicada. O prática de modelagem padrão na inferência Bayesiana é escrever formalmente uma verossimilhança (como fizemos com MLE) e estimar distribuições de porbabilidades para os parâmetros, e checar o modelo para ver se essas distribuições fazem sentido. Portanto, a checagem do modelo acontece automaticamente e de maneira integrada, ao contrário do que faremos aqui. Ou seja, recomendo que vocês aprendam a fazer inferência Bayesiana e prescindam de toda essa maquinaria que irei apresentar. 10.1 Resíduos Os resíduo para cada observação \\(i\\) é a diferença entre a previsão do modelo de regressão \\(\\hat{y_i}\\) e o valor observado \\(y_i\\), as vezes chamado de \\(\\hat{e}\\), para diferenciar do erro populacional, \\(e\\). No caso de nosso modelo de regressão linear com um único preditor, temos: \\[ \\hat{e_i} = y_i - (\\hat{\\alpha} + \\hat{\\beta} \\cdot x_i) \\] Eis algumas propriedade dos resíduos. Os resíduos deveriam ter esperança zero, condicional aos preditores. Formalmente, \\(\\mathbb{E}[\\hat{e}|X=x] = 0\\). Isso sempre será verdade, e é uma consequência de como calculamos o estimador de mínimos quadrados. Se estivermos supondo homecedasticidade, devem ter variância constante (o que raramente será o caso). Por se tratar de uma amostra, a probabilidade dos resíduos serem completamente não-correlacionados entre si é zero, contudo a correlação deve ser baixa e convergir para zero à medida que \\(n\\) cresce para infinito. Se estamos supondo que o erro é Gaussiano (Normal), como no modelo de MLE, os resíduos devem também ser normais. Cada uma dessas propriedades nos leva a um diagnóstico ou checagem. library(ggplot2) library(knitr) library(tidyverse) library(electionsBR) library(readr) library(here) library(tidyr) 10.2 Modelo no R Para fazer os testes do nosso modelo, vamos fazer um modelo preditivo. Vou utilizar a votação no primeiro turno presidencial de 2018 para prever o voto no segundo turno, no estado de Alagoas. Para tanto, vamos baixar os dados de votação presidencial de 2018 ao nível de seção eleitoral do portal de dados abertos do TSE. Podem baixar o arquivo clicando no link do dataset. Vamos deszipar e depois filtrar os dados para o estado de Alagoas (para ter uma base de dados pequena). library(data.table) # lista o nome do arquivo em csv # unzip(here(&quot;dados&quot;, &quot;votacao_secao_2018_BR.zip&quot;), list = TRUE) #read data1.csv into data frame presid_al18 &lt;- fread(here(&quot;dados&quot;,&quot;votacao_secao_2018_BR.csv&quot;), encoding = &quot;Latin-1&quot;) # filtrando só AL presid_al18 &lt;- presid_al18 %&gt;% filter(SG_UF == &quot;AL&quot;) # modelo voto em Bolsonaro 1t prediz voto no 2t # descobre o que é voto nulo e branco presid_al18 %&gt;% group_by(NM_VOTAVEL) %&gt;% summarise(max(NR_VOTAVEL)) ## # A tibble: 15 × 2 ## NM_VOTAVEL `max(NR_VOTAVEL)` ## &lt;chr&gt; &lt;int&gt; ## 1 ALVARO FERNANDES DIAS 19 ## 2 BENEVENUTO DACIOLO FONSECA DOS SANTOS 51 ## 3 CIRO FERREIRA GOMES 12 ## 4 FERNANDO HADDAD 13 ## 5 GERALDO JOSÉ RODRIGUES ALCKMIN FILHO 45 ## 6 GUILHERME CASTRO BOULOS 50 ## 7 HENRIQUE DE CAMPOS MEIRELLES 15 ## 8 JAIR MESSIAS BOLSONARO 17 ## 9 JOSE MARIA EYMAEL 27 ## 10 JOÃO DIONISIO FILGUEIRA BARRETO AMOEDO 30 ## 11 JOÃO VICENTE FONTELLA GOULART 54 ## 12 MARIA OSMARINA MARINA DA SILVA VAZ DE LIMA 18 ## 13 VERA LUCIA PEREIRA DA SILVA SALGADO 16 ## 14 VOTO BRANCO 95 ## 15 VOTO NULO 96 # 95 e 96 presid_al18_valido &lt;- presid_al18 %&gt;% filter(!NR_VOTAVEL %in% c(95,96)) %&gt;% group_by(NR_SECAO,NR_ZONA, CD_MUNICIPIO,NM_MUNICIPIO, NR_TURNO, NR_VOTAVEL ) %&gt;% summarise(validos = sum(QT_VOTOS)) %&gt;% mutate(bol_bolsonaro = NR_VOTAVEL == 17, validos_bolsonaro = sum(validos*bol_bolsonaro)) %&gt;% summarise(total_validos = sum(validos), validos_bolsonaro = max(validos_bolsonaro), perc_bolsonaro = validos_bolsonaro/total_validos) %&gt;% dplyr::select(-total_validos) %&gt;% pivot_wider(id_cols = c(NR_SECAO, NR_ZONA, CD_MUNICIPIO, NM_MUNICIPIO), names_from = NR_TURNO, values_from = perc_bolsonaro) %&gt;% rename(perc_bolso_turno1 = &#39;1&#39;, perc_bolso_turno2 = &#39;2&#39;) # modelo de regressão reg1 &lt;- lm(perc_bolso_turno2 ~ perc_bolso_turno1, data = presid_al18_valido) summary(reg1) ## ## Call: ## lm(formula = perc_bolso_turno2 ~ perc_bolso_turno1, data = presid_al18_valido) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.147833 -0.018587 -0.000424 0.018027 0.170229 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.0011688 0.0008273 1.413 0.158 ## perc_bolso_turno1 1.1640700 0.0022516 516.986 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.02885 on 6385 degrees of freedom ## Multiple R-squared: 0.9767, Adjusted R-squared: 0.9767 ## F-statistic: 2.673e+05 on 1 and 6385 DF, p-value: &lt; 2.2e-16 presid_al18_valido %&gt;% ggplot(aes(x=perc_bolso_turno1, y=perc_bolso_turno2)) + geom_point() + geom_abline(slope = coef(reg1)[2] , intercept = coef(reg1)[1], colour = &quot;blue&quot;) 10.2.1 Resíduos contra o preditor Se os resíduos devem ter \\(\\mathbb{E}[\\hat{e}|X=x] = 0\\), então para cada \\(x_i\\) os resíduos devem ter média zero. Em um gráfico, isso significa que, se eu tiver pontos suficientes na proximidade de cada \\(x_i\\), a dispersão dos resíduos deve ser aleatória, no sentido de não ter padrão claro, o que vai implicar uma reta horizontal cuja média é zero. Como vimos, essa esperança condicional será sempre zero. df &lt;- data.frame(residuos = residuals(reg1), preditor = presid_al18_valido$perc_bolso_turno1) df %&gt;% ggplot(aes(x=preditor, y = residuos)) + geom_point() + geom_smooth(method=&quot;lm&quot;, se=F) Se houver outros preditores, faça a mesma coisa e o gráfico também deve ser uma reta em cima do eixo \\(x\\). O que seria um exemplo de esperança condicional zero, mas que mostra um problema nos pressupostos dos modelos? Se a dispersão ao redor da reta não for aleatória. set.seed(123) n &lt;- 1000 df_nl &lt;- data.frame(x = rnorm(n), e = rnorm(n)) df_nl &lt;- df_nl %&gt;% mutate(y = 2 + 1.5*x - .5*x^2 + e) reg_nl &lt;- lm(y ~x, df_nl) df &lt;- data.frame(residuos = residuals(reg_nl), preditor = df_nl$x) df %&gt;% ggplot(aes(x=preditor, y = residuos)) + geom_point() + geom_smooth(method=&quot;lm&quot;, se=F) Outra possibilidade são variáveis omitidas. Se o preditor for relevante, a esperança condicional contra esse preditor omitido não será zero. Por outro lado, se for irrelevante, será zero. set.seed(1234) n &lt;- 1000 df_nl &lt;- data.frame(x1 = rnorm(n), x2 = rnorm(n), x3 = rnorm(n), e = rnorm(n)) df_nl &lt;- df_nl %&gt;% mutate(y = 2 + 1.5*x1 - .5*x2 + e) reg_nl &lt;- lm(y ~x1, df_nl) df &lt;- data.frame(residuos = residuals(reg_nl), preditor2 = df_nl$x2, preditor3 = df_nl$x3) df %&gt;% ggplot(aes(x=preditor2, y = residuos)) + geom_point() + geom_smooth(method=&quot;lm&quot;, se=F) df %&gt;% ggplot(aes(x=preditor3, y = residuos)) + geom_point() + geom_smooth(method=&quot;lm&quot;, se=F) 10.2.2 Magnitude dos Resíduos Contra o Preditor Uma vez que \\(\\mathbb{E}[\\hat{e}|X] = 0\\), segue-se que \\(\\mathbb{Var}[\\hat{e}|X] = \\mathbb{E}[\\hat{e}^2|X]\\). Seja \\(A\\) uma v.a. Então, \\(\\mathbb{Var}[A|X] = \\mathbb{E}[(A - \\mathbb{E}[A|X])^2|X]\\) \\[ \\mathbb{Var}[\\hat{e}|X] = \\mathbb{E}[(\\hat{e} - \\mathbb{E}[\\hat{e}|X])^2|X] = \\mathbb{E}[(\\hat{e} - 0)^2|X] = \\mathbb{E}[\\hat{e}^2|X] \\] Portanto, se estamos assumindo homecadisticidade, isto é, que \\(\\mathbb{Var}[\\hat{e}|X] = \\sigma^2\\), podemos checar esta suposição olhando para a esperança condicional do quadrado dos resíduos. E podemos fazer isso plotando o gráfico do quadrado dos resíduos contra o preditor. É útil olhar tanto a reta ajustada quanto um modelo não-linear, disponível no comando geom_smooth do ggplot2. df &lt;- data.frame(residuos_sq = residuals(reg1)^2, preditor = presid_al18_valido$perc_bolso_turno1) df %&gt;% ggplot(aes(x=preditor, y = residuos_sq)) + geom_point() + geom_smooth(method=&quot;lm&quot;, se=F) df %&gt;% ggplot(aes(x=preditor, y = residuos_sq)) + geom_point() + geom_smooth(se=F) A reta deve ser horizontal, e não deveria ter mais pontos acima do que abaixo da reta. A altura da reta deveria ser aproximadamente o EQM. Conjuntos de pontos persistentemente acima ou abaixo da reta são sinais de problema com alguma de nossas hipóteses, ou há heterocedasticidade (a variância muda com o preditor) ou a forma funcional do modelo está errada de algum modo. No meu gráfico, a reta não é exatamente horizontal, mas parece “good enough”. Uma forma de olhar isso é computar erros padrões que lidem com a heterocedasticidade. Voltaremos a isso mais à frente. Vale perceber também que existem alguns outliers no quadrado dos resíduos, que poderiam ser investigados qualitativamente, para entender o que está causando isso e pode sugerir algum preditor ou forma funcional não considerados na modelagem. Ás vezes quando temos resíduos muito grandes, o quadrado deles fica gigante e o gráfico fica não-informativo (especialmente quando o modelo é problemático). Nesses casos, podemos plotar o valor absoluto dos resíduos, em vez do quadrado. df &lt;- data.frame(residuos_abs = abs(residuals(reg1)), preditor = presid_al18_valido$perc_bolso_turno1) df %&gt;% ggplot(aes(x=preditor, y = residuos_abs)) + geom_point() + geom_smooth(method=&quot;lm&quot;, se=F) 10.2.3 Resíduos com dados Temporais e/ou Espaciais Com frequência nossos dados possuem uma certa ordem, seja temporal e/ou espacial. Em um modelo simples, isso não deveria importar (nossa amostra supostamente é i.i.d, e os erros populacionais são não correlacionados entre si). Se os erros forem correlacionados, podemos usar algo chamado mínimos quadrados generalizados, o que também não iremos cobrir no curso. Então, deveríamos sempre fazer um plot dos resíduos contra o tempo, ou contra o espaço. Aqui, como a ordem é apenas espacial, vamos fazer isso. Podemos fazer um plot dos resíduos contra seção e contra os dados ordenados aleatoriamente. Vejamos como fica. presid_al18_valido &lt;- presid_al18_valido %&gt;% mutate(id_secao = paste0(NR_SECAO, NR_ZONA , CD_MUNICIPIO)) df &lt;- data.frame(residuos = residuals(reg1), id = as.numeric(presid_al18_valido$id_secao)) df %&gt;% ggplot(aes(x=id, y = residuos)) + geom_point() + geom_smooth(method=&quot;lm&quot;, se=F) df %&gt;% ggplot(aes(x=id, y = residuos)) + geom_point() + geom_smooth( se=F) df &lt;- data.frame(residuos = residuals(reg1), id = as.numeric(presid_al18_valido$id_secao)) df_randomized &lt;- df %&gt;% sample_n(nrow(df)) %&gt;% mutate(new_id = 1:n()) df_randomized %&gt;% ggplot(aes(x=new_id, y = residuos)) + geom_point() + geom_smooth(method=&quot;lm&quot;, se=F) O que os dois plots mostram é que existe um padrão na forma como o TSE numera as seções, mas não há padrão espacial claro na votação. Uma técnica simples, que é uma versão simplificada de algo mais formal chamado “permutation test”, é plotar os resíduos permutados aleatoriamente contra o plot real que queremos fazer. Vamos usar o pacote nullabor para isso. Vamos replicar o teste de homecedasticidade e olhar os resíduos ao quadrado contra os preditores, e olhar os resíduos contra os valores previstos Primeiro contra o preditor. library(nullabor) set.seed(1234) # Aleatoriza do mesmo jeito sempre elec_reg &lt;- data.frame(presid_al18_valido, .resid = residuals(reg1), .predicted = model.matrix(reg1)[,2]) shuffled_residuals &lt;- lineup(null_lm(perc_bolso_turno2 ~ perc_bolso_turno1, method = &quot;rotate&quot;), true = elec_reg, n = 9) ggplot(shuffled_residuals, aes(x = .predicted, y = .resid^2)) + geom_point() + facet_wrap(vars(.sample)) Depois contra os valores previstos. elec_reg &lt;- data.frame(presid_al18_valido, .resid = residuals(reg1), .fitted = fitted(reg1)) shuffled_residuals &lt;- lineup(null_lm(perc_bolso_turno2 ~ perc_bolso_turno1, method = &quot;rotate&quot;), true = elec_reg, n = 9) ## decrypt(&quot;ve5B DEyE l6 GuClylu6 dT&quot;) ggplot(shuffled_residuals, aes(x = .fitted, y = .resid)) + geom_point() + facet_wrap(vars(.sample)) A mensagem encriptada tem o plot real, contrao o nulo. Se for possível identificar o plot real, é porque existe algum padrão nos dados. 10.3 Normalidade dos resíduos Se estamos supondo normalidade dos resíduos, podemos verificar se são de fato normais. Uma forma de verificar é plotando o histograma dos resíduos e por cima a densidade de uma distribuição normal com média zero (resíduos têm média zero) e desvio-padrão igual ao desvio-padrão dos resíduos. df &lt;- data.frame(residuos = residuals(reg1), preditor = presid_al18_valido$perc_bolso_turno1, density_points = rnorm(length(residuals(reg1)) , 0, sd(residuals(reg1)))) print(sd(residuals(reg1))) ## [1] 0.02884366 df %&gt;% ggplot(aes(residuos)) + geom_histogram(aes(y=..density..)) + geom_density(aes(density_points), colour = &quot;blue&quot;) Uma alternativa mais tradicional são os chamados Q-Q plots, cujo nome deriva de sere um plot “quantil-quantil”. A Função de Distribuição Acumulada (FDA) é \\(F(x) = P(X \\leq x)\\). Se a função é contínua (como a Normal), então ela possui uma inversa: \\(F^{-1}(p)\\), que nos diz o único \\(x\\) tal que \\(P(X \\leq x) = p\\). Ou seja, a função quantil me diz qual é o ponto que divide a minha distribuição de probabilidade em exatamente \\(p\\%\\). Por exemplo, posso me perguntar qual ponto divide meus dados exatamente no meio (mediana), ou no primeiro quartil, primeiro percentil, em suma, em qualquer percentual da distribuição. O R tem uma função para isso que vale vermos para entender. x &lt;- rnorm( 1000) q50 &lt;- quantile(x, .5) q025 &lt;- quantile(x, .025) q975 &lt;-quantile(x, .975) print(c(q50, q025, q975)) ## 50% 2.5% 97.5% ## -0.04257219 -1.91943889 1.83665303 y &lt;- rnorm(1000) df &lt;- data.frame(y=y, x=x) # plot de x contra y df %&gt;% ggplot(aes(x=x, y=y)) + geom_point() # plot de x ordenado contra y ordenado df %&gt;% ggplot(aes(x=sort(x), y=sort(y))) + geom_point() + geom_abline(intercept = 0, slope = 1, colour =&quot;blue&quot;) Vejam que \\(p\\) varia entre \\(0\\) e \\(1\\), e o menor valor que a função quantil retorna é o menor valor possível da minha distribuição de probabilidade, e o maior é o máximo da distribuição de probabilidade. No caso da Normal, o mínimo é menos infinito e o máximo é mais infinito, porém, em uma amostra serão valores finitos. Se plotarmos duas variáveis ordenadas, podemos verificar se têm a mesma distribuição. Em caso positivo, deveriam seguir uma linha reta de 45 graus. Quando queremos testar os resíduos, uma das variáveis são os resíduos e a outra a distribuição teórica dos mesmos (Normal). O Q-Q plot de uma regressão é uma versão um pouco mais complicadinha disso, que não irei mostrar aqui (pois precisaria trabalhar com a inversa da distribuição Normal), mas a lógica final é a mesma. Podemos ver isso plotando o Q-Q plot do R, e um construído por nós, utilizando a lógica que apresentei. df &lt;- data.frame(residuos = residuals(reg1), preditor = presid_al18_valido$perc_bolso_turno1, density_points = rnorm(length(residuals(reg1)) , 0, sd(residuals(reg1))), fi_percentil = 1:length(residuals(reg1))/1:length(residuals(reg1))) print(sd(residuals(reg1))) ## [1] 0.02884366 # plot de x ordenado contra y ordenado df %&gt;% ggplot(aes(y=sort(residuos), x=sort(density_points))) + geom_point() + geom_abline(intercept = 0, slope = 1, colour =&quot;blue&quot;) qqnorm(residuals(reg1)) qqline(residuals(reg1)) Obviamente o melhor é usar a função do R, já implementada, que faz tudo “by the book”, mas a intuição você já sabem qual é. Se os resíduos tiverem distribuição aproximadamente normal, devem ficar em torno da reta e não haver padrões significativos. Os extremos, por serem valores mais improváveis, terão poucos casos e tendem a ficar um pouco mais distante da reta. 10.4 Generalização Uma possibilidade de generalização é dividir os dados em treinamento e teste, onde estimamos o modelo com base nos dados de treinamento (uma amostra aleatória) e fazemos os testes olhando para os erros das previsoes do modelo. E aí podemos repetir os plots anteriores, agora para os dados de treinamento. Outra possibilidade é estimar o modelo com os dados onde o ajuste é bom, e colocar para teste onde o ajuste parece ruim. Isso é uma forma de confirmar que de fato o modelo está prevendo mal essa partedos dados. "],["inferência.html", "Capítulo 11 - Inferência 11.1 Normal 11.2 T-student 11.3 Teste de Hipótese e IC 11.4 Propriedades Assintóticas 11.5 Inferência Preditiva", " Capítulo 11 - Inferência Nós já trabalhamos com um modelo de regressão em que supomos normalidade do termo de erro e isso permitiu calcular o estimador de máxima verossimilhança e fazer inferência desse estimador de máxima verossimilhança. Contudo, a suposição de normalidade do erro é bastante restritiva, já que se isso não for verdade nem aproximadamente, não poderemos fazer inferência. Isso é particularmente relevante em amostras finitas (pequenas), onde não poderemos contar com alguma versão do Teorema Central do Limite para justificar a suposição de normalidade. Assim, queremos ser capazes de realizar testes de hipótese, calcular o intervalo de confiança (IC) e também quantificar a incerteza de nossas estimativas por meio do cálculo do p-valor. Aqui não é o lugar para discutir os problemas de misturar o paradigma de teste de hipótese de Neyman-Pearson, com o cálcul do p-valor do Fisher. Basta dizer que essas duas abordagens não se misturam muito bem, mas na prática os pesquisadores as têm combinado como se não houvesse nenhum problema e irei assumir essa mesma postura aqui. Mas o leitor fique avisado que há inconsistências em combinar ambas as abordagens e que ideialmente deveríamos utilizar apenas uma delas. 11.1 Normal Nós já vimos o caso mais simples em que podemos supor que os erros são normalmente distribuídos, isto é \\(e \\sim N(0, \\sigma^2)\\). Nós vimos também que o meu estimador pode ser decomposto no parâmetro populacional mais uma média ponderada dos erros: \\[ \\hat{\\beta} = \\beta + \\sum_{i=1}^{n} w_i \\cdot e_i \\] E os pesos são dados por \\(\\sum_{i=1}^n \\frac{x_i - \\bar{x}}{nS_x^2}\\). Essa expressão reforça por que os erros associados a observações com \\(x_i\\) muito distantes da média têm maior influência no estimador de MQO. Ou seja, o estimador é uma média ponderada dos erros. Ou seja, nosso estimador é igual a uma constante mais uma soma ponderada de variáveis aleatórias normais. E nós sabemos que a soma de variáveis aleatórias normais é uma normal, cuja média é \\(\\beta\\), já que o estimador é não-viesado e sua esperança é o parâmetro populacional e a variância nós também já calculamos e é dada por \\(\\frac{\\sigma^2}{n \\cdot S^2_x}\\) Se eu normalizar meu estimador, isto é, subtrair sua média e dividir pelo desvio-padrão amostral, tenho que o estimador normalizado segue a distribuição Normal padrão. Formalmente, \\[ z = \\frac{\\hat{\\beta} - \\beta}{\\sqrt{\\frac{\\sigma^2}{n \\cdot S^2_x}}} = \\frac{\\hat{\\beta} - \\beta}{\\frac{\\sigma}{\\sqrt{n \\cdot S^2_x}}} \\sim N(0,1 ) \\] Em que \\(S^2_x\\) é a variância amostral de \\(X\\) e \\(se[\\hat{\\beta}] = \\sqrt{\\frac{\\sigma^2}{n \\cdot S^2_x}}\\) é o erro padrão do \\(\\hat{\\beta}\\). Vejam também que: \\[ \\frac{\\hat{\\beta} - \\beta}{\\sigma} \\sim N(0, \\frac{1}{n \\cdot S^2_x} ) \\] Com isso pudemos realizar testes de hipótese, da seguinte maneira: Sabemos que, sob a hipótese de erros normais, \\[ z = \\frac{\\hat{\\beta} - \\beta}{se(\\hat{\\beta})} \\sim N(0,1) \\] Isso significa que: \\[ Pr(-1.96 \\le z \\le 1.96) = .95 \\] pois, pela Normal padrão, \\(\\Phi(1.96) = 0.975\\) e \\(\\Phi(-1.96) = 0.025\\), logo \\(0.975 - 0.025 = .95\\). Substituindo \\(z\\) pela definição acima, temos: \\[\\begin{align*} \\mathrm{P}(\\beta - 1.96 \\cdot se[\\hat{\\beta}] \\le \\hat{\\beta} \\le \\beta + 1.96 \\cdot se[\\hat{\\beta}] ) \\\\ = \\mathrm{P}( -1.96 \\cdot se[\\hat{\\beta}] ≈ \\hat{\\beta} - \\beta \\le 1.96 \\cdot se[\\hat{\\beta}] ) \\\\ = \\mathrm{P}( -1.96 \\le \\frac{\\hat{\\beta} - \\beta}{se[\\hat{\\beta}]} \\le 1.96 ) \\\\ = \\Phi(1.96) - \\Phi(-1.96) \\\\ = .95 \\end{align*}\\] Em que \\(\\Phi(x)\\) representa a probabilidade de observar um valor menor ou igual que \\(x\\) na distrinbuição normal padrão. Lembrando que a fórmula refere-se ao comportamento da estatística \\(\\hat{\\beta}\\) em muitas amostras — não à incerteza sobre um valor fixo de \\(\\beta\\). 11.2 T-student O problema é que nós não sabemos o valor de \\(\\sigma^2\\). A gente pode tentar substituir \\(\\sigma^2\\) por \\(\\hat{\\sigma}^2\\), o estimador da variância. Um estimador da variância é dado pela soma dos resíduos ao quadrado (já que a média é zero) dividido pelo \\(n\\). \\[ \\hat{\\sigma}^2 = \\frac{\\sum_{i=1}^n (Y_i - \\hat{\\alpha} - \\hat{\\beta} x_i)^2}{n} \\] Esse estimador é viesado e normalmente dividimos por \\(n-2\\), que são os chamados graus de liberdade: \\(\\frac{\\sum_{i=1}^{n}\\hat{e}_i^2}{n-2}\\). Esta maneira de explicar graus de liberdade, embora intuitiva, é errada. Como disse Rachael Meager no Twitter uma vez, ” I know enough to know it [degrees of freedom] can’t be ‘N minus number of parameters’ because once you do hierarchical/shrinkage it’s actually a substantively subtle and challenging task to ‘count’ how many parameters you have” 2. Eu não vou expandir muito aqui porque a definição correta requeriria conhecimento avançado de Álgebra Linear, mas basta dizer que o que ela está falando tem a ver com o fato de que em modelos multiníveis Bayesianos (por exemplo), o número de parâmetros não é facilmente determinado, de forma que essa fórmula de número de observações menos o número de parâmetros falha nesses e em outros casos (como modelos com regularização etc.). De todo modo, para nós importa que, substituindo meu estimador não-viesado para a variância em nossa fórmula, temos: \\[ z^* = \\frac{\\hat{\\beta} - \\beta}{\\sqrt{\\frac{\\hat{\\sigma}^2}{(n -2) \\cdot S^2_x}}} \\] Lembrando que: \\[ \\hat{se}(\\hat{\\beta}) = \\sqrt{\\frac{\\hat{\\sigma}^2}{(n - 2) \\cdot S^2_x}} = \\frac{\\hat{\\sigma}}{S_x \\sqrt{n - 2 }} \\] E essa estatística de teste não é mais normalmente distribuída. Na verdade, é possível mostrar que ela na verdade segue uma distribuição t-Student com \\(n-2\\) graus de liberdade, usando a distribuição qui-quadrado na derivação. 11.2.1 Avançado - Derivação da t- de student Comecemos com algumas preliminares. Se \\(Z_1, Z_2, \\cdots, Z_d \\sim N(0,1)\\) são independentes, então definimos a distribuição Qui-quadrado de tal forma que: \\(\\chi^2_{d} = \\sum_{i=1}^d Z_i^2\\) Se \\(Z \\sim N(0,1)\\), então (pela definição acima). \\(Z^2 \\sim \\chi^2_{1}\\) Lemrbrando que a soma dos erros tem média zero, então: \\(e_i - \\mathbb{E}[e] = e_i\\) e \\(\\frac{e_i}{\\sqrt{Var(e_i)}} = \\frac{e_i}{\\sigma} \\sim N(0,1)\\) Disso segue que: \\[ \\sum_{i=1}^n \\frac{e_i^2}{\\sigma^2} = \\sum_{i=1}^n (\\frac{e_i}{\\sigma})^2 \\sim \\chi^2_{n} \\] Como porém só temos \\(n-2\\) graus de liberdade, só temos \\(n-2\\) erros verdadeiramente independentes, então só podemos somar até \\(n-2\\), o que dá a \\(\\chi^2_{n-2}\\). Por fim: Se \\(Z \\sim N(0,1)\\), \\(S^2 \\sim \\chi^2_d\\) e \\(Z\\) e \\(S^2\\) são independentes, então: \\[ \\frac{Z}{\\sqrt{S^2/d}} \\sim t_d \\] \\[\\begin{align} \\frac{\\hat{\\beta} - \\beta}{\\hat{se}(\\hat{\\beta})} = \\frac{\\hat{\\beta} - \\beta}{\\sigma} \\times \\frac{\\sigma}{\\hat{se}(\\hat{\\beta})} \\\\ = \\frac{\\frac{\\hat{\\beta} - \\beta}{\\sigma}}{\\frac{\\hat{se}(\\hat{\\beta})}{\\sigma}} = \\frac{N(0,1/ns^2_x)}{\\frac{\\hat{se}(\\hat{\\beta})}{\\sigma}} = \\frac{N(0,1/ns^2_x)}{\\frac{\\hat{\\sigma}}{\\sigma S_x\\sqrt{n-2}}}\\\\ = \\frac{S_x \\times N(0,1/ns^2_x)}{\\frac{\\hat{\\sigma}}{\\sigma \\sqrt{n-2}}} = \\frac{N(0,1/n)}{\\frac{\\hat{\\sigma}}{\\sigma \\sqrt{n-2}}} = \\\\ = \\frac{\\sqrt{n} \\times N(0,1/n)}{\\frac{\\sqrt{n} \\times \\hat{\\sigma}}{\\sigma \\sqrt{n-2}}} = \\frac{N(0,1)}{\\frac{\\sqrt{n\\hat{\\sigma}^2}}{\\sqrt{\\sigma^2 \\times (n-2)}}} \\\\ = \\frac{N(0,1)}{\\sqrt{\\frac{n\\hat{\\sigma}^2}{\\sigma^2 \\times (n-2)}}} \\\\ = \\frac{N(0,1)}{\\sqrt{\\chi^2_{n-2}/(n-2)}} \\sim t_d \\end{align}\\] No penúltimo passo, usamos de maneira pouco rigorosa que se \\(sum_{i=1}^n (\\frac{e_i}{\\sigma})^2 \\sim \\chi^2_{n}\\), “então” eu posso aproximar o numerador por \\(\\sum_{i=1}^n e_i^2\\) por \\(n\\hat{\\sigma}^2\\), usando o fato de que a média é zero, e portanto ali é a variância. Digo que é pouco rigorosa porque o erros não são a mesma coisa que os resíduos. Mas a matemática para fazer a prova correta está além dos nosso conhecimentos. Formalmente, a prova depende de mostrar que o estimador da variância residual, ao ser escalonado por \\(\\sigma^2\\), segue uma distribuição qui-quadrado com \\(n-2\\) graus de liberdade, e que é independente de \\(\\hat{\\beta}\\). Esses resultados exigem álgebra matricial, mas a intuição é que o numerador e o denominador são independentes e cada um tem a forma de uma Normal padrão e de uma raiz de qui-quadrado, respectivamente. 11.2.2 Teste t Ou seja, \\[ z^* = \\frac{\\hat{\\beta} - \\beta}{\\sqrt{\\frac{\\hat{\\sigma^2}}{n S^2_x})}} \\sim t_{n-2} \\] Por um raciocínio similar podemos derivar que o intercepto (padronizado) também segue uma distribuição \\(t\\) com \\(n-2\\) graus de liberdade. 11.3 Teste de Hipótese e IC Como consequência, podemos derivar o seguinte, para qualquer \\(k &gt;0\\) \\[\\begin{align*} \\mathrm{P}(\\beta - k \\cdot \\hat{se}[\\hat{\\beta}] \\le \\hat{\\beta} \\le \\beta + k \\cdot \\hat{se}[\\hat{\\beta}] ) \\\\ = \\mathrm{P}( -k \\cdot \\hat{se}[\\hat{\\beta}] \\le \\hat{\\beta} - \\beta \\le k \\cdot \\hat{se}[\\hat{\\beta}] ) \\\\ = \\mathrm{P}( -k \\le \\frac{\\hat{\\beta} - \\beta}{\\hat{se}[\\hat{\\beta}]} \\le k ) \\\\ = t_{n-2}(k) \\end{align*}\\] Em que, com um grande abuso de notação, estou chamando \\(t_{n-2}(k)\\) a probabilidade, na distribuição \\(t\\) de student com \\(n-2\\) graus de liberdade, de \\(\\hat{\\beta}\\) estar no intervalo \\(-k, k\\). Se eu definir um nível de significância \\(\\alpha\\) entre \\(0\\) e \\(1\\), sempre exisitrá um \\(k\\) que que a equação acima seja verdadeira. # se n = 11, tenho 9 d.f. # alpha = 5% qt(0.025, df=9) ## [1] -2.262157 qt(0.975, df=9) ## [1] 2.262157 # alpha = 50% qt(0.25, df=9) ## [1] -0.7027221 qt(0.75, df=9) ## [1] 0.7027221 Vamos então definir o intervalo amostral para \\(\\hat{\\beta}\\) como: \\([\\beta - k(n,\\alpha)\\hat{se}(\\hat{\\beta}),\\beta + k(n,\\alpha)\\hat{se}(\\hat{\\beta})]\\), ou seja, se a verdadeira inclinação da reta é \\(\\beta\\), então meu \\(\\hat{\\beta}\\) estará nesse intervalo com probabilidade \\(1 - \\alpha\\). O que nos leva ao teste de hipóte nulo de que \\(\\beta = \\beta^*\\), em particular, de que \\(\\beta^* = 0\\), mas não precisa ser essa hipótese nula. E rejeitamos a nula se meu estimador está fora dese intervalo \\([\\beta^* - k(n,\\alpha)\\hat{se}(\\hat{\\beta}),\\beta^* + k(n,\\alpha)\\hat{se}(\\hat{\\beta})]\\), e aceitamos a nula se ele está no intervalo. 11.4 Propriedades Assintóticas O que acontece à medida que \\(n\\) cresce? Como exercício, faça uma simulação no R, aumentando o \\(n\\) e verificando como fica o histograma da t de student. 11.5 Inferência Preditiva É muito comum que queiramos fazer inferência para nossas previsões, isto é, quantificar a incerteza de nossas previsões da amostra para a população. Vamos retomar o modelo preditivo para eleições como exemplo. Vamos carregar os dados de 2018, que usamos para estimar um modelo preditivo, e vamos utilizar o modelo para prever o resultado de 2022. Como o TSE disponibilizou os dados por Zona eleitoral, vou utilizá-la como unidade (em vez de seção) para as observações. Peguem os dados desse link. Baixem, descompactem e salvem na pasta onde o R aponta. library(data.table) library(tidyverse) library(janitor) library(tidyr) # lista o nome do arquivo em csv # unzip(here(&quot;dados&quot;, &quot;votacao_secao_2018_BR.zip&quot;), list = TRUE) #read data1.csv into data frame presid_18 &lt;- fread(here(&quot;dados&quot;,&quot;votacao_secao_2018_BR.csv&quot;), encoding = &quot;Latin-1&quot;) # Supondo que seu dataframe seja chamado df df_resultados &lt;- presid_18 %&gt;% dplyr::filter(!NR_VOTAVEL %in% c(95,96)) %&gt;% group_by(NR_ZONA, CD_MUNICIPIO, SG_UF, NR_VOTAVEL, NR_TURNO) %&gt;% summarise(total_votos = sum(QT_VOTOS)) %&gt;% pivot_wider(names_from = NR_TURNO, values_from = total_votos, values_fill = 0) %&gt;% janitor::clean_names() %&gt;% group_by(nr_zona, cd_municipio, sg_uf) %&gt;% mutate(total_validos_1t = sum(x1), total_validos_2t = sum(x2)) %&gt;% dplyr::filter(nr_votavel == 17) %&gt;% mutate(percentual_bolso_1t = x1 /total_validos_1t , percentual_bolso_2t = x2 / total_validos_2t) # remove # rm(presid_18) # modelo de regressão reg1 &lt;- lm(percentual_bolso_2t ~ percentual_bolso_1t, data = df_resultados) summary(reg1) ## ## Call: ## lm(formula = percentual_bolso_2t ~ percentual_bolso_1t, data = df_resultados) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.189870 -0.022821 -0.004147 0.018858 0.311149 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.019864 0.001058 18.78 &lt;2e-16 *** ## percentual_bolso_1t 1.152802 0.002387 483.02 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.03557 on 6238 degrees of freedom ## Multiple R-squared: 0.974, Adjusted R-squared: 0.974 ## F-statistic: 2.333e+05 on 1 and 6238 DF, p-value: &lt; 2.2e-16 df_resultados %&gt;% ggplot(aes(x=percentual_bolso_1t, y=percentual_bolso_2t)) + geom_point() + geom_abline(slope = coef(reg1)[2] , intercept = coef(reg1)[1], colour = &quot;blue&quot;) # dados de 2022 #read into data frame presid_22 &lt;- fread(here(&quot;dados&quot;,&quot;votacao_secao_2022_BR.csv&quot;), encoding = &quot;Latin-1&quot;) df_resultados_22 &lt;- presid_22 %&gt;% dplyr::filter(!NR_VOTAVEL %in% c(95,96)) %&gt;% group_by(NR_ZONA, CD_MUNICIPIO, SG_UF, NR_VOTAVEL, NR_TURNO) %&gt;% summarise(total_votos = sum(QT_VOTOS)) %&gt;% pivot_wider(names_from = NR_TURNO, values_from = total_votos, values_fill = 0) %&gt;% janitor::clean_names() %&gt;% group_by(nr_zona, cd_municipio, sg_uf) %&gt;% mutate(total_validos_1t = sum(x1), total_validos_2t = sum(x2)) %&gt;% dplyr::filter(nr_votavel == 22) %&gt;% dplyr::filter(total_validos_1t &gt;0) %&gt;% mutate(percentual_bolso_1t = x1 /total_validos_1t , percentual_bolso_2t = x2 / total_validos_2t) Agora que importamos os dados de 22, podemos fazer nossa previsão, usando os resultados do primeiro turno. df_resultados_22 &lt;- df_resultados_22 %&gt;% mutate(y_prev_2t = coef(reg1)[1] + coef(reg1)[2]*percentual_bolso_1t, validos_previsto = total_validos_1t*y_prev_2t) # previsão do resultado eleitoral antes de observar apuração do 2t, supondo comparecimento igual ao 1t df_resultados_22 %&gt;% ungroup() %&gt;% summarise(total_bolso = sum(validos_previsto), total_valido_previsto = sum(total_validos_1t), perc_previsto = total_bolso/total_valido_previsto) ## # A tibble: 1 × 3 ## total_bolso total_valido_previsto perc_previsto ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 61224817. 118229715 0.518 # previsão do resultado eleitoral com o comparecimento do 2o turno real df_resultados_22 &lt;- df_resultados_22 %&gt;% mutate(y_prev_2t_alt = coef(reg1)[1] + coef(reg1)[2]*percentual_bolso_1t, validos_previsto_alt = total_validos_2t*y_prev_2t_alt) df_resultados_22 %&gt;% ungroup() %&gt;% summarise(total_bolso = sum(validos_previsto_alt), total_valido_previsto = sum(total_validos_2t), perc_previsto = total_bolso/total_valido_previsto) ## # A tibble: 1 × 3 ## total_bolso total_valido_previsto perc_previsto ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 61501668. 118552342 0.519 # mesma coisa #E incerteza nas previsões? previsoes &lt;- predict(reg1, newdata = df_resultados_22, interval = &quot;prediction&quot;, level = .95) %&gt;% as.data.frame() df_resultados_22 &lt;- df_resultados_22 %&gt;% ungroup() %&gt;% mutate(prev_perc = previsoes$fit, prev_perc_lower = previsoes$lwr, prev_perc_upper = previsoes$upr, validos_prev = total_validos_2t*prev_perc, validos_prev_lower = total_validos_2t*prev_perc_lower, validos_prev_upper = total_validos_2t*prev_perc_upper) df_resultados_22 %&gt;% summarise(perc_previsto = sum(validos_prev)/sum(total_validos_2t), perc_previsto_lower = sum(validos_prev_lower)/sum(total_validos_2t), perc_previsto_upper = sum(validos_prev_upper)/sum(total_validos_2t)) ## # A tibble: 1 × 3 ## perc_previsto perc_previsto_lower perc_previsto_upper ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.519 0.449 0.589 Como vimos, apenas com base nos dados do primeiro turno, ao nível da zona eleitoral, não era possível prever o resultado final. E à medida que os dados por zona eleitoral foram divulgados, podíamos melhorar nossa previsão de algum modo? E será que se incluírmos mais variáveis, podemos melhorar o modelo? Vamos deixar isso para a próxima aula. Por enquanto, vamos fazer o seguinte exercício. Como nossas previsões iriam evoluir, com basse nesse modelo, à medida que acontecia a apuração? see (if twitter is still availble to look at) https://twitter.com/economeager/status/1596450647599190017)↩︎ "],["regressao-múltipla.html", "Capítulo 12 - Regressao Múltipla 12.1 Revisão de Matriz e Vetores 12.2 Modelo básico 12.3 Modelo com matrizes 12.4 Interpretação dos coeficientes 12.5 Regressão múltipla versus Múltiplas Regressões Separadas ou Viés de variável omitida 12.6 Matriz chapéu 12.7 Multicolinearidade 12.8 Erro padrão Robusto 12.9 Termo de interação 12.10 Análise de sensibilidade causal 12.11 Referências", " Capítulo 12 - Regressao Múltipla library(ggplot2) library(knitr) library(tidyverse) library(here) library(tidyr) É muito raro que tenhamos apenas um preditor em um contexto de regressão. Talvez em um experimento de laboratório, e mesmo assim é difícil não ter outros preditores. De modo que tudo que vimos até agora é um caso particular raríssimo e pouco útil per se. Para generalizar as derivações e resultados para múltiplos preditores é conveniente utilizar álgebra linear. Basicamente, isso significa usar vetores e matrizes para representar a regressão. As derivações das fórmulas ficam bem fáceis, bem como as propriedades do modelo de regressão. Como não estamos pressupondo que vocês conheçam o básico de Álgebra Linear, iremos pular as derivações e apresentar apenas as intuições, esperando que o que vocês aprenderam com um preditor seja suficiente para entender com múltiplos preditores. 12.1 Revisão de Matriz e Vetores Regressão múiltipla é mais facilmente compreendida com uso de matrizes. Portanto, vamos fazer uma rápida revisão da álgebra de vetores e matrizes. Se eu tenho um vetor \\(x = [ a_1, a_2, ..., a_n]\\), digo que este é um vetor linha com \\(n\\) elementos. É possível também ter um vetor coluna: \\[ \\begin{align} x &amp;= \\begin{bmatrix} a_{1} \\\\ a_{2} \\\\ \\vdots \\\\ a_{n} \\end{bmatrix} \\end{align} \\] Eu posso somar dois vetores linhas ou dois vetores colunas, se tiverem o mesmo número de elementos. Por exemplo, dois vetores colunas. \\[ \\begin{align} \\begin{bmatrix} a_{1} \\\\ \\vdots \\\\ a_{n} \\end{bmatrix} + \\begin{bmatrix} b_{1} \\\\ \\vdots \\\\ b_{n} \\end{bmatrix} &amp;= \\begin{bmatrix} a_1 + b_{1} \\\\ \\vdots \\\\ a_n + b_{n} \\end{bmatrix} \\end{align} \\] E posso fazer multiplicação de vetores (existem vários tipos, aqui me restringo ao produto interno ou produto ponto de vetores), desde que a gente multiplique um vetor linha por uma vetor coluna, mas não o contrário. \\[ \\begin{align} \\begin{bmatrix} a_{1}, a_2, \\cdots, a_{n} \\end{bmatrix} \\cdot \\begin{bmatrix} b_{1} \\\\ b_{2} \\\\ \\vdots \\\\ b_{n} \\end{bmatrix} &amp;= a_1 \\cdot b_{1} + a_2 \\cdot b_2 \\cdots + a_n \\cdot b_{n} \\end{align} \\] A razão é que a multiplicação de vetores (e matrizes em geral) é basicamente multiplicar linha com coluna. No caso de um vetor coluna multiplicado por um linha, isso não é possível. A adição e multiplicação de matrizes é basicamente a generalização da ágebra com vetores 12.2 Modelo básico O modelo básico de regressão linear múltipla pode ser especificado por: Existem p preditores, \\(X_1\\), \\(X_2\\), …, \\(X_p\\). Não precisamos fazer suposições sobre a distribuição dos preditores, e podem ser correlacionados ou não. Há uma única variável resposta, \\(Y\\). Se houvesse mais de uma, teríamos um modelo de regressão multivariada. \\(y_i = \\alpha + \\beta_1 \\cdot x_{1i} + \\beta_2 \\cdot x_{2i} + ... + \\beta_p \\cdot x_{pi} + e_i\\). Portanto, temos \\(p+1\\) parâmetros ou coeficientes de regressão a estimar. O erro \\(e_i\\) possui esperança condicional zero e variância condicional constante no modelo homocedástico, e não correlacionado entre observações. Se assumirmos normalidade do termo de erro, temos também: O erro \\(e_i\\) tem uma distribuição normal multivariada, com vetor de médias zero e matriz de variância e covariância cujos elementos fora da diagonal (covariância) são zero, e a diagonal principal é \\(\\sigma^2\\). 12.3 Modelo com matrizes Vejam que \\(\\alpha + \\beta_1 \\cdot x_{1i} + \\beta_2 \\cdot x_{2i} + ... + \\beta_p \\cdot x_{pi} + e_i\\) é uma soma de produtos, similar ao que eu tinha com vetores no exemplo acima. Exceto que \\(\\alpha\\) não multiplica nada. Então, vou considerar que tenhao um preditor cujo valor é uma constante e igual a \\(1\\), e os demais preditores, de forma que o lado direito da equação de regressão pode ser reescrito como soma e multiplicação de matrizes. Para o caso de um preditor \\(y_i = \\alpha + \\beta_ix_{1i} + e_i\\), a equação de regressão com matrizes, fica: \\[ \\begin{align} \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_{n} \\end{bmatrix} &amp;= \\begin{bmatrix} 1 &amp; x_{11} \\\\ 1 &amp; x_{12} \\\\ \\vdots \\\\ 1 &amp; x_{1n} \\end{bmatrix} \\begin{bmatrix} \\alpha \\\\ \\beta_1 \\\\ \\end{bmatrix} + \\begin{bmatrix} e_1 \\\\ e_2 \\\\ \\vdots \\\\ e_n \\end{bmatrix} \\end{align} \\] Se eu chamar o vetor coluna com os \\(y\\) de \\(Y\\), a matriz com a constante \\(1\\) e \\(x_{1i}\\) de \\(X\\), o vetor de coeficientes de \\(B\\) e o vetor de erros \\(\\epsilon\\), tenho então: \\[ Y = XB + \\epsilon \\] Veja que a generalização para \\(p\\) preditores gera a mesma equação: \\[ \\begin{align} \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_{n} \\end{bmatrix} &amp;= \\begin{bmatrix} 1 &amp; x_{11} &amp; x_{21} \\cdots &amp; x_{p1}\\\\ 1 &amp; x_{12} &amp; x_{22} \\cdots &amp; x_{p2}\\\\ \\vdots \\\\ 1 &amp; x_{1n} &amp; x_{2n} \\cdots &amp; x_{pn} \\end{bmatrix} \\begin{bmatrix} \\alpha \\\\ \\beta_1 \\\\ \\beta_2 \\\\ \\vdots \\\\ \\beta_p \\end{bmatrix} + \\begin{bmatrix} e_1 \\\\ e_2 \\\\ \\vdots \\\\ e_n \\end{bmatrix} \\end{align} \\] \\[ Y = XB + \\epsilon \\] A única diferença é o tamanho da matriz \\(X\\) e \\(B\\), em que \\(X\\) é uma matriz \\(n \\times (p+1)\\), isto é, com \\(n\\) linhas e \\(p+1\\) colunas, e \\(B\\) é uma matriz de tamanho \\((p+1) \\times 1\\) e \\(Y\\) e \\(\\epsilon\\) são \\(n \\times 1\\). E as suposições podem ser escritas como \\(\\mathbb{E}[\\epsilon|X] = 0\\) e \\(\\mathbb{Var}[\\epsilon|X] = \\sigma^2I\\), em que \\(I\\) é a matriz identidade, isto é, uma matriz cuja diagonal principal é \\(1\\) e o resto é \\(zero\\). 12.3.1 Estimador de MQO É possível mostrar que o estimador de mínimos quadrados ordinários é dados por: \\[ \\hat{B} = (X&#39;X)^{-1} \\cdot X&#39;Y \\] Veja que \\(X&#39;Y\\) é um produto (com soma) entre \\(X\\) e \\(Y\\), ou seja, é como se fosse a covariância entre \\(X\\) e \\(Y\\), e \\(X&#39;X\\) se assemelha à variância de \\(X\\). E está elevado a \\(-1\\) porque não existe divisão em matriz, de forma que preciso multiplicar pela inversa. 12.4 Interpretação dos coeficientes Nosso coeficiente \\(\\alpha\\) é novamente o valor esperado do \\(Y\\) na origem, isto é: \\[ \\alpha = \\mathbb{E}[Y|X_1=0, X_2=0, \\cdots, X_p=0]$ \\] Em um modelo sem interações, o efeito de cada variávei \\(X_i\\) é a contribuição separada para a resposta esperada (média). Portanto, \\(B_i\\) mede a contribuição de como \\(\\mathbb{E}[Y]\\) muda à medida que \\(X_i\\) (e apenas \\(X_i\\)) muda, para qualquer valor de \\(X_i\\) (se a equação for linear nas variáveis) e para qualquer valor das demais variáveis (pressuposto de aditividade, sem interação, dos preditores). Vamos retomar nosso modelo de previsão eleitoral e rodar no R, agora adicionando múltipals variáveis. library(data.table) # lista o nome do arquivo em csv # unzip(here(&quot;dados&quot;, &quot;votacao_secao_2018_BR.zip&quot;), list = TRUE) #read data1.csv into data frame presid_18 &lt;- fread(here(&quot;dados&quot;,&quot;votacao_secao_2018_BR.csv&quot;), encoding = &quot;Latin-1&quot;) # Supondo que seu dataframe seja chamado df df_resultados &lt;- presid_18 %&gt;% dplyr::filter(!NR_VOTAVEL %in% c(95,96)) %&gt;% group_by(NR_ZONA, CD_MUNICIPIO, SG_UF, NR_VOTAVEL, NR_TURNO) %&gt;% summarise(total_votos = sum(QT_VOTOS)) %&gt;% pivot_wider(names_from = NR_TURNO, values_from = total_votos, values_fill = 0) %&gt;% janitor::clean_names() %&gt;% group_by(nr_zona, cd_municipio, sg_uf) %&gt;% mutate(total_validos_1t = sum(x1), total_validos_2t = sum(x2)) %&gt;% dplyr::filter(nr_votavel %in% c(13,17)) %&gt;% group_by(nr_votavel) %&gt;% mutate(percentual_1t = x1 /total_validos_1t, percentual_2t = x2 / total_validos_2t) %&gt;% ungroup() %&gt;% dplyr::select(-c(x1, x2, total_validos_1t, total_validos_2t)) %&gt;% pivot_wider(names_from = nr_votavel, values_from = c(percentual_1t, percentual_2t)) # remove # rm(presid_18) df_resultados %&gt;% ggplot(aes(x=percentual_1t_17, y=percentual_2t_17)) + geom_point() + facet_wrap(~sg_uf) + geom_smooth(method=&quot;lm&quot;, se=F, linewidth = .5) # modelo de regressão reg1 &lt;- lm(percentual_2t_17 ~ percentual_1t_17 + percentual_1t_13 + sg_uf, data = df_resultados) summary(reg1) ## ## Call: ## lm(formula = percentual_2t_17 ~ percentual_1t_17 + percentual_1t_13 + ## sg_uf, data = df_resultados) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.190977 -0.010934 -0.001161 0.009747 0.272558 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.355531 0.005763 61.689 &lt;2e-16 *** ## percentual_1t_17 0.777791 0.005276 147.419 &lt;2e-16 *** ## percentual_1t_13 -0.277355 0.004577 -60.603 &lt;2e-16 *** ## sg_ufAL -0.101170 0.004644 -21.787 &lt;2e-16 *** ## sg_ufAM -0.084715 0.004815 -17.596 &lt;2e-16 *** ## sg_ufAP -0.087085 0.006414 -13.577 &lt;2e-16 *** ## sg_ufBA -0.086392 0.004355 -19.835 &lt;2e-16 *** ## sg_ufCE -0.150819 0.004654 -32.406 &lt;2e-16 *** ## sg_ufDF -0.077372 0.006221 -12.437 &lt;2e-16 *** ## sg_ufES -0.079796 0.004701 -16.974 &lt;2e-16 *** ## sg_ufGO -0.085402 0.004352 -19.624 &lt;2e-16 *** ## sg_ufMA -0.105331 0.004464 -23.596 &lt;2e-16 *** ## sg_ufMG -0.065445 0.004235 -15.454 &lt;2e-16 *** ## sg_ufMS -0.061922 0.004685 -13.219 &lt;2e-16 *** ## sg_ufMT -0.088621 0.004493 -19.722 &lt;2e-16 *** ## sg_ufPA -0.071259 0.004489 -15.876 &lt;2e-16 *** ## sg_ufPB -0.119179 0.004447 -26.801 &lt;2e-16 *** ## sg_ufPE -0.113396 0.004454 -25.458 &lt;2e-16 *** ## sg_ufPI -0.091415 0.004480 -20.404 &lt;2e-16 *** ## sg_ufPR -0.060601 0.004284 -14.145 &lt;2e-16 *** ## sg_ufRJ -0.096635 0.004433 -21.797 &lt;2e-16 *** ## sg_ufRN -0.112249 0.004522 -24.826 &lt;2e-16 *** ## sg_ufRO -0.059576 0.004958 -12.017 &lt;2e-16 *** ## sg_ufRR -0.085327 0.006511 -13.104 &lt;2e-16 *** ## sg_ufRS -0.061179 0.004262 -14.353 &lt;2e-16 *** ## sg_ufSC -0.061522 0.004327 -14.220 &lt;2e-16 *** ## sg_ufSE -0.094247 0.004814 -19.578 &lt;2e-16 *** ## sg_ufSP -0.039378 0.004250 -9.265 &lt;2e-16 *** ## sg_ufTO -0.095958 0.004526 -21.202 &lt;2e-16 *** ## sg_ufZZ -0.081547 0.004558 -17.892 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.02 on 6209 degrees of freedom ## (1 observation deleted due to missingness) ## Multiple R-squared: 0.9918, Adjusted R-squared: 0.9918 ## F-statistic: 2.588e+04 on 29 and 6209 DF, p-value: &lt; 2.2e-16 # AC é a categoria de referência A interpretação das variáveis, portanto, é a seguinte: O intercepto mede o percentual médio no segundo turno quando todas as variáveis são zero. Ou seja, Haddad e Bolsonaro tiveram 0 pontos percentuais (não existe caso assim!) e a UF é o Acre, que é a categoria de referência. A variável “percentual_1t_17” mede o efeito preditivo no voto do 2o turno do Bolsonaro de aumento de um ponto percentual no voto do primeiro turno, que é de 0,77 pontos percentuais. A variável “percentual_1t_13” mede similarmente o efeito preditivo de aumento de um ponto percentual do voto do Haddad no primeiro turno sobre o voto do Bolsonaro no 2o turno. Como esperado, a relação é negativa, isto é, quanto melhor o Haddad foi no primeiro turno, pior o Bolsonaro no segundo turno naquela seção eleitoral. E o efeito de cada UF é o efeito de estar naquela UF, em comparação com a categoria de referência, ACRE. 12.5 Regressão múltipla versus Múltiplas Regressões Separadas ou Viés de variável omitida Rodar uma regressão com duas variáveis (digamos), não é o mesmo que rodar duas regressões separadas, uma com cada variável. A razão é que os preditores em geral terão alguma correlação entre si. Para ver isso, suponha que o verdadeiro modelo é \\(Y = \\alpha + \\beta_1 \\cdot X_1 + \\beta_2 \\cdot X_2 + \\epsilon\\). O que aconteceria se rodássemos uma regressão com um preditor apenas (\\(X_1\\))? Vamos usar as seguintes propriedades da covariância nessa derivação. Sejam \\(X\\) e \\(Y\\) duas v.a., Seja \\(A\\) uma constante. Então, \\(\\mathbb{Cov}[X,Y + A] = \\mathbb{Cov}[X,Y] + \\mathbb{Cov}[X,A]\\). Como \\(A\\) é constante, \\(\\mathbb{Cov}[X,A] = 0\\) e, portanto, \\(\\mathbb{Cov}[X,Y + A] = \\mathbb{Cov}[X,Y]\\). \\(\\mathbb{Cov}[X,Y \\cdot A] = A \\cdot \\mathbb{Cov}[X,Y]\\). Em um modelo com um único preditor, teremos: \\(Y = \\alpha + \\beta_1^* \\cdot X_1 + \\epsilon\\) Designei o beta da equação com um preditor por \\(\\beta_1^*\\), para diferenciar do \\(\\beta_1\\) da verdadeira equação, com dois preditores. Nós sabemos que \\(\\beta_1^* = \\frac{\\mathbb{Cov}[X_1,Y]}{\\mathbb{Var}[X_1]}\\) Vamos substituir o \\(Y\\) do modelo verdadeiro na equação do \\(\\beta_1^*\\). \\[\\begin{align*} \\beta_1^* = \\frac{\\mathbb{Cov}[X_1,\\alpha + \\beta_1 \\cdot X_1 + \\beta_2 \\cdot X_2 + e]}{\\mathbb{Var}[X_1]} = \\\\ \\frac{\\mathbb{Cov}[X_1,\\beta_1 \\cdot X_1] + \\mathbb{Cov}[X_1, \\beta_2 \\cdot X_2] + \\mathbb{Cov}[X_1, e]}{\\mathbb{Var}[X_1]} = \\\\ \\frac{\\beta_1 \\cdot \\mathbb{Cov}[X_1, X_1] + \\beta_2 \\cdot \\mathbb{Cov}[X_1, X_2] + \\mathbb{ Cov}[X_1,e]}{\\mathbb{Var}[X_1]} = \\\\ \\frac{\\beta_1 \\cdot \\mathbb{Var}[X_1] + \\beta_2 \\cdot \\mathbb{Cov}[X_1, X_2] + 0}{\\mathbb{Var}[X_1]} = \\\\ \\frac{\\beta_1 \\cdot \\mathbb{Var}[X_1]}{\\mathbb{Var}[X_1]} + \\frac{\\beta_2 \\cdot \\mathbb{Cov}[X_1, X_2] + 0}{\\mathbb{Var}[X_1]} = \\\\ \\beta_1 + \\frac{\\beta_2 \\cdot \\mathbb{Cov}[X_1, X_2] + 0}{\\mathbb{Var}[X_1]} = \\\\ \\beta_1^* = \\beta_1 + \\frac{\\beta_2 \\cdot \\mathbb{Cov}[X_1, X_2]}{\\mathbb{Var}[X_1]} \\end{align*}\\] Vemos que a inclinação \\(\\beta_1^*\\) inclui a contribuição direta de \\(X_1\\) via \\(\\beta_1\\) mais a contribuição indireta da correlação com \\(X_2\\), via \\(\\beta_2\\). Portanto, se eu rodar uma regressão com um preditor quando o verdadeiro modelo tem dois preditores, o coeficiente de \\(\\beta_1^*\\) será uma média entre \\(\\beta_1\\) e \\(\\beta_2\\). Por outro lado, se eu rodar a regressão com o modelo correto com os dois preditores, consigo que \\(\\beta_1^*\\) reflita só a contribuição de \\(\\beta_1\\). Talvez você esteja se perguntando a essa altura: quem garante que o verdadeiro modelo possua só dois preditores? Isso é o que chamamos de viés de variável omitida. Se omitirmos da regressão uma variável \\(X_k\\) correlacionada com \\(X_j\\), \\(j \\neq k\\), então o coeficiente \\(\\beta_j\\) reflitirá também o efeito de \\(\\beta_k\\). Aqui não estamos falando de causalidade, apenas da contribuição para a previsão da nossa variável resposta. Naturalmente, antes da moderna abordagem de inferência causal por resultados potenciais de Rubin (ou redes Bayesianas em modelos estruturais de Pearl), as pessoas pensavam que, controlando para o máximo de variáveis possível, com sorte seria possível eliminar (ou reduzir a um mínimo) o viés de variável omitida e, portanto, estar seguro que \\(\\beta_1\\) estimaria o efeito causal. Nós hoje sabemos que o modo mais seguro de pensar causalidade é usando uma das duas abordagens (as iniciadas por Rubin ou Pearl), e verificando (por exemplo com resultados potenciais) que a suposição de independência condicional (CIA, de Conditional Independence Assumption) é plausível para poder interpretar \\(\\beta_1\\) causalmente. Sem um modelo causal, a abordagem de introdução de regressores para controlar o viés de variável omitida não nos permite fazer inferência causal, exceto em casos muitos simples ou quando implicitamente temos garantida a validade da CIA (como em um experimento bem conduzido e com compliance), como é o caso das ciências naturais como física e química. 12.6 Matriz chapéu Uma forma interessante de visualizar as previsões do modelo é que podemos escrever \\(\\hat{Y} = X\\hat{B}\\). Substituindo a fórmula do \\(\\hat{B}\\), temos que: \\[ \\hat{Y} = X (X&#39;X)^{-1}X&#39;Y = \\\\ (X (X&#39;X)^{-1}X&#39;)Y = \\\\ HY \\] Essa equação mostra que as previsões são dadas pelas respostas observadas, ponderadas pela matriz chapéu (hat), \\(H\\). 12.7 Multicolinearidade Até o momento não falamos sobre em que condições a matrix inversa \\(X&#39;X^{-1}\\) existe. Nós sabemos que nem todas as matrizes podem ser invertíveis. Matrizes com determinante zero são não-invertíveis. A intuição é como pensar que não é possível dividir um escalar por zero. O determinante é zero quando as colunas não são linearmente independentes. Ou seja, quando uma coluna (ou mais) é uma combinação linear de um ou mais colunas. No nosso caso, quando a correlação for \\(1\\) (ou \\(-1\\)). Nesses casos, não é possível estimar os coeficientes da regressão e acontece quando temos multicolinearidade. Softwares modernos, como R, irão “dropar” uma (ouas mais) variável(eis) se isso ocorrer, automaticamente, para evitar que a matriz não seja invertível. Assim, a menos que acorrelação seja perfeita, multicolinearidade não costuma ser um problema. 12.8 Erro padrão Robusto Na presença de heterocedasticidade ou correlação nos erros (como autocorrelação temporal ou autocorrelação espacial), precisamos corrigir o cálculo do erro padrão. Para explicar como é calculado o erro padrão robusto, vamos derivar o erro padrão novamente, agora com a notação matricial. Lembremos que: \\(\\hat{B} = (X&#39;X)^{-1}(X&#39;Y)\\) e \\(Y = XB + e\\). Logo, reescrevendo a equação de regressão, temos: \\[\\begin{align} \\hat{B} = (X&#39;X)^{-1}(X&#39;[XB + e]) \\\\ = (X&#39;X)^{-1}(X&#39;XB + X&#39;e) \\\\ = (X&#39;X)^{-1}X&#39;XB + (X&#39;X)^{-1}X&#39;e \\\\ = B + (X&#39;X)^{-1}X&#39;e \\end{align}\\] E a partir dessa equação, podemos calcular a variância dos estimadores. \\[\\begin{align} \\mathbb{Var}[\\hat{B}|X] = \\mathbb{Var}[B + (X&#39;X)^{-1}X&#39;e|X] \\\\ = \\mathbb{Var}[(X&#39;X)^{-1}X&#39;e|X] \\\\ = (X&#39;X)^{-1}X&#39;\\mathbb{Var}[e|X]X(X&#39;X)^{-1} \\\\ = (X&#39;X)^{-1}X&#39;\\sigma^2IX(X&#39;X)^{-1} \\\\ = \\sigma^2(X&#39;X)^{-1}X&#39;X(X&#39;X)^{-1} \\\\ = \\sigma^2(X&#39;X)^{-1} \\end{align}\\] Para ficar mais familiar para a gente, posso multiplicar e dividir por \\(n\\), que não altero a equação. Assim, temos: \\[ \\mathbb{Var}[\\hat{B}|X] = \\frac{\\sigma^2}{n}(n^{-1}(X&#39;X)^{-1}) \\] Lembrem-e que a variância do \\(\\hat{\\beta|x}\\) no modelo de regressão simples era dada por: \\(\\frac{\\sigma^2}{nS_x^2}\\) Então, \\(\\frac{\\sigma^2}{n}\\) é igual ao que tínhamos antes. À medida que \\(n\\) cresce, esperamos que \\(X&#39;X\\) cresça, já que é uma soma sobre todos os dados \\(n\\). Dividingo todas as entradas da matriz por \\(n\\) compensa isso. Se a covariância amostral entre todos os preditores fossem iguais a zero (sem correlação), então quando calculássemos a inversa obteríamos apenas a variância amostral de \\(X\\) \\(1/S_x^2\\) na diagonal principal, e temos um termo que já conhecemos de regressão simples. Lá, como só tem um preditor, não tem como ter covariância com outro preditor. Agora podemos falar de erro padrão robusto. Notem que em nossa derivação, a certa altura, tivemos: \\[ \\mathbb{Var}[\\hat{B}|X] = (X&#39;X)^{-1}X&#39;\\sigma^2IX(X&#39;X)^{-1} \\] Se chamarmos \\(\\sigma^2I\\) de \\(\\Omega\\), reescrevo a equação como: \\[ \\mathbb{Var}[\\hat{B}|X] = (X&#39;X)^{-1}X&#39;\\Omega X(X&#39;X)^{-1} \\] E essa equação, escrita desse formato, é chamada de equação sanduíche, pois temos 1. \\(X&#39;\\Omega X\\) no meio 2. \\((X&#39;X)^{-1}\\) nas pontas. Ou seja, a carne \\(X&#39;\\Omega X\\) vai no meio de duas fatias de pão \\((X&#39;X)^{-1}X&#39;\\). Eu suponho que isso seja engraçado de alguma forma (ou talvez fosse uma forma menmômica de memorizar a equação). Mas o fato é que o termo sanduíche pegou de tal forma que nosso erro padrão robusto envolve trocar a “carne”. Veja que definimos \\(\\Omega = \\sigma^2I\\). E \\(\\sigma^2I\\) pressupõe que temos homecedasticidade, já que a variância do erro é constante. Temos portanto de modificar a “carne” para calcular o erro padrão-robusto. Vamos fazer isso manualmente, primeiro calculando o erro padrão tradicional, e depois o que seria um erro padrão robusto. library(ggplot2) library(tidyverse) set.seed(123) x &lt;- c(1:8, 10, 15) y &lt;- c(5 + rnorm(8,sd = 1.5), 40, 65) df &lt;- data.frame(y=y, x=x) df %&gt;% ggplot(aes(x=x, y=y)) + geom_point() fit &lt;- lm(y ~x) summary(fit) ## ## Call: ## lm(formula = y ~ x) ## ## Residuals: ## Min 1Q Median 3Q Max ## -19.736 -6.320 2.465 8.326 12.481 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -11.0831 6.6974 -1.655 0.13655 ## x 4.2401 0.9208 4.605 0.00174 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 11.53 on 8 degrees of freedom ## Multiple R-squared: 0.7261, Adjusted R-squared: 0.6918 ## F-statistic: 21.2 on 1 and 8 DF, p-value: 0.001745 Nós construímos um “toy model” em que não existe relação entre \\(x\\) e \\(y\\). Porém, por causa de dois “outliers” o R achou que existia associação entre as variáveis. Como o R calcula o erro padrão? Nossa fórmula requer \\(\\sigma^2\\), \\(I\\) e a matriz de preditores \\(X\\), que depois vou transpor, calcular inversa etc. No R, podemos computar cada um desses itens manualmente da seguinte forma. n &lt;- length(y) sigma2 &lt;- sigma(fit)^2 mat_I &lt;- diag(n) X &lt;- model.matrix(fit) omega &lt;- sigma2*mat_I bread &lt;- solve(t(X)%*%X) meat &lt;- (t(X) %*% omega %*% X) vce &lt;- bread %*% meat %*% bread sqrt(diag(vce)) ## (Intercept) x ## 6.6974057 0.9208285 Se inspecionarmos omega mais detalhadamente, temos: print(omega) ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] ## [1,] 133.0395 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 ## [2,] 0.0000 133.0395 0.0000 0.0000 0.0000 0.0000 0.0000 ## [3,] 0.0000 0.0000 133.0395 0.0000 0.0000 0.0000 0.0000 ## [4,] 0.0000 0.0000 0.0000 133.0395 0.0000 0.0000 0.0000 ## [5,] 0.0000 0.0000 0.0000 0.0000 133.0395 0.0000 0.0000 ## [6,] 0.0000 0.0000 0.0000 0.0000 0.0000 133.0395 0.0000 ## [7,] 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 133.0395 ## [8,] 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 ## [9,] 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 ## [10,] 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 ## [,8] [,9] [,10] ## [1,] 0.0000 0.0000 0.0000 ## [2,] 0.0000 0.0000 0.0000 ## [3,] 0.0000 0.0000 0.0000 ## [4,] 0.0000 0.0000 0.0000 ## [5,] 0.0000 0.0000 0.0000 ## [6,] 0.0000 0.0000 0.0000 ## [7,] 0.0000 0.0000 0.0000 ## [8,] 133.0395 0.0000 0.0000 ## [9,] 0.0000 133.0395 0.0000 ## [10,] 0.0000 0.0000 133.0395 Como falamos, é uma matriz de variância constante (homocedástica). Em nosso “toy model”, contudo, não faz sentido assumir que a variância é constante. É provável que os dois “outliers” tenham vindo de uma distribuição com variância bem maior. E isso levaria a um erro padrão maior para a o coeficiente da variável \\(x\\). A questão então é como calcular essas variância diferente para esse pontos. Há inúmeras maneiras de fazer isso. A default do Stata, chamada de “HC1”, em que “HC” quer dizer “Heteroscedasticity-Consistent”. A fórmular para o erro padrão robusto “HC1” é a seguinte: \\[ \\frac{n}{n-k}\\hat{e}^2 \\] em que \\(\\hat{e}^2\\) é o quadrado dos resíduos e \\(k\\) o número de parâmetros. Em nosso modelo simples, \\(k=2\\). E substituirmos a variância constante por essa fórmula em nossa “carne”, temos: sigma2_hc1 &lt;- residuals(fit)^2*(n/(n-2)) omega &lt;- sigma2_hc1*mat_I bread &lt;- solve(t(X)%*%X) meat &lt;- (t(X) %*% omega %*% X) vce &lt;- bread %*% meat %*% bread sqrt(diag(vce)) ## (Intercept) x ## 5.863432 0.992626 Vemos que o erro-padrão robusto é maior que o anterior, mas a associação ainda é significativa. O erro padrão robusto do pacote do R “sandwich” usa o “HC3”, que possui outra fórmula, e tem desempenho melhor em amostras pequenas. Sua fórmula é: \\[ \\frac{\\hat{e}^2}{(1-h_i)^2} \\] Aqui, \\(h_i\\) são os valores chapéus da matriz chapéu, \\(H\\), que vimos antes. Os valores chpéus variam entre \\(0\\) e \\(1\\) e quanto maior o número, mais influente a observação. Então, sabemos que nossos dois últimos números terão valores chapéu maiores, ou seja, vão inflar a variância para esses dois valores. No R, podemos calcular os valores chapéus com a função “hatvalues”. sigma2_hc3 &lt;- residuals(fit)^2/(1 - hatvalues(fit))^2 omega &lt;- sigma2_hc3*mat_I bread &lt;- solve(t(X)%*%X) meat &lt;- (t(X) %*% omega %*% X) vce &lt;- bread %*% meat %*% bread sqrt(diag(vce)) ## (Intercept) x ## 9.541330 1.917635 E posso calcular o intervalo de confiança com o novo erro padrão. Obviamente, não iremos fazer na “mão” esse cálculo todo. Vamos usar os pacotes “sandwich” e “lmtest” para fazer o teste de hipótese com erro padrão robusto. library(lmtest) library(sandwich) coeftest(fit, vcovHC(fit, &quot;HC3&quot;)) ## ## t test of coefficients: ## ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -11.0831 9.5413 -1.1616 0.27889 ## x 4.2401 1.9176 2.2111 0.05798 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Vemos, portanto, que nosso erro padrão é robusto a observações muito influentes que são “outliers”. Uma forma de verificar se temos esse problema é ver se temos resíduos grandes e observações muito influentes (valores chapéu altos). Um comando básico do R permite inspecionar visualmente se é o caso. Pontos nos cantos direitos superiores ou inferiores indica observações exibindo influência em nosso modelo. Resíduos grandes (ou variância não constante) pode decorrer de um modelo mal-especificado (preditor ausente, ausência de interação, efeitos não lineares etc.). plot(fit, which = 5) Como mencionado, há vários tipos de erros padrão robustos, “HC0”, “HC1”, “HC2”, “HC3”, entre outros. Vamos falar rapidamente sobre cada uma deles. Antes disso, vamos comparar para nossa regressão como o erro padrão muda para cada um desses tipos. library(sandwich) library(stargazer) library(lmtest) m2 &lt;- coeftest(reg1, vcovHC(reg1, type = &quot;HC0&quot;)) m3 &lt;- coeftest(reg1, vcovHC(reg1, type = &quot;HC1&quot;)) m4 &lt;- coeftest(reg1, vcovHC(reg1, type = &quot;HC2&quot;)) m5 &lt;- coeftest(reg1, vcovHC(reg1, type = &quot;HC3&quot;)) stargazer(reg1, m2, m3, m4, m5, type = &quot;html&quot;) Dependent variable: percentual_2t_17 OLS coefficient test (1) (2) (3) (4) (5) percentual_1t_17 0.778*** 0.778*** 0.778*** 0.778*** 0.778*** (0.005) (0.010) (0.010) (0.010) (0.010) percentual_1t_13 -0.277*** -0.277*** -0.277*** -0.277*** -0.277*** (0.005) (0.008) (0.008) (0.008) (0.008) sg_ufAL -0.101*** -0.101*** -0.101*** -0.101*** -0.101*** (0.005) (0.007) (0.007) (0.007) (0.007) sg_ufAM -0.085*** -0.085*** -0.085*** -0.085*** -0.085*** (0.005) (0.007) (0.007) (0.007) (0.007) sg_ufAP -0.087*** -0.087*** -0.087*** -0.087*** -0.087*** (0.006) (0.008) (0.008) (0.008) (0.008) sg_ufBA -0.086*** -0.086*** -0.086*** -0.086*** -0.086*** (0.004) (0.007) (0.007) (0.007) (0.007) sg_ufCE -0.151*** -0.151*** -0.151*** -0.151*** -0.151*** (0.005) (0.008) (0.008) (0.008) (0.008) sg_ufDF -0.077*** -0.077*** -0.077*** -0.077*** -0.077*** (0.006) (0.007) (0.007) (0.007) (0.008) sg_ufES -0.080*** -0.080*** -0.080*** -0.080*** -0.080*** (0.005) (0.007) (0.007) (0.007) (0.007) sg_ufGO -0.085*** -0.085*** -0.085*** -0.085*** -0.085*** (0.004) (0.007) (0.007) (0.007) (0.007) sg_ufMA -0.105*** -0.105*** -0.105*** -0.105*** -0.105*** (0.004) (0.007) (0.007) (0.007) (0.007) sg_ufMG -0.065*** -0.065*** -0.065*** -0.065*** -0.065*** (0.004) (0.007) (0.007) (0.007) (0.007) sg_ufMS -0.062*** -0.062*** -0.062*** -0.062*** -0.062*** (0.005) (0.007) (0.007) (0.007) (0.007) sg_ufMT -0.089*** -0.089*** -0.089*** -0.089*** -0.089*** (0.004) (0.007) (0.007) (0.007) (0.007) sg_ufPA -0.071*** -0.071*** -0.071*** -0.071*** -0.071*** (0.004) (0.007) (0.007) (0.007) (0.007) sg_ufPB -0.119*** -0.119*** -0.119*** -0.119*** -0.119*** (0.004) (0.007) (0.007) (0.007) (0.007) sg_ufPE -0.113*** -0.113*** -0.113*** -0.113*** -0.113*** (0.004) (0.007) (0.007) (0.007) (0.007) sg_ufPI -0.091*** -0.091*** -0.091*** -0.091*** -0.091*** (0.004) (0.007) (0.007) (0.007) (0.007) sg_ufPR -0.061*** -0.061*** -0.061*** -0.061*** -0.061*** (0.004) (0.007) (0.007) (0.007) (0.007) sg_ufRJ -0.097*** -0.097*** -0.097*** -0.097*** -0.097*** (0.004) (0.007) (0.007) (0.007) (0.007) sg_ufRN -0.112*** -0.112*** -0.112*** -0.112*** -0.112*** (0.005) (0.007) (0.007) (0.007) (0.008) sg_ufRO -0.060*** -0.060*** -0.060*** -0.060*** -0.060*** (0.005) (0.007) (0.007) (0.007) (0.007) sg_ufRR -0.085*** -0.085*** -0.085*** -0.085*** -0.085*** (0.007) (0.007) (0.007) (0.008) (0.008) sg_ufRS -0.061*** -0.061*** -0.061*** -0.061*** -0.061*** (0.004) (0.007) (0.007) (0.007) (0.007) sg_ufSC -0.062*** -0.062*** -0.062*** -0.062*** -0.062*** (0.004) (0.007) (0.007) (0.007) (0.007) sg_ufSE -0.094*** -0.094*** -0.094*** -0.094*** -0.094*** (0.005) (0.007) (0.007) (0.007) (0.008) sg_ufSP -0.039*** -0.039*** -0.039*** -0.039*** -0.039*** (0.004) (0.007) (0.007) (0.007) (0.007) sg_ufTO -0.096*** -0.096*** -0.096*** -0.096*** -0.096*** (0.005) (0.007) (0.007) (0.007) (0.007) sg_ufZZ -0.082*** -0.082*** -0.082*** -0.082*** -0.082*** (0.005) (0.009) (0.009) (0.009) (0.009) Constant 0.356*** 0.356*** 0.356*** 0.356*** 0.356*** (0.006) (0.010) (0.010) (0.011) (0.011) Observations 6,239 R2 0.992 Adjusted R2 0.992 Residual Std. Error 0.020 (df = 6209) F Statistic 25,882.990*** (df = 29; 6209) Note: p&lt;0.1; p&lt;0.05; p&lt;0.01 # conf int library(knitr) coefci(reg1, vcov. = vcovHC(reg1, type = &#39;HC1&#39;)) %&gt;% kable() ## Warning in attr(x, &quot;align&quot;): &#39;xfun::attr()&#39; is deprecated. ## Use &#39;xfun::attr2()&#39; instead. ## See help(&quot;Deprecated&quot;) ## Warning in attr(x, &quot;format&quot;): &#39;xfun::attr()&#39; is deprecated. ## Use &#39;xfun::attr2()&#39; instead. ## See help(&quot;Deprecated&quot;) 2.5 % 97.5 % (Intercept) 0.3350033 0.3760590 percentual_1t_17 0.7583797 0.7972015 percentual_1t_13 -0.2936573 -0.2610519 sg_ufAL -0.1151015 -0.0872385 sg_ufAM -0.0986993 -0.0707314 sg_ufAP -0.1026175 -0.0715529 sg_ufBA -0.1003028 -0.0724807 sg_ufCE -0.1662120 -0.1354263 sg_ufDF -0.0915291 -0.0632155 sg_ufES -0.0937037 -0.0658873 sg_ufGO -0.0991474 -0.0716567 sg_ufMA -0.1192041 -0.0914571 sg_ufMG -0.0792045 -0.0516851 sg_ufMS -0.0757868 -0.0480581 sg_ufMT -0.1024452 -0.0747959 sg_ufPA -0.0851832 -0.0573345 sg_ufPB -0.1332370 -0.1051214 sg_ufPE -0.1273447 -0.0994480 sg_ufPI -0.1054809 -0.0773482 sg_ufPR -0.0743198 -0.0468815 sg_ufRJ -0.1104185 -0.0828524 sg_ufRN -0.1266669 -0.0978309 sg_ufRO -0.0735371 -0.0456145 sg_ufRR -0.0999596 -0.0706939 sg_ufRS -0.0748960 -0.0474614 sg_ufSC -0.0752199 -0.0478245 sg_ufSE -0.1085156 -0.0799776 sg_ufSP -0.0531533 -0.0256020 sg_ufTO -0.1098511 -0.0820642 sg_ufZZ -0.0987445 -0.0643491 12.8.1 Entendendo o erro padrão robusto Vamos fazer uma simulação no R para entender os vários tipos de erro padrão robusto. Vamos criar um modelo com heterocedasticidade. x &lt;- rnorm(10) e &lt;- rnorm(10, 0, x^2) # o DP do erro é igual a .5*x^2 a &lt;- 2 b &lt;- -2 y &lt;- a + b*x + e df &lt;- data.frame(y=y, x=x) df %&gt;% ggplot(aes(y=y, x=x)) + geom_point() + geom_smooth(method=&quot;lm&quot;) ## `geom_smooth()` using formula = &#39;y ~ x&#39; m_sim &lt;- lm(y ~ x, data=df) # plots de checagem do modelo library(easystats) check_model(m_sim) f2 &lt;- coeftest(m_sim, vcovHC(m_sim, type = &quot;HC0&quot;)) f3 &lt;- coeftest(m_sim, vcovHC(m_sim, type = &quot;HC1&quot;)) f4 &lt;- coeftest(m_sim, vcovHC(m_sim, type = &quot;HC2&quot;)) f5 &lt;- coeftest(m_sim, vcovHC(m_sim, type = &quot;HC3&quot;)) stargazer(m_sim, f2, f3, f4, f5, type = &quot;html&quot;) Dependent variable: y OLS coefficient test (1) (2) (3) (4) (5) x -3.230*** -3.230*** -3.230*** -3.230*** -3.230*** (0.402) (0.529) (0.591) (0.696) (0.923) Constant 1.455*** 1.455*** 1.455*** 1.455*** 1.455** (0.403) (0.341) (0.381) (0.424) (0.538) Observations 10 R2 0.890 Adjusted R2 0.876 Residual Std. Error 1.272 (df = 8) F Statistic 64.688*** (df = 1; 8) Note: p&lt;0.1; p&lt;0.05; p&lt;0.01 # dados de 2022 #read into data frame presid_22 &lt;- fread(here(&quot;dados&quot;,&quot;votacao_secao_2022_BR.csv&quot;), encoding = &quot;Latin-1&quot;) df_resultados_22_aux &lt;- presid_22 %&gt;% dplyr::filter(!NR_VOTAVEL %in% c(95,96)) %&gt;% group_by(NR_ZONA, CD_MUNICIPIO, SG_UF, NR_VOTAVEL, NR_TURNO) %&gt;% summarise(total_votos = sum(QT_VOTOS)) %&gt;% pivot_wider(names_from = NR_TURNO, values_from = total_votos, values_fill = 0) %&gt;% janitor::clean_names() %&gt;% group_by(nr_zona, cd_municipio, sg_uf) %&gt;% mutate(total_validos_1t = sum(x1), total_validos_2t = sum(x2)) %&gt;% dplyr::filter(nr_votavel %in% c(13,22)) %&gt;% group_by(nr_votavel) %&gt;% mutate(percentual_1t = x1/total_validos_1t, percentual_2t = x2/total_validos_2t) %&gt;% ungroup() df_resultados_22 &lt;- df_resultados_22_aux%&gt;% dplyr::select(-c(x1, x2, total_validos_1t, total_validos_2t)) %&gt;% pivot_wider(names_from = nr_votavel, values_from = c(percentual_1t, percentual_2t)) %&gt;% rename(percentual_1t_17 = percentual_1t_22, percentual_2t_17 = percentual_2t_22) Agora que importamos os dados de 22, podemos fazer nossa previsão, usando os resultados do primeiro turno. df_resultados_22_aux1 &lt;- df_resultados_22_aux %&gt;% ungroup() %&gt;% group_by(nr_zona, cd_municipio, sg_uf) %&gt;% summarise(total_validos_1t = sum(total_validos_1t)) #Previsão previsoes &lt;- predict(reg1, newdata = df_resultados_22, interval = &quot;prediction&quot;, level = .95) %&gt;% as.data.frame() df_resultados_22_final &lt;- df_resultados_22 %&gt;% ungroup() %&gt;% mutate(prev_perc = previsoes$fit, prev_perc_lower = previsoes$lwr, prev_perc_upper = previsoes$upr, validos_prev = df_resultados_22_aux1$total_validos_1t*prev_perc, validos_prev_lower = df_resultados_22_aux1$total_validos_1t*prev_perc_lower, validos_prev_upper = df_resultados_22_aux1$total_validos_1t*prev_perc_upper) tot_valido &lt;- sum(df_resultados_22_aux1$total_validos_1t) df_resultados_22_final %&gt;% summarise(perc_previsto = sum(validos_prev,na.rm = T)/tot_valido, perc_previsto_lower = sum(validos_prev_lower,na.rm = T)/tot_valido, perc_previsto_upper = sum(validos_prev_upper,na.rm = T)/tot_valido) ## # A tibble: 1 × 3 ## perc_previsto perc_previsto_lower perc_previsto_upper ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.481 0.442 0.521 12.9 Termo de interação No trabalho de Corrêa (2015), o autor estima um modelo para explicar o efeito do desempenho econômico (crescimento do PIB e inflação) sobre o desempenho eleitoral de candidatos governistas na América Latina. E a certa altura argumenta que o efeito do crescimento do PIB e da inflação dependem (ou variam) conforme o candidato é de esquerda, centro ou de direita. Em particular, candidatos de centro seriam mais afetados por essas variáveis. Vamos simplificar aqui e considerar apenas uma variável econômica (crescimento) e tratar a variável ideologia do presidente como binária (centro ou não). Nesse caso, dizemos que o modelo contém um termo interativo, do seguinte modo: \\[ y_i = \\alpha + \\beta_1 \\cdot x_1 + \\beta_2 \\cdot x_2 + \\beta_3 \\cdot x_1 \\cdot x_2 + e_i \\] Nesse tipo de modelo, o efeito de \\(x_1\\) depende do valor de \\(x_2\\) (e vice-versa). Isso significa que temos que mudar nossa interpretação do significado dos parâmetros. Agora, \\(\\beta_1\\) mede o efeito no \\(y\\) médio de aumentar uma unidade \\(x_1\\) quando \\(x_2 = 0\\). Para ver isso, podemos substiruir \\(x_2 = 0\\) na equação acima, e temos: \\[ y_i = \\alpha + \\beta_1 \\cdot x_1 + \\beta_2 \\cdot 0 + \\beta_3 \\cdot x_1 \\cdot 0 + e_i = \\alpha + \\beta_1 \\cdot x_1 + e_i \\] E nesse caso, \\[\\begin{align} \\mathbb{E}[Y|x_1=1] - \\mathbb{E}[Y|X=0] = \\\\ \\mathbb{E}[\\alpha|x_1=1] + \\mathbb{E}[\\beta_1 \\cdot x_1|x_1=1] + \\mathbb{E}[e_i|x_1=1] - (\\mathbb{E}[\\alpha|x_1=0] + \\mathbb{E}[\\beta_1 \\cdot x_1|x_1=0] + \\mathbb{E}[e_i|x_1=0]) \\\\ = \\alpha + \\beta_1 - \\alpha = \\\\ \\beta_1 \\end{align}\\] Que é o que já estamos acostumados. Porém, considere o caso em que \\(x_2 \\neq 0\\). \\(y_i = \\alpha + \\beta_1 \\cdot x_1 + \\beta_2 \\cdot x_2 + \\beta_3 \\cdot x_1 \\cdot x_2 + e_i\\) Estamos interessados em calcular: \\(\\mathbb{E}[Y|x_1=1] - \\mathbb{E}[Y|X=0]\\). Comecemos pelo primeiro termo. \\[\\begin{align} \\mathbb{E}[Y|x_1=1] = \\\\ \\mathbb{E}[\\alpha|x_1=1] + \\mathbb{E}[\\beta_1 \\cdot 1|x_1=1] + \\mathbb{E}[\\beta_2 \\cdot x_2|x_1=1] + \\mathbb{E}[\\beta_3 \\cdot 1 \\cdot x_2|x_1=1] + \\mathbb{E}[e_i|x_1=1] \\\\ = \\alpha + 1\\beta_1 + \\beta_2\\mathbb{E}[x_2|x_1=1] + \\beta_3\\mathbb{E}[x_2|x_1=1] + 0 \\\\ = \\alpha + \\beta_1 + (\\beta_2+\\beta_3)\\mathbb{E}[x_2|x_1=1] \\end{align}\\] Já o segundo termo, fica: \\[\\begin{align} \\mathbb{E}[Y|x_1=0] = \\\\ \\mathbb{E}[\\alpha|x_1=0] + \\mathbb{E}[\\beta_1 \\cdot 0|x_1=0] + \\mathbb{E}[\\beta_2 \\cdot x_2|x_1=0] + \\mathbb{E}[\\beta_3 \\cdot 0 \\cdot x_2|x_1=0] + \\mathbb{E}[e_i|x_1=0] \\\\ = \\alpha + 0\\beta_1 + \\beta_2\\mathbb{E}[x_2|x_1=0] + 0\\mathbb{E}[x_2|x_1=1] + 0 \\\\ = \\alpha + \\beta_2 \\mathbb{E}[x_2|x_1=0] \\end{align}\\] Computando a diferença, temos: \\[\\begin{align} \\mathbb{E}[Y|x_1=1] - \\mathbb{E}[Y|x_1=0] = \\\\ \\alpha + \\beta_1 + (\\beta_2+\\beta_3)\\mathbb{E}[x_2|x_1=1] -\\alpha - \\beta_2\\mathbb{E}[x_2|x_1=0] \\\\ = \\beta_1 + (\\beta_2+\\beta_3)\\mathbb{E}[x_2|x_1=1] - \\beta_2\\mathbb{E}[x_2|x_1=0] \\end{align}\\] Se \\(x_1\\) e \\(x_2\\) forem independentes na média, então: \\(\\mathbb{E}[x_2|x_1=1] = \\mathbb{E}[x_2|x_1=0]\\), de forma que a equação acima simplifica para: \\[\\begin{align} \\mathbb{E}[Y|x_1=1] - \\mathbb{E}[Y|x_1=0] = \\beta_1 + \\beta_3\\mathbb{E}[x_2|x_1=1] \\end{align}\\] Portanto, a interpretação do que é o efeito marignal ou preditivo Em um modelo com termo de interação é mais complexa do que em uma regressão sem termo de interação. 12.9.1 Variância do termo de interação O erro-padrão do \\(\\beta\\) é dado por: \\(\\mathbb{Var}[\\beta|X] = \\sigma^2 (X&#39;X)^{-1}\\), que podemos reescrever para: \\(\\sigma^2(X&#39;X)^{-1} = \\frac{\\sigma^2}{n}n^{-1}(X&#39;X)^{-1}\\). Lebrem-se que para um único preditor, \\(\\mathbb{Var}[\\beta|X] = \\frac{\\sigma^2}{nS^2_x}\\) Vejam que \\(\\frac{\\sigma^2}{n}\\) é o que tínhamos antes. E \\(n^{-1}(X&#39;X)^{-1}\\) é uma matriz de variância e covariância. Se as covariâncias forem zero, na diagonal principal temos as variâncias amostrais de cada preditor, de tal modo que: Supondo que os preditores \\(X_1, X_2, \\ldots, X_p\\) são não correlacionados, a matriz \\(X&#39;X\\) será aproximadamente diagonal. Assim, podemos representar a matriz de variância e covariância apenas com as diagonais principais, onde cada elemento \\(S_{x_j}^2\\) na diagonal representa a variância amostral do preditor \\(X_j\\). Dessa forma, temos: \\[ \\mathbb{Var}[\\hat{\\beta} \\mid X] = \\frac{\\sigma^2}{n} \\begin{bmatrix} \\frac{1}{S_{x_1}^2} &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; \\frac{1}{S_{x_2}^2} &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\cdots &amp; \\frac{1}{S_{x_p}^2} \\\\ \\end{bmatrix}. \\] No caso de um único preditor (\\(p = 1\\)), essa expressão se reduz para a fórmula clássica: \\[ \\mathbb{Var}[\\hat{\\beta} \\mid X] = \\frac{\\sigma^2}{n S_x^2}, \\] onde \\(S_x^2\\) é a variância amostral do preditor \\(X\\). Assim, cada elemento da diagonal principal da matriz \\(\\mathbb{Var}[\\hat{\\beta} \\mid X]\\) representa a variância de \\(\\hat{\\beta}_j\\) para o preditor \\(X_j\\), dada por \\(\\frac{\\sigma^2}{n S_{x_j}^2}\\), generalizando a fórmula da variância para \\(p\\) preditores. Obviamente, se houver correlação entre preditores, a fórmula se torna mais complicada do que a original. No caso de termo de interação, o pressuposto é de que há correlação, então fiquemos com a fórmula matricial, que é mais simples: \\[ \\mathbb{Var}[\\beta|X] = \\sigma^2 (X&#39;X)^{-1} \\] E em geral a variância de cada coeficiente depende dos demais. E quanto maior a covariância entre duqas variáveis, maior a variância (e portanto, maior o erro-padrão). O que importa para nós aqui é na verdade o seguinte. Sejam \\(X\\) e \\(Y\\) duas variáveis aleatórias dependentes (cuja covariância não é zero). Então, \\(Var(X+Y) = Var(X) + Var(Y) + 2*Cov(X,Y)\\). Portanto, no caso de nosso modelo com dois regressores e termo de interação, temos: \\[ Var(\\beta_1 + \\beta3 x_2|X) = Var(\\beta_1|X) + Var(\\beta3 x_2|X) + 2Cov(\\beta_1, \\beta3) = Var(\\beta_1|X) + x_2^2Var(\\beta3|X) + 2x_2Cov(\\beta_1, \\beta3|X) \\] Como as variâncias são os erros-padrão ao quadrado, podemos computar manualmente a variância do efeito marginal, e depois calcular o erro-padrão. No R, porém, usualmente usamos alguns dos pacotes que permite a visualização desses efeitos já computados corretamente. library(marginaleffects) library(sjPlot) set.seed(123) n &lt;- 100 x1 &lt;- rnorm(n) x2 &lt;- rbinom(n, size=1, p=.5) a &lt;- 2 b1 &lt;- 1.5 b2 &lt;- -1 b3 &lt;- 1 y &lt;- a+ b1*x1 + b2*x2 + b3*x2*x1 + rnorm(n) reg1 &lt;- lm(y ~ x1*x2) summary(reg1) ## ## Call: ## lm(formula = y ~ x1 * x2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.4291 -0.6824 -0.1309 0.5935 3.1949 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.9987 0.1299 15.381 &lt; 2e-16 *** ## x1 1.4690 0.1482 9.910 2.32e-16 *** ## x2 -0.9273 0.1890 -4.905 3.80e-06 *** ## x1:x2 0.8029 0.2070 3.878 0.000193 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.9368 on 96 degrees of freedom ## Multiple R-squared: 0.7872, Adjusted R-squared: 0.7805 ## F-statistic: 118.4 on 3 and 96 DF, p-value: &lt; 2.2e-16 # Pacote marginaleffects plot_slopes(reg1, variables = &quot;x1&quot;, condition = &quot;x2&quot;) # pacote sjPlot plot_model(reg1, type = &quot;pred&quot;, terms = c(&quot;x1&quot;, &quot;x2&quot;)) # Truque do Mcdermont reg2 &lt;- lm(y ~ factor(x2) / x1) # lm(y ~ x2 + x2:x1) summary(reg2) ## ## Call: ## lm(formula = y ~ factor(x2)/x1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.4291 -0.6824 -0.1309 0.5935 3.1949 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.9987 0.1299 15.381 &lt; 2e-16 *** ## factor(x2)1 -0.9273 0.1890 -4.905 3.80e-06 *** ## factor(x2)0:x1 1.4690 0.1482 9.910 2.32e-16 *** ## factor(x2)1:x1 2.2719 0.1445 15.724 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.9368 on 96 degrees of freedom ## Multiple R-squared: 0.7872, Adjusted R-squared: 0.7805 ## F-statistic: 118.4 on 3 and 96 DF, p-value: &lt; 2.2e-16 12.9.2 Calculando o erro padrão do termo de interação 12.10 Análise de sensibilidade causal Diante do problema de viés de variável omitida, podemos realizar análises de sensibilidade. Essa é uma área que em que houve bastante desenvolvimentos recentes. Apresento aqui o trabalho do Cinelli e Hazlet (2019). Antes, porém, vamos revisar o R-quadrado e derivar algumas formas de definir essa quantidade. 12.10.1 Derivação do R-quadrado Será útil reescrever o viés de variável omitida em termos do R-quadrado. Então, vamos derivar o R-quadrado. Antes vamos definir algumas quantidades que serão úteis. Seja \\(T\\) quanto o vetor de respostas \\(Y\\) se desvia da média amostral \\(\\bar{Y}\\). \\[ T = Y - \\bar{Y} \\] E \\(T\\) pode ser decomposto em duas partes. Primeiro, a porção do desvio em \\(T\\) que é determinado pelo modelo de regressão, dada pelo vetor \\(\\hat{M}\\), definido como: \\[ \\hat{M} = \\hat{Y} - \\bar{Y} \\] E em segundo lugar, a porção do desvio em \\(T\\) determinado pelos resíduos: \\[ \\hat{e} = Y - \\hat{Y} \\] Portanto, \\(T = \\hat{M} + \\hat{e}\\). Se nós somarmos o quadrado dessas três quantidades para toda a amostra, teremos: Soma dos Quadrados Totais (em inglês SST) \\[ SST = \\sum{(y_i - \\bar{y_i})^2} \\] Soma dos Quadrados Médios (SQM) \\[ SSM = \\sum{(\\hat{y_i} - \\bar{y})^2} \\] Soma dos Quadrados dos Resíduos (SSE) \\[ SSE = \\sum{\\hat{e_i}^2} \\] Vamos mostrar que, do mesmo jeito que \\(\\hat{T} = \\hat{M} + \\hat{E}\\), \\(SST = SSM + SSE\\) Lembremos que \\(\\sum{\\hat{e_i}} = 0\\) e \\(\\sum{\\hat{e_i}\\hat{y_i}} = 0\\) \\[\\begin{align} SST = \\sum{(y_i - \\bar{y_i})^2} = \\\\ \\sum{(\\hat{e_i} + \\hat{y_i} - \\bar{y_i})^2} = \\\\ \\sum{(\\hat{e_i} + (\\hat{y_i} - \\bar{y_i}))^2} = \\\\ \\sum{\\hat{e_i}^2 + 2\\hat{e_i}(\\hat{y_i} - \\bar{y_i}) + (\\hat{y_i} - \\bar{y_i})^2} = \\\\ \\sum{\\hat{e_i}^2} + 2\\sum{\\hat{e_i}(\\hat{y_i} - \\bar{y_i})} + \\sum{(\\hat{y_i} - \\bar{y_i})^2} = \\\\ SSE + 2\\sum{(\\hat{e_i}\\hat{y_i} - \\hat{e_i}\\bar{y_i})} + SSM = \\\\ SSE + 2\\sum{(\\hat{e_i}\\hat{y_i}} - 2\\sum{\\hat{e_i}\\bar{y_i})} + SSM = \\\\ SSE + 0 - 0 + SSM = \\\\ SST = SSE + SSM = \\\\ \\end{align}\\] A intuição dessas quantidades é que SST é uma medida da variação na resposta, \\(Y\\) (é a variância sem dividir por \\(n\\)); SSM é uma medida dessa variação em \\(Y\\) preditiva ou captada pela variação do modelo, e SSE a variação em \\(Y\\) não capturada pelo modelo. Note também que se eu dividir ambos os lados da equação por \\(n\\), tenho a variância do lado esquerdo, e não altero a igualdade da equação. E o \\(R^2\\) é definido pela razão entre SSM e SST: \\[ R^2 = \\frac{SSM}{SST} = \\frac{Var(\\hat{y})}{Var(y)} \\] O \\(R^2\\) pode ser reescrito de várias maneiras. Veja que \\(y = \\hat{y} + \\hat{e}\\), logo, \\(\\hat{y} = y - \\hat{e}\\). E note que \\(Cov(\\hat{y}, \\hat{e}) = 0\\). Portanto, temos: \\[ R^2 = \\frac{Var(y - \\hat{e})}{Var(y)} = 1 - \\frac{Var(\\hat{e})}{Var(y)} = 1 - \\frac{Var(Y^{\\perp X}))}{Var(y)} \\] Ou seja, o R-quadrado é \\(1\\) menos a variância em \\(Y\\) não é predita por \\(X\\), dividido pela variância de \\(Y\\). Vamos mostrar agora que o R-quadrado pode ser também definido como a correlação (ao quadrado) entre \\(Y\\) e \\(\\hat{y}\\). Primeiro, vamos mostrar que \\[ R^2 = \\frac{Cov(Y, \\hat{y})}{Var(y)} \\] \\[ Cov(Y, \\hat{y}) = Cov(\\hat{y} + \\hat{e}, \\hat{y}) = Var(\\hat{y}) + Cov(\\hat{e}, \\hat{y}) = Var(\\hat{y}) \\] E podemos mostrar que: \\[ Var(\\hat{y}) = Var(\\hat{\\alpha} + \\hat{\\beta} X) = Var(\\hat{\\beta}X) = \\hat{\\beta}^2 Var( X) \\] Portanto, \\[ R^2 = \\frac{\\hat{\\beta}^2 Var(X)}{Var(y)} \\] E a fórmula do \\(\\hat{\\beta} = Cov(X,Y)/Var(x)\\). Logo: \\[ R^2 = \\frac{ Cov(X,Y)^2 Var(X)}{Var(x)^2 Var(y)} = \\frac{ Cov(X,Y)^2}{Var(x)Var(y)} \\] Designando a variância de \\(x\\) por \\(S_x^2\\) e similarmente para \\(y\\), temos: \\[ R^2 = \\frac{ Cov(X,Y)^2}{S_x^2S_y^2} = \\left(\\frac{ Cov(X,Y)}{S_xS_y}\\right)^2 \\] Vamos agora definir o R-quadrado parcial, para regressão múltiplas. Seja uma regressão completa (full) \\(y_i = \\alpha + \\beta_1 x_{1i} + \\beta_2 x_{2i} + e_i\\) e a regressão restrita, dada por: \\(y_i = \\alpha + \\beta_1 x_{1i} + e_i\\). O R-quadrado parcial é definido como: \\[ R^2_{partial} = \\frac{SSE(reduced) - SSE(full)}{SSE(reduced)} \\] Veja que posso definir o R-quadrado parcial a partir da regressão restria só com intercepto, sem preditor, e a regressão completa com um preditor. No modelo restrito, \\(\\hat{y} = \\bar{y}\\), o que implica que \\(SSE(reduced) = SST\\). Logo, temos: \\[ R^2_{partial} = \\frac{SST - SSE(full)}{SST} = 1 - \\frac{SSE}{SST} \\] Que é nossa definição de R-quadrado em modelo de regressão simples. Então, o R-quadrado parcial me dá uma medida da proporção da variância predita pelo modelo completo que não pode ser predita pelo modelo simples.Ainda em outras palavras, o R quadrado parcial mede o poder preditivo adicional do modelo reduzido comparado ao completo. No caso de modelos com uma e duas variáveis, estou vendo o poder preditivo de adicionar uma variável ao modelo. Em termos de notação, podemos escrever o R-quadrado parcial do seguinte modo: \\[ R^2_{y \\sim x1|x2} = 1 - \\frac{Var(y^{\\perp x_1, x_2})}{Var(y^{\\perp x_1})} = \\left(Cor(x_1^{\\perp x_2}, y^{\\perp x_2})\\right)^2 \\] library(here) library(data.table) library(tidyverse) library(sjlabelled) # pra remover labelled variables library(haven) library(janitor) library(lubridate) ## dados # https://www.latinobarometro.org/latContents.jsp lat_bar23 &lt;- sjlabelled::read_spss(here(&quot;Dados&quot;, &quot;Latinobarometro_2023_Eng_Spss_v1_0.sav&quot;), drop.labels = TRUE) ## Invalid date string (length=9): 09 032 23 lat_bar23 &lt;- lat_bar23 %&gt;% mutate(S17 = as.Date(as.character(S17), format=&quot;%Y%m%d&quot;)) lat_bar23 &lt;- lat_bar23 %&gt;% janitor::clean_names() # get_label(lat_bar23) lat_bar23 &lt;- lat_bar23 %&gt;% mutate(data_base = as.Date(paste(diareal, mesreal, &quot;2023&quot;, sep=&quot;-&quot;), &quot;%d-%m-%Y&quot;), idade = year(as.period(interval(s17,data_base))), econ_12_meses = ifelse(p6stgbs %in% c(1,2), &quot;better&quot;, ifelse(p6stgbs == 8, NA, &quot;other&quot;)), econ_12_meses = relevel(as.factor(econ_12_meses), ref = &quot;other&quot;), aprovacao_presidente = ifelse(p15stgbs == 0, NA, p15stgbs), ideologia = ifelse(p16st %in% c(97, 98, 99), NA, p16st), votaria_governo = ifelse(perpart == 4, NA, ifelse(perpart == 1, 1, 0)), genero = factor(sexo, labels = c(&quot;homem&quot;, &quot;mulher&quot;)), evangelico = ifelse(s1 %in% c(0,98), NA, ifelse(s1 %in% c(2,3,4,5), 1, 0))) # não considera adventista, testemunha Jeová, Mórmon ## Warning: There was 1 warning in `mutate()`. ## ℹ In argument: `idade = year(as.period(interval(s17, data_base)))`. ## Caused by warning in `.local()`: ## ! NAs introduced by coercion to integer range br_latbar_23 &lt;- lat_bar23 %&gt;% mutate(idenpa = remove_all_labels(idenpa)) %&gt;% # haven_labelled problems filter(idenpa == 76) %&gt;% ## seelciona brasil filter(!is.na(votaria_governo) &amp; !is.na(evangelico) &amp; !is.na(ideologia) &amp; !is.na(econ_12_meses)) glimpse(br_latbar_23) ## Rows: 472 ## Columns: 282 ## $ numinves &lt;fct&gt; 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23,… ## $ idenpa &lt;fct&gt; 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76,… ## $ numentre &lt;dbl&gt; 372, 373, 1100, 351, 15, 1101, 375, 953, 84… ## $ reg &lt;fct&gt; 76002, 76002, 76004, 76002, 76001, 76004, 7… ## $ ciudad &lt;fct&gt; 76002020, 76002020, 76004129, 76002192, 760… ## $ tamciud &lt;fct&gt; 7, 7, 4, 6, 8, 4, 7, 7, 7, 6, 7, 8, 4, 4, 6… ## $ comdist &lt;dbl&gt; 35, 35, 102, 34, 3, 102, 35, 89, 80, 34, 80… ## $ edad &lt;dbl&gt; 57, 37, 18, 47, 47, 33, 47, 65, 26, 55, 35,… ## $ sexo &lt;fct&gt; 2, 2, 2, 1, 1, 2, 1, 1, 2, 2, 1, 2, 2, 2, 2… ## $ codigo &lt;dbl&gt; 80, 80, 9, 78, 20, 9, 80, 53, 44, 78, 44, 4… ## $ diareal &lt;fct&gt; 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3… ## $ mesreal &lt;fct&gt; 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3… ## $ ini &lt;dbl&gt; 757, 905, 913, 909, 946, 1002, 1046, 1010, … ## $ fin &lt;dbl&gt; 859, 946, 956, 1002, 1035, 1042, 1123, 1059… ## $ dura &lt;dbl&gt; 62, 41, 43, 53, 49, 40, 37, 49, 49, 65, 38,… ## $ totrevi &lt;dbl&gt; 4, 3, 6, 3, 0, 1, 6, 0, 1, 1, 0, 0, 0, 2, 0… ## $ totcuot &lt;dbl&gt; 99, 0, 0, 99, 0, 0, 99, 0, 0, 99, 0, 0, 0, … ## $ totrech &lt;dbl&gt; 5, 0, 0, 2, 0, 1, 2, 0, 0, 6, 2, 3, 1, 1, 1… ## $ totperd &lt;dbl&gt; 2, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0… ## $ numcasa &lt;dbl&gt; 6, 1, 1, 3, 1, 2, 3, 1, 1, 7, 3, 4, 2, 2, 2… ## $ codsuper &lt;dbl&gt; 2, 2, 13, 2, 1, 13, 2, 11, 10, 2, 10, 10, 4… ## $ supervvi &lt;fct&gt; 2, 1, 2, 2, 1, 1, 2, 2, 2, 2, 2, 1, 2, 1, 2… ## $ superven &lt;fct&gt; 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 2, 1, 1… ## $ codif &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1… ## $ digit &lt;dbl&gt; 80, 80, 9, 78, 20, 9, 80, 53, 44, 78, 44, 4… ## $ p1st &lt;fct&gt; 4, 3, 2, 2, 2, 2, 3, 1, 2, 2, 2, 3, 2, 2, 1… ## $ p2st &lt;fct&gt; 1, 2, 2, 3, 2, 3, 2, 2, 2, 2, 1, 1, 2, 1, 1… ## $ p3n &lt;fct&gt; 4, 4, 2, 2, 3, 3, 2, 3, 2, 4, 3, 2, 3, 3, 2… ## $ p4stgbs &lt;fct&gt; 16, 16, 36, 16, 5, 4, 9, NA, 16, 9, 26, 16,… ## $ p5stgbs &lt;fct&gt; 5, 4, 3, 4, 3, 5, 4, 2, 4, 3, 3, 3, 5, 3, 4… ## $ p6stgbs &lt;fct&gt; 3, 3, 2, 2, 3, 3, 3, 3, 3, 2, 2, 3, 2, 2, 2… ## $ p7st &lt;fct&gt; 1, 2, 2, 1, 2, 3, 5, 2, 5, 4, 2, 3, 2, 2, 2… ## $ p8stgbs &lt;fct&gt; 1, 2, 3, 1, 2, 3, 2, 1, 3, 1, 2, NA, 2, 2, … ## $ p9stgbs &lt;fct&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1… ## $ p10stgbs &lt;fct&gt; 1, 3, 1, 1, 3, 3, 1, 3, NA, 1, 1, NA, 3, NA… ## $ p11stgbs_a &lt;fct&gt; 2, 3, 3, 3, 3, 4, 4, 3, NA, 3, 2, 3, 3, 3, … ## $ p11stgbs_b &lt;fct&gt; 3, 4, 3, 3, 3, 4, 3, NA, 4, 4, 2, 3, 3, 3, … ## $ p12st &lt;dbl&gt; 2, 2, 1, 2, 1, 1, 1, 2, NA, 1, 2, 2, 2, 2, … ## $ p13stgbs_a &lt;fct&gt; 3, 3, 2, 1, 1, 4, 1, 1, 1, 2, 3, 3, 4, 3, 2… ## $ p13stgbs_b &lt;fct&gt; 2, 3, 1, 1, 1, 1, 3, 1, 1, 1, 2, 3, 3, 1, 2… ## $ p13st_c &lt;fct&gt; 2, 3, 3, 1, 1, 3, 1, 1, 1, 1, 4, 1, 1, 3, 4… ## $ p13st_d &lt;fct&gt; 1, 3, 3, 2, 1, 4, 3, 2, 2, 2, 1, 3, 3, NA, … ## $ p13st_e &lt;fct&gt; 1, 3, 3, 1, 3, 4, 4, 1, 2, 3, 3, 2, 3, 3, 2… ## $ p13st_f &lt;fct&gt; 1, 3, 3, 1, 1, 4, 4, 1, 3, 4, 1, 3, 2, 3, 1… ## $ p13st_g &lt;fct&gt; 2, 4, 3, 2, 3, 4, 3, 4, 3, 3, 2, 3, 4, NA, … ## $ p13st_h &lt;fct&gt; 1, 3, 3, 1, 1, 4, 4, 1, 2, 4, 1, 3, 1, NA, … ## $ p13st_i &lt;fct&gt; 1, 3, 3, 1, 3, 3, 4, 1, 3, 3, 1, 1, 1, 3, 2… ## $ p14st_a &lt;fct&gt; 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1… ## $ p14st_b &lt;fct&gt; 2, 3, 2, 2, 1, 3, 3, 1, 2, 2, 2, 4, 3, 3, 4… ## $ p14st_c &lt;fct&gt; 2, 4, 3, 2, 3, 3, 4, 4, 2, 2, 4, 4, 3, 3, 2… ## $ p14st_d &lt;fct&gt; 1, 4, 2, 2, 3, 3, 4, 1, 3, 4, 4, 3, 2, 2, 3… ## $ p14st_e &lt;fct&gt; 2, 4, 2, 2, 3, 3, 3, 1, 3, 4, 2, NA, 4, 3, … ## $ p14st_f &lt;fct&gt; 2, 3, 1, 2, 1, 2, 3, 1, 2, 3, 2, 4, 4, 3, 2… ## $ p14st_g &lt;fct&gt; 1, 4, 2, 2, 1, 3, 2, 1, 2, 3, 2, 3, 3, 1, 2… ## $ p14st_h &lt;fct&gt; 2, 3, 2, 2, 3, 2, 3, 1, 2, 3, 2, 4, 3, NA, … ## $ p14n_i &lt;fct&gt; 3, 4, 2, 2, 3, 4, 3, 1, 3, 4, 4, 4, 4, 3, 4… ## $ p15stgbs &lt;fct&gt; 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1… ## $ p16st &lt;fct&gt; 5, 6, 6, 5, 6, 5, 10, 10, 5, 8, 5, 7, 6, 10… ## $ p17st &lt;fct&gt; 3, 2, 3, 2, 1, 3, 4, 1, 4, 2, 3, 3, 4, 2, 4… ## $ p18st_a &lt;fct&gt; 1, 1, 2, 1, 1, 1, 1, 1, 3, 1, 1, 1, 2, 1, 1… ## $ p18stm_b &lt;fct&gt; 1, 4, 3, 3, 2, 2, 1, 1, 4, 4, 4, 1, 4, 1, 4… ## $ p18stm_c &lt;fct&gt; 3, 1, 3, 2, 2, 4, 4, 1, 4, 2, 4, 1, 2, 4, 4… ## $ p18n_d &lt;fct&gt; 2, 2, 2, 2, 2, 4, 1, 4, 2, 1, 2, 4, 2, NA, … ## $ p18n_e &lt;fct&gt; 3, 3, 2, 3, 2, 3, 1, 3, 2, 3, 1, 1, 3, NA, … ## $ p18n_f &lt;fct&gt; 2, 4, 3, 3, 2, 4, 2, 4, 4, 2, 3, 4, 3, 1, 3… ## $ p18n_g &lt;fct&gt; 2, 1, 2, 2, 1, 1, 1, 4, 4, 1, 3, 1, 1, 1, 2… ## $ p18n_h &lt;fct&gt; 1, 2, 2, 2, 1, 1, 1, 1, 2, 1, 3, 1, 1, 4, 1… ## $ p18st_i &lt;fct&gt; 2, 1, 2, 1, 1, 4, 2, 1, 4, 1, 1, 4, 3, 1, 1… ## $ p19n &lt;fct&gt; 2, 2, 2, 2, 2, 2, 1, 1, 2, 2, 1, 1, 2, 1, 2… ## $ p20stm &lt;fct&gt; 1, 2, 1, 2, 2, 2, 2, 2, 1, 1, 2, 2, 2, NA, … ## $ p21st &lt;fct&gt; 1, 2, 1, 2, 2, 2, 1, 1, 2, 1, 1, 2, 2, NA, … ## $ p22st &lt;fct&gt; 1, 1, 6, 2, 7, 5, 1, 3, 1, 8, 8, 1, 3, 10, … ## $ p23st_a &lt;fct&gt; 2, 2, 1, 1, 2, 2, 1, 4, 1, 1, 2, 2, 4, NA, … ## $ p23st_b &lt;fct&gt; 3, 3, 2, 2, 3, 2, 2, 4, 3, 1, 2, 4, NA, 3, … ## $ p23st_c &lt;fct&gt; 2, 3, 2, 2, 3, 2, 4, 4, 1, 3, 1, 4, 4, NA, … ## $ p23st_d &lt;fct&gt; 2, 3, 2, 3, 2, 2, 2, 4, 2, 2, 2, 4, NA, NA,… ## $ p23st_e &lt;fct&gt; 3, 3, 4, 2, 2, 2, 2, 4, 2, 4, 2, 4, 4, 3, 2… ## $ p23st_f &lt;fct&gt; 3, 3, 4, 3, 3, 3, 4, 4, 3, 3, 3, 4, 4, NA, … ## $ p24st_a &lt;fct&gt; 2, 2, 2, 1, 1, 2, 2, 2, 1, 1, 2, 2, 2, 2, 2… ## $ p24st_b &lt;fct&gt; 3, 2, 2, 2, 2, 2, 2, 3, NA, 2, 2, 3, NA, 3,… ## $ p24st_c &lt;fct&gt; 1, 2, 2, 1, 2, 2, 2, 3, 2, 3, 1, 3, 4, 2, 2… ## $ p24st_d &lt;fct&gt; 1, 2, 2, 2, 1, 2, 2, 3, NA, 3, 2, 3, 2, NA,… ## $ p24st_e &lt;fct&gt; 2, 2, 3, 1, 3, 3, 4, 3, NA, 3, 3, 3, 3, NA,… ## $ p25n_a &lt;fct&gt; 2, 2, 1, 1, 2, 2, 2, 3, 1, 2, 2, 2, 2, 2, 1… ## $ p25n_b &lt;fct&gt; 2, 2, 1, 2, 2, 2, 2, 3, 3, 1, 2, 3, 3, NA, … ## $ p25n_c &lt;fct&gt; 1, 3, 1, 1, 2, 2, 2, 3, 1, 3, 1, 3, 4, NA, … ## $ p26sdn_a &lt;fct&gt; 1, 3, 2, 1, 2, 1, 4, 4, 3, 4, 2, NA, 4, 3, … ## $ p26sdn_b &lt;fct&gt; 3, 4, 1, 4, 3, 1, 1, 4, NA, 1, 4, 4, 1, 3, … ## $ p26sdn_c &lt;fct&gt; 2, NA, 2, 3, 3, 1, 3, 4, 2, 3, 2, 4, 3, NA,… ## $ p26sdn_d &lt;fct&gt; 1, 1, 3, 1, 1, 1, 3, 4, NA, 3, 1, 4, 4, NA,… ## $ p26sdn_e &lt;fct&gt; 3, 3, 2, 1, 1, 1, 3, 4, 1, 3, 4, 3, 4, 3, 3… ## $ p26sdn_f &lt;fct&gt; 3, 3, 1, 1, 2, 1, 3, 4, 3, 3, 1, NA, 4, 3, … ## $ p27sdn &lt;fct&gt; 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, N… ## $ p28stin &lt;fct&gt; 4, 1, 1, 2, 1, NA, 3, 3, NA, 1, 4, 3, 2, 2,… ## $ p29stin_a &lt;fct&gt; 1, 2, 1, 2, 2, 1, 2, 4, 1, 3, 1, NA, 2, 2, … ## $ p29stin_b &lt;fct&gt; 1, 2, 1, 2, 1, NA, 1, 4, 2, 4, 1, 1, 2, NA,… ## $ p30stin_a &lt;fct&gt; 4, 4, 2, 2, 4, 4, 1, 1, 2, 1, 3, 4, 1, 4, 4… ## $ p30inn_b &lt;fct&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1… ## $ p30inn_c &lt;fct&gt; 2, 1, 2, 3, 2, 1, 4, 3, 1, 1, 1, 4, 1, NA, … ## $ p30inn_d &lt;fct&gt; 4, 1, 1, 4, 2, 1, 2, 1, 1, 4, 1, 1, 1, 1, 1… ## $ p30inn_e &lt;fct&gt; 4, 1, 3, 4, 2, 4, 2, 4, 2, 4, 1, 3, 4, 1, 1… ## $ p30inn_f &lt;fct&gt; 2, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 2, 1, 2… ## $ p31inn_a &lt;fct&gt; 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1… ## $ p31inn_b &lt;fct&gt; 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0… ## $ p31inn_c &lt;fct&gt; 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0… ## $ p31inn_d &lt;fct&gt; 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0… ## $ p31inn_e &lt;fct&gt; 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0… ## $ p31inn_f &lt;fct&gt; 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1… ## $ p31inn_1 &lt;fct&gt; 3, 1, 1, 3, 2, 2, 2, 4, 1, 1, 6, NA, 1, 3, … ## $ p31inn_2 &lt;fct&gt; 6, 3, 3, NA, 3, NA, 3, NA, 2, 3, NA, NA, NA… ## $ p31inn_3 &lt;fct&gt; NA, 4, 6, NA, 4, NA, 4, NA, 3, NA, NA, NA, … ## $ p31inn_4 &lt;fct&gt; NA, 5, NA, NA, 5, NA, 5, NA, 4, NA, NA, NA,… ## $ p31inn_5 &lt;fct&gt; NA, 6, NA, NA, 6, NA, 6, NA, NA, NA, NA, NA… ## $ p31inn_6 &lt;fct&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,… ## $ p32inn &lt;fct&gt; 1, 1, 1, 3, 2, 2, 1, 2, 3, 2, 3, 2, 3, NA, … ## $ p33n_a &lt;fct&gt; 3, 2, 2, 3, 2, 4, 2, 2, 2, 3, 2, 1, 2, 1, 2… ## $ p33st_b &lt;fct&gt; 3, 3, 4, 4, 2, 1, 3, 4, 3, 1, 4, 4, 3, NA, … ## $ p33n_c &lt;fct&gt; 2, 2, 4, 3, 2, 4, 3, 4, 3, 4, 4, 1, 3, 4, 4… ## $ p33n_d &lt;fct&gt; 2, 1, 1, 2, 2, 4, 2, 4, 2, 3, 1, 4, 1, 1, 1… ## $ p34wvsna &lt;fct&gt; 2, 1, 2, 3, 2, 2, 3, 1, 1, 2, 1, 2, 4, 4, 2… ## $ p34wvsnb &lt;fct&gt; 3, 3, 1, 4, 1, 3, 1, 2, 3, 1, 2, 3, 2, 2, 4… ## $ p35na &lt;fct&gt; 2, 4, 4, 4, 4, 4, 4, 1, 1, 4, 2, 2, 4, NA, … ## $ p35nb &lt;fct&gt; 4, 1, 2, 3, 2, 3, 2, 4, 3, 1, 4, 4, 3, NA, … ## $ p36st &lt;fct&gt; 2, 1, 1, 2, 1, 1, 1, 2, 2, 2, 1, 2, 1, NA, … ## $ p37csn_a &lt;fct&gt; 1, 4, 1, 2, 1, 1, 4, 4, 3, 4, 1, 1, 1, 1, 1… ## $ p37csn_b &lt;fct&gt; 1, 4, 1, 2, 2, 1, 4, 4, 4, 4, 1, 4, 1, 1, 1… ## $ p37stcs_c &lt;fct&gt; 4, 4, 4, 1, 2, 4, 1, 1, 4, 4, 4, 4, 4, NA, … ## $ p37wvsstcs_d &lt;fct&gt; 2, 4, 4, 4, 2, 4, 2, 1, 4, 4, 4, 1, 4, NA, … ## $ p37stcs_e &lt;fct&gt; 4, 4, 2, 2, 2, 4, 3, 2, 4, 4, 3, 4, 4, 2, 2… ## $ p37csn_f &lt;fct&gt; 4, 4, 1, 1, 2, 1, 4, 1, 4, 4, 1, 4, 1, 1, 1… ## $ p37wvscs_g &lt;fct&gt; 4, 4, 4, 4, 2, 4, 2, 1, 4, 4, 4, 4, 4, 4, 4… ## $ p37wvscs_h &lt;fct&gt; 1, 4, 1, 3, 2, 1, 4, 4, 4, 4, 1, 1, 1, NA, … ## $ p38csn_a &lt;fct&gt; 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1… ## $ p38csn_b &lt;fct&gt; 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ p38csn_c &lt;fct&gt; 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0… ## $ p38csn_d &lt;fct&gt; 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1… ## $ p38csn_e &lt;fct&gt; 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1… ## $ p38csn_f &lt;fct&gt; 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1… ## $ p38csn_g &lt;fct&gt; 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1… ## $ p39n &lt;fct&gt; 2, 2, 3, 2, 1, 2, 2, 1, 2, 1, 3, 2, 3, 2, 3… ## $ p40stgbs &lt;fct&gt; 1, 4, 2, 1, 3, 3, 1, 4, 3, 2, 1, 3, 3, 3, 2… ## $ p41st_a &lt;fct&gt; 2, 3, 2, 1, 1, 3, 3, 1, 4, 1, 1, 3, 2, NA, … ## $ p41st_b &lt;fct&gt; 2, 3, 2, 1, 3, 3, 4, 1, 3, 1, 3, 3, 2, 1, 1… ## $ p41st_c &lt;fct&gt; 3, 4, 3, 3, 1, 3, 3, 1, 3, 1, 3, 4, 2, 3, 3… ## $ p41st_d &lt;fct&gt; 3, 3, 2, 2, 3, 3, 3, 1, 3, 2, 1, NA, 2, 4, … ## $ p41st_e &lt;fct&gt; 4, 4, 3, 3, 2, 3, 4, 1, 4, 3, 3, 4, 4, NA, … ## $ p41st_f &lt;fct&gt; 2, 3, 2, 1, 3, 3, 3, 1, 4, 1, 4, NA, 2, 3, … ## $ p41st_g &lt;fct&gt; 2, 3, 3, 3, 1, 3, 3, 1, NA, 1, 3, NA, 2, 1,… ## $ p41st_h &lt;fct&gt; 1, 3, 3, 1, 3, 3, 4, 1, 3, 1, 3, NA, 2, 3, … ## $ p41st_i &lt;fct&gt; 1, 1, 3, 1, 1, 3, 4, 1, 2, 1, 1, 4, 2, 3, 1… ## $ p41st_j &lt;fct&gt; 3, 4, 3, 4, 3, 3, 4, 1, 4, 1, 3, 4, 3, 3, 3… ## $ p41st_k &lt;fct&gt; 3, 4, 2, 2, 1, 3, 3, 1, 3, 1, 3, 3, 3, 1, 4… ## $ p41st_l &lt;fct&gt; 2, 3, 3, 3, 1, 3, 4, 1, 4, 1, 4, 4, 2, 1, 2… ## $ p41st_m &lt;fct&gt; 3, 3, 2, 2, 3, 3, 3, 1, 3, 1, 3, 3, 2, 3, 2… ## $ p42stgbs &lt;fct&gt; 1, 2, 2, 1, 2, 2, 1, 2, 2, 1, 2, NA, 2, 1, … ## $ p43stgbs_a &lt;fct&gt; 76001, 76001, 76021, 76038, 76014, 76001, 7… ## $ p43stgbs_b &lt;fct&gt; 1, 1, 2, NA, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, … ## $ p44st_a &lt;fct&gt; 1, 4, 2, 1, 2, 4, 2, 4, 4, 1, 1, 4, 2, 2, 2… ## $ p44st_b &lt;fct&gt; 2, 2, 2, 4, 1, 2, 4, 4, 4, 1, 4, NA, 4, 2, … ## $ p44st_c &lt;fct&gt; 1, 4, 3, 1, 2, 4, 2, 4, 4, 1, 4, 2, 4, 4, 1… ## $ p44st_d &lt;fct&gt; 2, 4, 3, 1, 4, 4, 4, 4, 4, 1, 4, NA, 4, 4, … ## $ p45st_a &lt;fct&gt; 2, 3, 2, 2, 1, 2, 2, 3, 2, 1, 1, 1, 2, 3, 1… ## $ p45s_b &lt;fct&gt; 2, 3, 2, 2, 1, 2, 2, 3, 3, 1, 3, 3, 2, 3, 2… ## $ p45st_c &lt;fct&gt; 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3… ## $ p45stn_d &lt;fct&gt; 1, 3, 2, 3, 1, 3, 2, 3, 3, 1, 3, 3, 2, 3, 2… ## $ p46n &lt;fct&gt; 2, 2, 2, 2, 1, 2, 1, 2, 2, 1, 2, NA, 2, NA,… ## $ p47st &lt;fct&gt; 2, 1, 2, 5, 1, 5, 2, 1, 1, 1, 2, 5, 2, 1, 1… ## $ p48st &lt;fct&gt; 2, 1, 1, 2, 2, 2, 2, 1, 2, 1, 1, 2, NA, 2, … ## $ p49st &lt;fct&gt; 1, 1, 1, 2, 2, 1, 1, 2, 1, 1, 1, 2, 1, 2, 2… ## $ p50st &lt;fct&gt; 1, 5, 3, 4, 3, 5, 5, 5, 5, 4, 3, 5, 1, 3, 3… ## $ p51n &lt;fct&gt; 1, 3, 1, 1, 1, 2, 1, 2, 3, 2, 3, 2, 2, 2, 1… ## $ p52n &lt;fct&gt; 2, 2, 1, 2, 2, 2, 2, 1, 2, 2, 1, 2, 1, 2, 1… ## $ p53n_a &lt;fct&gt; 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 2… ## $ p53n_b &lt;fct&gt; 2, 1, 2, 1, 1, 2, 1, 1, 1, 2, 2, 1, 3, NA, … ## $ p53n_c &lt;fct&gt; 2, 3, 3, 1, 1, 2, 3, 1, 3, 2, 2, 1, 1, 3, 3… ## $ p53n_d &lt;fct&gt; 1, 1, 3, 1, 1, 2, 1, 1, 1, 2, 2, 1, 1, 3, 3… ## $ p54n_a &lt;fct&gt; 2, 3, 2, 3, 1, 4, 2, 3, 3, 1, 2, 4, 2, 1, 2… ## $ p54st_b &lt;fct&gt; 2, 2, 4, 2, 1, 2, 2, 1, 2, 2, 2, 1, 2, 1, 3… ## $ p55n &lt;fct&gt; 3, 1, 4, 3, 2, 5, 2, 5, 3, 2, 4, 1, 4, 1, 5… ## $ p56n_a &lt;fct&gt; 3, 4, 2, 1, 1, 1, 1, 1, 2, 2, 2, 1, 2, 1, 2… ## $ p56n_b &lt;fct&gt; 4, 4, 2, 3, 1, 2, 1, 1, 2, 2, 2, 1, 2, 1, 3… ## $ p56n_c &lt;fct&gt; 3, 4, 1, 1, 1, 1, 3, 1, 1, 1, 2, 1, 2, 1, 1… ## $ p56n_d &lt;fct&gt; 2, 2, 1, 2, 1, 1, 2, 1, 1, 2, 2, 1, 2, NA, … ## $ p57st_a &lt;fct&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1… ## $ p57st_b &lt;fct&gt; 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0… ## $ p57st_c &lt;fct&gt; 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1… ## $ p57st_d &lt;fct&gt; 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0… ## $ p57st_e &lt;fct&gt; 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1… ## $ p57st_f &lt;fct&gt; 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1… ## $ p57st_g &lt;fct&gt; 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0… ## $ p57st_h &lt;fct&gt; 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1… ## $ p58st &lt;fct&gt; 4, 4, 4, 4, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2… ## $ p59st &lt;fct&gt; 1, 1, 2, 1, 1, 1, 1, 4, 1, 1, 3, 1, 3, 1, 3… ## $ p60st &lt;fct&gt; 4, 3, 3, 3, 3, 4, 1, 3, 3, 1, 2, 4, 4, 3, 4… ## $ p61st &lt;fct&gt; 10, 8, 4, 10, 8, 1, 1, 8, 1, 8, 1, 1, 3, 8,… ## $ p62n_1 &lt;fct&gt; 5, 7, 96, 0, 8, 96, 96, 0, 96, 7, 5, 96, 96… ## $ p62st_2 &lt;fct&gt; 4, 96, 96, 0, 8, 96, 96, 10, 96, 3, 96, 96,… ## $ p62st_3 &lt;fct&gt; 96, 96, 96, 0, 8, 96, 96, 0, 96, 4, 5, 96, … ## $ p62st_4 &lt;fct&gt; 2, 0, 5, 6, 7, 0, 8, 8, 1, 8, 0, 0, 0, 0, 2… ## $ p62st_5 &lt;fct&gt; 0, 0, 96, 0, 6, 96, 0, 0, 1, 8, 0, 96, 0, N… ## $ p62n_6 &lt;fct&gt; 96, 96, 96, 0, 7, 96, 96, 0, 96, 8, 96, 96,… ## $ p62st_7 &lt;fct&gt; 9, 10, 96, 10, 10, 8, 0, 0, 8, 4, 9, 10, 9,… ## $ p62st_8 &lt;fct&gt; 7, 96, 96, 0, 8, NA, 0, 0, 96, 4, 9, 96, 96… ## $ p62n_9 &lt;fct&gt; 0, 96, 96, 1, 8, 96, 96, 0, 96, 8, 5, 96, 9… ## $ p62st_10 &lt;fct&gt; 5, 5, 8, 0, 8, 96, 1, 0, 5, 7, 0, 0, 0, 96,… ## $ p62st_11 &lt;fct&gt; 96, 96, 96, 0, 8, 96, 96, 0, 96, 5, NA, 96,… ## $ p62n_12 &lt;fct&gt; 3, 96, 96, 0, 8, 96, 0, 0, 4, 8, 0, 96, 96,… ## $ p62n_13 &lt;fct&gt; 96, 96, 96, 0, 8, 96, 96, 0, 96, 4, 5, 96, … ## $ p62n_14 &lt;fct&gt; 5, 4, 3, 2, 8, 96, 1, 0, NA, 8, 7, 96, 96, … ## $ p62n_15 &lt;fct&gt; 3, 2, 96, 0, 8, 96, 5, 0, 96, 8, 2, 96, 96,… ## $ p62n_16 &lt;fct&gt; 9, 10, 6, 10, 7, 5, 0, 10, 0, 8, 7, 10, 10,… ## $ p62n_17 &lt;fct&gt; 8, 9, 3, 5, 8, 96, 1, 0, 96, 8, 1, 8, 96, 1… ## $ p63st &lt;fct&gt; 4, 4, 2, 4, 3, 3, 3, 1, 3, 3, 3, NA, NA, NA… ## $ s1 &lt;fct&gt; 2, 2, 1, 1, 1, 1, 2, 1, 2, 5, 96, 1, 1, 1, … ## $ s1a &lt;fct&gt; 3, 3, 3, 3, 2, 3, 2, 1, 2, 2, 2, 2, 2, 1, 3… ## $ s2 &lt;fct&gt; 4, 3, 4, 4, 3, 4, 5, 3, 4, 3, 3, 5, 4, 3, 3… ## $ s3 &lt;fct&gt; 1, 1, 1, 1, 4, 1, 2, 1, 1, 3, 1, 3, 1, 1, 1… ## $ s4 &lt;fct&gt; 5, 5, 5, 1, 5, 3, 5, 5, 5, 5, 4, 5, 5, 4, 4… ## $ s5 &lt;fct&gt; 3, 2, 1, 1, 4, 2, 4, 2, 3, 1, 1, 4, 1, 2, 2… ## $ s6 &lt;fct&gt; 1, 1, 4, 2, 2, 2, 3, 1, 4, 1, 4, 1, 1, 1, 4… ## $ s7 &lt;fct&gt; 4, 6, 6, 6, 4, 6, 6, 6, 6, 6, 4, 6, 6, 5, 6… ## $ s8 &lt;fct&gt; 1, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 1… ## $ s9 &lt;fct&gt; 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2… ## $ s10 &lt;dbl&gt; 18, 12, 18, 22, 19, 20, 22, 12, 18, 32, 17,… ## $ s11 &lt;fct&gt; 11, 7, 13, 13, 12, 13, 8, 5, 13, 13, 12, 7,… ## $ s12 &lt;fct&gt; 5, 2, 15, 9, 9, 5, 5, 1, 13, 4, 9, 1, 5, 1,… ## $ s13inn_a_a &lt;fct&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ s13inn_a_b &lt;fct&gt; 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0… ## $ s13inn_a_c &lt;fct&gt; 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1… ## $ s13inn_a_d &lt;fct&gt; 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0… ## $ s13inn_a_1 &lt;fct&gt; 4, 4, 2, 2, 4, 4, 4, 4, 2, 2, 3, 4, 4, 4, 3… ## $ s13inn_a_2 &lt;dbl&gt; 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ s13inn_a_3 &lt;fct&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,… ## $ s14m_a &lt;fct&gt; 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1… ## $ s14m_b &lt;fct&gt; 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ s14m_c &lt;fct&gt; 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1… ## $ s14m_d &lt;fct&gt; 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ s14m_e &lt;fct&gt; 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1… ## $ s14m_f &lt;fct&gt; 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1… ## $ s14m_g &lt;fct&gt; 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ s14m_h &lt;fct&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1… ## $ s14m_i &lt;fct&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ s14m_j &lt;fct&gt; 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0… ## $ s15_a &lt;fct&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2… ## $ s15_b &lt;fct&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2… ## $ s16 &lt;fct&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1… ## $ s17 &lt;date&gt; 1965-11-30, 1985-10-01, 2004-07-23, 1975-0… ## $ s18_a &lt;fct&gt; 4, 4, 4, 1, 4, 3, 4, 5, 4, 6, 1, 5, 6, 1, 3… ## $ s18_b &lt;fct&gt; 8, 3, 8, NA, 4, NA, 8, 8, 8, NA, NA, 8, NA,… ## $ s19 &lt;fct&gt; NA, NA, NA, 2, NA, 8, NA, NA, NA, NA, 2, NA… ## $ s20_a &lt;fct&gt; 2, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1… ## $ s20_b &lt;fct&gt; 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 2… ## $ s20_c &lt;fct&gt; 2, 2, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1… ## $ s20_d &lt;fct&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1… ## $ s20_e &lt;fct&gt; 2, 1, 1, 1, 2, 1, 2, 1, 1, 2, 1, 2, 2, 2, 1… ## $ s20_f &lt;fct&gt; 2, 2, 1, 1, 2, 1, 2, 1, 1, 2, 1, 1, 2, 2, 1… ## $ s20_g &lt;fct&gt; 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 2, 1… ## $ s20_h &lt;fct&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1… ## $ s20_i &lt;fct&gt; 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1… ## $ s20_j &lt;fct&gt; 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1… ## $ s20_k &lt;fct&gt; 2, 2, 1, 1, 2, 1, 2, 1, 2, 2, 1, 2, 2, 2, 1… ## $ s21a &lt;dbl&gt; 21, NA, 26, NA, NA, NA, NA, NA, NA, NA, NA,… ## $ s21b &lt;fct&gt; 15, 5, 15, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ s22_a &lt;fct&gt; 1, 1, 3, NA, NA, NA, NA, NA, NA, NA, NA, 5,… ## $ s22_b &lt;fct&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,… ## $ s23 &lt;fct&gt; 1, 4, 1, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ s24 &lt;fct&gt; 3, 2, 3, 2, 3, 3, 3, 2, 2, 3, 2, 2, 4, 3, 1… ## $ reeeduc_1 &lt;fct&gt; 4, 2, 6, 6, 5, 6, 2, 2, 6, 6, 5, 2, 6, 1, 7… ## $ reeduc_2 &lt;fct&gt; 2, 2, 7, 3, 3, 2, 2, 1, 6, 2, 3, 1, 2, 1, 7… ## $ reeduc_3 &lt;fct&gt; 7, 2, 7, NA, NA, NA, NA, NA, NA, NA, NA, 2,… ## $ reedad &lt;fct&gt; 3, 2, 1, 3, 3, 2, 3, 4, 2, 3, 2, 4, 1, 3, 2… ## $ perpart &lt;fct&gt; 1, 1, 2, 3, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1… ## $ fampart &lt;fct&gt; 30, 30, 40, 20, 20, 30, 40, 30, 30, 30, 30,… ## $ wt &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1… ## $ data_base &lt;date&gt; 2023-03-03, 2023-03-03, 2023-03-03, 2023-0… ## $ idade &lt;int&gt; 4949181, 3235309, 1609495, 4144420, 4096621… ## $ econ_12_meses &lt;fct&gt; other, other, better, better, other, other,… ## $ aprovacao_presidente &lt;int&gt; 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1… ## $ ideologia &lt;int&gt; 6, 7, 7, 6, 7, 6, 11, 11, 6, 9, 6, 8, 7, 11… ## $ votaria_governo &lt;dbl&gt; 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1… ## $ genero &lt;fct&gt; mulher, mulher, mulher, homem, homem, mulhe… ## $ evangelico &lt;dbl&gt; 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0… reg_full &lt;- lm(votaria_governo ~ ideologia + idade + genero + econ_12_meses + evangelico, data=br_latbar_23 ) summary(reg_full) ## ## Call: ## lm(formula = votaria_governo ~ ideologia + idade + genero + econ_12_meses + ## evangelico, data = br_latbar_23) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.0002 -0.3261 0.1037 0.3224 0.6924 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 8.859e-01 7.719e-02 11.477 &lt; 2e-16 *** ## ideologia -3.860e-02 5.622e-03 -6.866 2.40e-11 *** ## idade -4.189e-09 1.594e-08 -0.263 0.79280 ## generomulher 1.195e-01 3.860e-02 3.097 0.00209 ** ## econ_12_mesesbetter 1.766e-01 3.970e-02 4.448 1.11e-05 *** ## evangelico -1.445e-01 4.510e-02 -3.204 0.00146 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.3949 on 417 degrees of freedom ## (49 observations deleted due to missingness) ## Multiple R-squared: 0.2274, Adjusted R-squared: 0.2181 ## F-statistic: 24.55 on 5 and 417 DF, p-value: &lt; 2.2e-16 residuos_full &lt;- resid(reg_full) reg_res_evan &lt;- lm(votaria_governo ~ ideologia + idade + genero + econ_12_meses, data=br_latbar_23 ) summary(reg_res_evan) ## ## Call: ## lm(formula = votaria_governo ~ ideologia + idade + genero + econ_12_meses, ## data = br_latbar_23) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.99732 -0.39237 0.09575 0.30292 0.60771 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 8.477e-01 7.710e-02 10.994 &lt; 2e-16 *** ## ideologia -4.141e-02 5.615e-03 -7.375 8.90e-13 *** ## idade 6.369e-11 1.606e-08 0.004 0.99684 ## generomulher 1.141e-01 3.899e-02 2.925 0.00363 ** ## econ_12_mesesbetter 1.907e-01 3.989e-02 4.781 2.42e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.3993 on 418 degrees of freedom ## (49 observations deleted due to missingness) ## Multiple R-squared: 0.2084, Adjusted R-squared: 0.2008 ## F-statistic: 27.51 on 4 and 418 DF, p-value: &lt; 2.2e-16 residuos_res_evan &lt;- resid(reg_res_evan) partial_R2_evangelico &lt;- (sum(residuos_res_evan^2) - sum(residuos_full^2))/sum(residuos_res_evan^2) print(partial_R2_evangelico) ## [1] 0.02402629 # alternative partial_R2_evangelico_alt &lt;- 1 - var(resid(reg_full))/var(resid(reg_res_evan)) print(partial_R2_evangelico_alt) ## [1] 0.02402629 # another alternative reg_evan &lt;- lm(evangelico ~ ideologia + idade + genero + econ_12_meses , data=br_latbar_23) partial_R2_evangelico_alt1 &lt;- cor(resid(reg_evan), resid(reg_res_evan))^2 print(partial_R2_evangelico_alt1) ## [1] 0.02402629 ## Cohen partial f2 r_squared_full &lt;- summary(reg_full)$r.squared r_squared_res &lt;- summary(reg_res_evan)$r.squared partial_f2 &lt;- (r_squared_full - r_squared_res)/(1-r_squared_full) print(partial_f2) ## [1] 0.02461776 E agora podemos reescrever o viés de variável omitida em termos do R-quadrado parcial. 12.10.2 Viés de variável omitida Nós vimos que o viés de variável omitida, em uma amostra, pode ser explicitado por: \\[ \\hat{\\beta_1^*} = \\hat{\\beta_1} + \\frac{\\hat{\\beta_2} \\cdot \\mathbb{Cov}[X_1, X_2]}{\\mathbb{Var}[X_1]} \\] Vamos reescrever essa equação do seguinte modo. Seja \\(\\frac{\\mathbb{Cov}[X_1, X_2]}{\\mathbb{Var}[X_1]} = \\hat{\\gamma}\\), então noss equação fica: \\[ \\hat{\\beta_1^*} = \\hat{\\beta_1} + \\hat{\\beta_2} \\cdot \\hat{\\gamma} \\] Em que \\(\\hat{\\beta_2}\\) mede o “impacto” da variável omitida sobre Y. Ou seja, ela mede a diferença na esperança linear da resposta (diferença média) entre indivíduos que diferem em uma unidade na variável omitida e têm o mesmo status no tratamento, bem como o mesmo valor para todas as variáveis de controle restante. Digamos que estou interessado em estimar o efeito causal da experiência (variávle binária, experiente ou não experiente) sobre votação. Candidatas mais experientes devem ter mais votos, mas também maior visibilidade e reconhecimento, o que viesa a regressão (efeito da experiência). Suponha que controlei para arrecadação de campanha. Então, \\(\\hat{\\gamma}\\) mede a diferença esperada (média) na votação entre pessoas experinetes (tratamento = 1) com uma mesma dada arrecadação de campanha, mas que difiram em uma unidade em visibilidade. Já a Quantidade \\(\\hat{\\gamma}\\) parece que mede o impacto de Z sobre o tratamento, mas na verdade ela é resultado da regressão reversa, isto é, \\(Z = \\hat{\\alpha_1} + \\hat{\\gamma} T + \\hat{\\psi} X + \\hat{u}\\). Ou seja, mede a diferença média na variável de confusão entre indivíduos que diferem de uma unidade no tratamento. Em nosso exemplo, como candidatas experientes diferem em visibilidade, em média, de não experientes. Ainda em outras palavras, mede o “balanceamento” (condicional aos controles) entre indivíduos alocados pro tratamento e controle no que diz respeito à variável omitida. Essa formulação é útil para pensar sobre como o viés de variável omitida impacta nossas estimativas. Quanto maior o “imbalance”, maior será \\(\\hat{\\gamma}\\). Similarmente, quanto maior o impacto da variável omitida na resposta, maior \\(\\hat{\\beta_2}\\). Por isso que é comum checar o imbalance nas variáveis de controle. Se ele é alto, pode significar alto viés das variáveis omitidas, se o imbalance delas for correlacionado com o imbalance das variáveis de controle. Numa regressão mais completa, da forma, \\(y = \\alpha + \\delta T + \\beta X + \\beta_2 Z + e\\), em que \\(T\\) é o tratamento, \\(X\\) controles e \\(Z\\) a variável omitida, o viés pode ser reescrito como: \\[ \\hat{&lt;viés&gt;} = \\hat{\\beta_2} \\cdot \\hat{\\gamma} \\] Em que $ hat{} = $ 12.10.3 Viés de variável omitida em função do R-quadrado \\[ \\hat{&lt;viés&gt;} = \\sqrt{\\frac{R^2_{y\\sim Z|T,X} R^2_{T \\sim Z|X}}{1 - R^2_{T \\sim Z|X}}} \\cdot \\frac{sd(Y^{\\perp X,T})}{sd(T^{\\perp X})} \\] Vamos olhar cada componente dessa parametrização do viés: \\(R^2_{y\\sim Z|T,X}\\) é o R-quadrado parcial de \\(Z\\) com relação à variável resposta \\(y\\) controlando para \\(T\\), o tratamento, e \\(X\\) os controles. Ou seja, quanto \\(Z\\) contribui para explicar a variação em \\(Y\\), após descontar a explicação de \\(X\\) e \\(T\\). \\(R^2_{T \\sim Z|X}\\) é o R-quadrado parcial de \\(Z\\) com relação à variável dependente \\(T\\), o tratamento. Ou seja, quanto a variável omitida explica do tratamento, após descontar a explicação na variância dos controles. \\(1 - R^2_{T \\sim Z|X}\\) é o complemento do R-quadrado parcial de Z sobre T, após controlar para \\(X\\). Representa a variância não-explicada em \\(T\\) por \\(Z\\) após descontar a influência de \\(X\\). E os demais termos são os desvios-padrões dos resíduos da regressão restrita, sem a variável não-observada \\(Z\\) e da regressão do tratamento em função dos controles. Em nosso exemplo, nós observamos a variável regilião evangélica, mas vamos supor que não é calcular o viés de variável omitida pela fórmula tradicional. ### # tratamento é ideologia. # omitida é religião beta2 &lt;- coef(reg_full)[6] cov_x1x2 &lt;- cov(br_latbar_23$ideologia, br_latbar_23$evangelico) var_x1 &lt;- var(br_latbar_23$ideologia) vies &lt;- beta2*cov_x1x2/var_x1 print(vies) ## evangelico ## -0.002611602 coef_res_sem_vies = coef(reg_res_evan)[2] - vies all.equal(coef_res_sem_vies, coef(reg_full)[2]) ## [1] &quot;Mean relative difference: 0.005018672&quot; coef_res_sem_vies - coef(reg_full)[2] ## ideologia ## -0.0001947166 O que essa equação nos mostra é que o viés depende de quanto a variável omitida explica o tratamento e quanto o tratamento é explicado pela variável omitida. O que é bastante intuitivo. E para entender melhor essa equação, vale falar o viés de relativo, definido como a razão entre o viés estimado e o coeifciente do tratamento estimado. \\[ Viés relatrivo = \\frac{\\hat{&lt;viés&gt;}}{\\hat{\\beta_1}} = \\frac{f(R^2_{y\\sim Z|T,X}, R^2_{D \\sim Z|X)})}{t_-do_-tratamento \\cdot \\sqrt(df)} \\] a parcela da variável resposta que o tratamento explica (medida pelo R-quadrado parcial do tratamento) dá uma medida da robustez da regressão à potenciais vieses de variável omitida. Uma intuição simples é que se o tratamento explicar 100% da variação, então não há, por definição, viés de variável omitida. Se explicar 95%, há muito pouco que um viés de variável omitida pode viesar as estimativas. library(sensemakr) # loads dataset data(&quot;darfur&quot;) # runs regression model model &lt;- lm(peacefactor ~ directlyharmed + age + farmer_dar + herder_dar + pastvoted + hhsize_darfur + female + village, data = darfur) summary(model) ## ## Call: ## lm(formula = peacefactor ~ directlyharmed + age + farmer_dar + ## herder_dar + pastvoted + hhsize_darfur + female + village, ## data = darfur) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.67487 -0.14712 0.00000 0.09857 0.90307 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.0818918 0.3149160 3.435 0.000623 *** ## directlyharmed 0.0973158 0.0232565 4.184 3.18e-05 *** ## age -0.0020717 0.0008232 -2.517 0.012044 * ## farmer_dar -0.0404429 0.0292449 -1.383 0.167087 ## herder_dar 0.0142791 0.0321319 0.444 0.656884 ## pastvoted -0.0480250 0.0240462 -1.997 0.046150 * ## hhsize_darfur 0.0012298 0.0019277 0.638 0.523685 ## female -0.2320514 0.0237057 -9.789 &lt; 2e-16 *** ## villageAbdi Dar -0.0192558 0.4411017 -0.044 0.965191 ## villageAbu Dejaj -0.7197809 0.3609182 -1.994 0.046465 * ## villageAbu Gamra -0.5271364 0.3151842 -1.672 0.094831 . ## villageAbu Gawar -1.0657332 0.4418454 -2.412 0.016094 * ## villageAbu Geran -0.8733307 0.3819967 -2.286 0.022507 * ## villageAbu Jidad -0.4878930 0.3826800 -1.275 0.202710 ## villageAbu Lihya -0.5707994 0.3477972 -1.641 0.101161 ## villageAbu Mugu -0.9671213 0.4407133 -2.194 0.028496 * ## villageAbu Shouka -0.7249739 0.4415203 -1.642 0.100993 ## villageAbu Sorog -0.5959154 0.3598746 -1.656 0.098143 . ## villageAbune -0.6855577 0.4402757 -1.557 0.119849 ## villageAdar -0.5229708 0.3172264 -1.649 0.099636 . ## villageAgadier -0.4777710 0.4409077 -1.084 0.278872 ## villageAgudna -0.8405289 0.4418986 -1.902 0.057527 . ## villageAgumi -0.7306179 0.4408603 -1.657 0.097869 . ## villageAid Alkhair -0.3317194 0.3594243 -0.923 0.356334 ## villageAirgy -1.0462455 0.4418388 -2.368 0.018130 * ## villageAish Barra -0.4528391 0.3422855 -1.323 0.186226 ## villageAjayana -0.9536066 0.4411943 -2.161 0.030965 * ## villageAjiji -0.1441859 0.4411427 -0.327 0.743871 ## villageAkomi -0.6962401 0.4410606 -1.579 0.114841 ## villageAl Gedisa -0.1618676 0.4402377 -0.368 0.713210 ## villageAl Geneina -0.5089806 0.3595054 -1.416 0.157238 ## villageAllah Forga -0.6846515 0.4413098 -1.551 0.121208 ## villageAlmetrabia -0.8571347 0.4411693 -1.943 0.052390 . ## villageAm Baro -0.4399481 0.3493009 -1.260 0.208221 ## villageAm Dalal -0.3310802 0.3605518 -0.918 0.358766 ## villageAm Dariro -0.4589021 0.4416337 -1.039 0.299078 ## villageAm Dukhn -0.6986997 0.4408066 -1.585 0.113359 ## villageAm Jarad -0.0324659 0.4402236 -0.074 0.941229 ## villageAm Jemma -0.4549818 0.4410044 -1.032 0.302534 ## villageAm Kharouba -0.8496693 0.4415697 -1.924 0.054691 . ## villageAmar -0.8092879 0.4416047 -1.833 0.067241 . ## villageAmdukhn -0.6676235 0.4412174 -1.513 0.130648 ## villageAndabook -0.1478622 0.4418610 -0.335 0.737990 ## villageAnderola -0.6171842 0.4412653 -1.399 0.162308 ## villageAndita -0.3409366 0.3828738 -0.890 0.373489 ## villageAnjilati -0.1729516 0.4415549 -0.392 0.695396 ## villageAnjokati -0.4587088 0.4416816 -1.039 0.299334 ## villageAorda -0.5289973 0.4406382 -1.201 0.230298 ## villageAradeeba -0.7560845 0.4427057 -1.708 0.088057 . ## villageArara -0.5076057 0.3367427 -1.507 0.132112 ## villageArgote -0.5396439 0.4423977 -1.220 0.222902 ## villageArgoy 0.0149665 0.4432207 0.034 0.973071 ## villageArimba -0.6829014 0.4406053 -1.550 0.121566 ## villageArmi -0.6461755 0.4424048 -1.461 0.144527 ## villageArola -0.8283216 0.4413782 -1.877 0.060935 . ## villageAroom -0.8244814 0.3268598 -2.522 0.011852 * ## villageArsa 0.0352721 0.4415647 0.080 0.936353 ## villageArsaliya -0.4433270 0.4407646 -1.006 0.314816 ## villageAsirni -0.5386620 0.3496601 -1.541 0.123835 ## villageAta -0.3173291 0.4414191 -0.719 0.472427 ## villageAurab -0.5240046 0.4433175 -1.182 0.237562 ## villageAwrish -0.9556967 0.4400959 -2.172 0.030188 * ## villageAwSanga -0.2316845 0.4406254 -0.526 0.599170 ## villageBadaria -0.7452505 0.4399552 -1.694 0.090677 . ## villageBajourba -0.8023955 0.4418360 -1.816 0.069745 . ## villageBakjo -0.7543060 0.4417158 -1.708 0.088094 . ## villageBaldi -0.6176931 0.4422282 -1.397 0.162877 ## villageBandis -0.6918758 0.3273657 -2.113 0.034877 * ## villageBango -0.4402689 0.4413116 -0.998 0.318764 ## villageBaranga -0.7021792 0.3826181 -1.835 0.066856 . ## villageBardia -0.5500589 0.3612288 -1.523 0.128226 ## villageBare -0.7405756 0.4423093 -1.674 0.094463 . ## villageBarkhalla -0.6033473 0.4420309 -1.365 0.172663 ## villageBasaw 0.0344349 0.4416336 0.078 0.937870 ## villageBeero -0.7571496 0.4422768 -1.712 0.087304 . ## villageBeida -0.3995823 0.3228242 -1.238 0.216172 ## villageBeir Saliba -0.4969939 0.3239577 -1.534 0.125401 ## villageBerdia -0.4840524 0.4415632 -1.096 0.273318 ## villageBertabid -0.6213939 0.4418463 -1.406 0.160015 ## villageBida Alginina 0.0506031 0.4404663 0.115 0.908565 ## villageBidigy -0.7467247 0.4416074 -1.691 0.091249 . ## villageBidt Faraga -0.4182936 0.4412517 -0.948 0.343437 ## villageBier Saliba -0.1137829 0.4401430 -0.259 0.796079 ## villageBiera -0.9352163 0.4398015 -2.126 0.033778 * ## villageBigaga -0.8191926 0.4412653 -1.856 0.063763 . ## villageBinary Mbila -0.7419358 0.4410630 -1.682 0.092938 . ## villageBindis -0.5660965 0.3226974 -1.754 0.079776 . ## villageBinediyah -0.4636796 0.4420565 -1.049 0.294541 ## villageBiot Mofarag -0.2313714 0.4416336 -0.524 0.600497 ## villageBir Tabat -0.3855859 0.3811145 -1.012 0.311979 ## villageBirdeya -0.5674605 0.3816577 -1.487 0.137462 ## villageBireiga -0.8047992 0.4405760 -1.827 0.068126 . ## villageBirgy -0.2774692 0.4422840 -0.627 0.530609 ## villageBirhaliba -0.5028462 0.4415694 -1.139 0.255147 ## villageBirtabit -0.4326597 0.4403429 -0.983 0.326132 ## villageBla Al Biyya -0.5203886 0.4415373 -1.179 0.238922 ## villageBoba -0.3232529 0.3613616 -0.895 0.371307 ## villageBoranga -0.8938475 0.4405666 -2.029 0.042811 * ## villageBorat -0.8254741 0.4418505 -1.868 0.062105 . ## villageBorgio -0.2946768 0.4428656 -0.665 0.505999 ## villageBoymawang -0.4645084 0.4443430 -1.045 0.296169 ## villageBrega -0.6417835 0.3602645 -1.781 0.075231 . ## villageBukar -0.2129811 0.4405112 -0.483 0.628886 ## villageCoda Mee -0.7370959 0.4411427 -1.671 0.095145 . ## villageCongi -0.7041392 0.4412732 -1.596 0.110960 ## villageDabaky -0.8039146 0.4417510 -1.820 0.069166 . ## villageDabis -0.7910301 0.4415104 -1.792 0.073576 . ## villageDagjor -0.8288332 0.3827254 -2.166 0.030642 * ## villageDalta -0.6429420 0.4415115 -1.456 0.145730 ## villageDamra -0.6914967 0.3418709 -2.023 0.043445 * ## villageDangojaro 0.2739928 0.4417491 0.620 0.535277 ## villageDanko -1.0656670 0.4413197 -2.415 0.015975 * ## villageDardure -0.8440405 0.4424176 -1.908 0.056784 . ## villageDargily -0.2051507 0.4417229 -0.464 0.642467 ## villageDasa -0.7082827 0.4412540 -1.605 0.108862 ## villageDeegada -0.9721730 0.4414091 -2.202 0.027926 * ## villageDeero -0.9531525 0.4407434 -2.163 0.030874 * ## villageDengar -1.0573931 0.4404987 -2.400 0.016608 * ## villageDerenda -0.9407220 0.4406995 -2.135 0.033103 * ## villageDerisa -0.7284137 0.4400596 -1.655 0.098272 . ## villageDinbo Kabdi -0.0762685 0.4416960 -0.173 0.862954 ## villageDingajour -0.8460460 0.4419309 -1.914 0.055929 . ## villageDinjir -0.4258255 0.4402739 -0.967 0.333751 ## villageDirdkna -0.9290673 0.4402316 -2.110 0.035140 * ## villageDirgily -0.1909702 0.4414838 -0.433 0.665450 ## villageDiria -0.0321443 0.4405844 -0.073 0.941858 ## villageDirisa -0.8467204 0.3827434 -2.212 0.027238 * ## villageDiro -0.6413792 0.3827291 -1.676 0.094176 . ## villageDofata -0.4621368 0.4414224 -1.047 0.295457 ## villageDongita -0.2273873 0.4415117 -0.515 0.606684 ## villageDoty -0.7117490 0.4414669 -1.612 0.107313 ## villageDoudata -0.4053538 0.4413884 -0.918 0.358713 ## villageDriol -0.1273862 0.4405906 -0.289 0.772561 ## villageDunjurou -0.7430352 0.4420034 -1.681 0.093150 . ## villageEid Elkhair 0.1471996 0.4413397 0.334 0.738824 ## villageEish Bida -0.7091246 0.4413971 -1.607 0.108557 ## villageEl Fasher -0.1290909 0.3826521 -0.337 0.735937 ## villageFanginta -0.4996049 0.4408992 -1.133 0.257498 ## villageFarne -0.7184251 0.4406761 -1.630 0.103445 ## villageFew 0.0489193 0.4407322 0.111 0.911648 ## villageFokdo -0.6846515 0.4413098 -1.551 0.121208 ## villageFondlayat -0.6371146 0.4413502 -1.444 0.149263 ## villageFonu -0.3863996 0.3823911 -1.010 0.312576 ## villageFur Baranga -0.4722050 0.4427732 -1.066 0.286539 ## villageFur Barenga -0.5386874 0.3816931 -1.411 0.158550 ## villageFurawiya -0.4018213 0.3160416 -1.271 0.203957 ## villageFuraWiya -0.6767524 0.4411722 -1.534 0.125437 ## villageFuro -0.7078286 0.4408655 -1.606 0.108778 ## villageGadir -0.7980483 0.3827604 -2.085 0.037394 * ## villageGalala -0.8499320 0.4411201 -1.927 0.054371 . ## villageGarady 0.0872493 0.4409768 0.198 0.843210 ## villageGarsila -0.4646958 0.3602177 -1.290 0.197417 ## villageGasheeb -1.1054843 0.4417947 -2.502 0.012543 * ## villageGasimba -0.3270378 0.3824628 -0.855 0.392766 ## villageGasmina -0.8672176 0.4422053 -1.961 0.050219 . ## villageGebeesh -0.6337299 0.3604515 -1.758 0.079112 . ## villageGedernii -0.0239234 0.4411098 -0.054 0.956762 ## villageGemana -0.7725639 0.4418454 -1.748 0.080770 . ## villageGemena -0.7589281 0.3830921 -1.981 0.047934 * ## villageGhibish -0.1200345 0.4401247 -0.273 0.785134 ## villageGhoz Doggy -0.3848804 0.4415179 -0.872 0.383628 ## villageGinfara -0.7091246 0.4413971 -1.607 0.108557 ## villageGirgida -0.4931539 0.3249533 -1.518 0.129515 ## villageGobi -0.7791942 0.3265448 -2.386 0.017261 * ## villageGodambi -0.7320502 0.4416464 -1.658 0.097809 . ## villageGogar 0.0249566 0.4418816 0.056 0.954975 ## villageGoji -0.7924017 0.4418578 -1.793 0.073304 . ## villageGondilat -0.7105749 0.3420180 -2.078 0.038072 * ## villageGono -0.5032090 0.3826007 -1.315 0.188817 ## villageGoraba -0.7503344 0.3830636 -1.959 0.050494 . ## villageGorboki -0.5462044 0.3410967 -1.601 0.109710 ## villageGoroku -0.7194172 0.4407247 -1.632 0.103008 ## villageGosimba -0.1532168 0.4414278 -0.347 0.728614 ## villageGoslayat -0.2325307 0.3820374 -0.609 0.542927 ## villageGouba 0.0523001 0.4412564 0.119 0.905682 ## villageGoz Bagar -0.5075722 0.3596931 -1.411 0.158605 ## villageGoz Beida -0.4741205 0.3830058 -1.238 0.216126 ## villageGoz Deega -0.5603087 0.3386511 -1.655 0.098420 . ## villageGoz Gabor -0.0629400 0.4410408 -0.143 0.886558 ## villageGoz Mono -0.4840189 0.3814558 -1.269 0.204863 ## villageGttara -0.8632253 0.4415476 -1.955 0.050939 . ## villageGubbey -0.5120457 0.3814134 -1.342 0.179825 ## villageHabila -0.6333912 0.3312515 -1.912 0.056226 . ## villageHabshaba -0.0210759 0.4416653 -0.048 0.961952 ## villageHadjer Suleiman -0.6158241 0.3827020 -1.609 0.107987 ## villageHai Jadid -0.7498481 0.4402517 -1.703 0.088923 . ## villageHajar farkhabta 0.0133910 0.4408325 0.030 0.975774 ## villageHajar Kharkhita -0.9514686 0.4404759 -2.160 0.031068 * ## villageHajar Seleman -0.7623162 0.4413206 -1.727 0.084499 . ## villageHajilija -1.0471667 0.4414993 -2.372 0.017941 * ## villageHaleila -0.3347389 0.3830161 -0.874 0.382411 ## villageHara -0.2983782 0.4418256 -0.675 0.499665 ## villageHarkow -0.7728384 0.4420899 -1.748 0.080830 . ## villageHashaba -0.2023767 0.3827321 -0.529 0.597116 ## villageHaskanita -0.7814071 0.3819002 -2.046 0.041080 * ## villageHee Al Gameena -0.7957580 0.4409485 -1.805 0.071513 . ## villageHeemada -1.0593079 0.4413909 -2.400 0.016631 * ## villageHijileja -0.5584427 0.4421291 -1.263 0.206938 ## villageHila Haraz -0.7607728 0.4412846 -1.724 0.085103 . ## villageHila Jadad 0.0489986 0.4411090 0.111 0.911581 ## [ reached getOption(&quot;max.print&quot;) -- omitted 293 rows ] ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.3102 on 783 degrees of freedom ## Multiple R-squared: 0.5115, Adjusted R-squared: 0.2046 ## F-statistic: 1.667 on 492 and 783 DF, p-value: 9.367e-11 # runs sensemakr for sensitivity analysis sensitivity &lt;- sensemakr(model = model, treatment = &quot;directlyharmed&quot;, benchmark_covariates = &quot;female&quot;, kd = 1:3) # short description of results sensitivity ## Sensitivity Analysis to Unobserved Confounding ## ## Model Formula: peacefactor ~ directlyharmed + age + farmer_dar + herder_dar + ## pastvoted + hhsize_darfur + female + village ## ## Null hypothesis: q = 1 and reduce = TRUE ## ## Unadjusted Estimates of &#39; directlyharmed &#39;: ## Coef. estimate: 0.09732 ## Standard Error: 0.02326 ## t-value: 4.18445 ## ## Sensitivity Statistics: ## Partial R2 of treatment with outcome: 0.02187 ## Robustness Value, q = 1 : 0.13878 ## Robustness Value, q = 1 alpha = 0.05 : 0.07626 ## ## For more information, check summary. summary(sensitivity) ## Sensitivity Analysis to Unobserved Confounding ## ## Model Formula: peacefactor ~ directlyharmed + age + farmer_dar + herder_dar + ## pastvoted + hhsize_darfur + female + village ## ## Null hypothesis: q = 1 and reduce = TRUE ## -- This means we are considering biases that reduce the absolute value of the current estimate. ## -- The null hypothesis deemed problematic is H0:tau = 0 ## ## Unadjusted Estimates of &#39;directlyharmed&#39;: ## Coef. estimate: 0.0973 ## Standard Error: 0.0233 ## t-value (H0:tau = 0): 4.1844 ## ## Sensitivity Statistics: ## Partial R2 of treatment with outcome: 0.0219 ## Robustness Value, q = 1: 0.1388 ## Robustness Value, q = 1, alpha = 0.05: 0.0763 ## ## Verbal interpretation of sensitivity statistics: ## ## -- Partial R2 of the treatment with the outcome: an extreme confounder (orthogonal to the covariates) that explains 100% of the residual variance of the outcome, would need to explain at least 2.19% of the residual variance of the treatment to fully account for the observed estimated effect. ## ## -- Robustness Value, q = 1: unobserved confounders (orthogonal to the covariates) that explain more than 13.88% of the residual variance of both the treatment and the outcome are strong enough to bring the point estimate to 0 (a bias of 100% of the original estimate). Conversely, unobserved confounders that do not explain more than 13.88% of the residual variance of both the treatment and the outcome are not strong enough to bring the point estimate to 0. ## ## -- Robustness Value, q = 1, alpha = 0.05: unobserved confounders (orthogonal to the covariates) that explain more than 7.63% of the residual variance of both the treatment and the outcome are strong enough to bring the estimate to a range where it is no longer &#39;statistically different&#39; from 0 (a bias of 100% of the original estimate), at the significance level of alpha = 0.05. Conversely, unobserved confounders that do not explain more than 7.63% of the residual variance of both the treatment and the outcome are not strong enough to bring the estimate to a range where it is no longer &#39;statistically different&#39; from 0, at the significance level of alpha = 0.05. ## ## Bounds on omitted variable bias: ## ## --The table below shows the maximum strength of unobserved confounders with association with the treatment and the outcome bounded by a multiple of the observed explanatory power of the chosen benchmark covariate(s). ## ## Bound Label R2dz.x R2yz.dx Treatment Adjusted Estimate Adjusted Se ## 1x female 0.0092 0.1246 directlyharmed 0.0752 0.0219 ## 2x female 0.0183 0.2493 directlyharmed 0.0529 0.0204 ## 3x female 0.0275 0.3741 directlyharmed 0.0304 0.0187 ## Adjusted T Adjusted Lower CI Adjusted Upper CI ## 3.4389 0.0323 0.1182 ## 2.6002 0.0130 0.0929 ## 1.6281 -0.0063 0.0670 12.11 Referências https://grantmcdermott.com/interaction-effects/ Brambor, T., Clark, W. R., &amp; Golder, M. (2006). Understanding interaction models: Improving empirical analyses. Political analysis, 14(1), 63-82. Corrêa, D. S. (2015). Economy, Ideology and Elections in Latin America. DADOS: Revista de Ciencias Sociais, 58(2), 401. "],["glm.html", "Capítulo 13 - GLM 13.1 PLM 13.2 Logística 13.3 Logística como variável latente 13.4 Probit 13.5 Logística como GLM 13.6 Logística com múltiplos preditores 13.7 Ajuste do modelo 13.8 Regressão com dados Ordenados 13.9 Referências", " Capítulo 13 - GLM GLM é a sigla de Modelos Lineares Generalizados. A regressão logística pode ser considerada como um dos tipos de modelos lineares generalizados. Vamos nos concentrar em entender a regressão logística, como ela se conecta com GLM e, rapidamente, ver outros modelos de GLM que existem, como Poisson e probit. Até o momento, variáveis resposta binárias foram modeladas com regressão linear. 13.1 PLM Até o momento, variáveis respostas binárias foram modeladas com regressão linear. \\[ y_i \\sim N(\\alpha + \\beta x_i, \\sigma^2) \\] Nessa parametrização do modelo de regressão, vemos que a resposta é modelada como uma variável Gaussiana e, portanto, é uma variável contínua (em vez de binária). Além disso, não está limitada, possuindo suporte entre −∞ e +∞. Esses modelos são chamados também de Modelos de Probabilidade Linear (MPL, ou LPM na sigla em inglês). Explicaremos mais à frente porque são chamados desse modo, após apresentarmos a regressão logística e probit. E então poderemos comparar modelos logísticos ou probit e MPL. 13.2 Logística Há várias formas de apresentar e/ou justificar a regressão logística. Qual delas você irá usar para pensar esse tipo de modelo depende do seu problema de pesquisa e das suas preferências sobre o que funciona melhor para você. 13.2.1 Logística como melhoria em relação ao MPL Como vimos, o problema da regressão linear para dados binários é que considera que a variável resposta possui distribuição Gaussiana. Faz muito mais sentido modelar dados binários como seguindo uma distribuição de Bernoulli, com parâmetro \\(p_i\\). Poderíamos, portanto, tentar reescrever um modelo para \\(y\\) binário da seguinte forma: \\[ y_i \\sim Ber(p_i) \\] Em que \\(p_i\\) é a probabilidade de sucesso para a unidade \\(i\\). Dessa forma, contudo, não incluí nenhum preditor para estimar o \\(p_i\\), e gostaríamos de fazê-lo. Posso então escrever algo como: \\[ p_i(x_i) = \\alpha + \\beta x_i \\] O problema dessa formulação é que probabilidades devem estar entre 0 e 1, e nada garante que \\(\\alpha + \\beta x_i\\) seja um número entre 0 e 1. Então, uma saída é tentar achar uma função \\(f\\) que transforme \\(\\alpha + \\beta x_i\\) em números entre 0 e 1, ou seja, \\(0 \\le f(\\alpha + \\beta x_i) \\le 1\\). E uma função que tem essa propriedade é a logística padrão (também chamada de sigmoide), dada por: \\[ f(x) = \\frac{1}{1 + \\exp(-x)} \\] Então, nossa relação entre a probabilidade e os preditores fica: \\[ p_i = \\frac{1}{1 + \\exp(-(\\alpha + \\beta x_i))} \\] Conectando com nossa variável resposta, temos: \\[ y_i \\sim Ber(\\frac{1}{1 + \\exp(-(\\alpha + \\beta x_i))}) \\] A probabilidade de sucesso, condicional ao preditor, fica então: \\[\\begin{align} Pr(y_i=1|x) = \\frac{1}{1 + \\exp(-(\\alpha + \\beta x_i))} \\tag{13.1} \\end{align}\\] Assim, a modelagem de variável binária por meio da regressão linear pode ser pensada como um modelo em que a probabilidade \\(p_i\\) é modelada sem a função logística, o que significa que podemos acabar com previsões de probabilidades negativas ou maiores que 1, o que não faz sentido. Essa é uma razão para usar modelos logísticos em vez de lineares. 13.2.2 Logito Às vezes a logística é designada como a função inversa da logito, em que a logito é dada por: \\[ logito(p) = log(\\frac{p}{1-p}) \\] Para entender isso, vamos lembrar que, se \\(f(x) = x + 2\\) é uma função, sua inversa \\(f^{-1}(x)\\), se existir, pode ser descoberta pelo algoritmo em que chamamos \\(f(x)\\) de \\(y\\), trocamos \\(y\\) por \\(x\\) e resolvemos para \\(y\\): \\[\\begin{align} y = x + 2 \\\\ x = y + 2 \\\\ y = x - 2 \\\\ f^{-1}(x) = x - 2 \\end{align}\\] Para uma função \\(f(x) = log(x+2)\\), a inversa é: \\[\\begin{align} y = log(x + 2) \\\\ x = log(y + 2) \\\\ \\exp(x) = \\exp(log(y + 2)) \\\\ \\exp(x) = y + 2 \\\\ y = \\exp(x) + 2 \\\\ f^{-1}(x) = \\exp(x) + 2 \\end{align}\\] Então, se \\(f(X) = log(x)\\), \\(f^{-1}(x) = \\exp(x)\\). Disso se segue que: \\[\\begin{align} f(x) = log(\\frac{x}{1-x}) \\\\ y = log(\\frac{x}{1-x}) \\\\ x = log(\\frac{y}{1-y}) \\\\ \\exp(x) = \\exp(log(\\frac{y}{1-y})) \\\\ \\exp(x) = \\frac{y}{1-y} \\\\ \\exp(x)(1-y) = y \\\\ \\exp(x)- y\\exp(x) = y \\\\ \\exp(x) = y + y\\exp(x) \\\\ \\exp(x) = y(1 + \\exp(x)) \\\\ \\frac{\\exp(x)}{(1 + \\exp(x))} = y \\\\ f^{-1}(x) = \\frac{\\exp(x)}{1 + \\exp(x)} \\\\ f^{-1}(x) = \\frac{\\frac{\\exp(x)}{\\exp(x)}}{\\frac{1 + \\exp(x)}{\\exp(x)}} \\\\ f^{-1}(x) = \\frac{1}{\\exp(-x) + 1}\\\\ f^{-1}(x) = \\frac{1}{1 + \\exp(-x)}\\\\ \\end{align}\\] No R, podemos acessar as duas funções por meio de: logit &lt;- qlogis invlogit &lt;- plogis E podemos modelar os dados de uma logística com nosso exemplo do Latinobarômetro usando apenas um preditor, ideologia, para simplificar. library(here) library(data.table) library(tidyverse) library(sjlabelled) # pra remover labelled variables library(haven) library(janitor) library(lubridate) library(knitr) library(broom) ## dados # https://www.latinobarometro.org/latContents.jsp lat_bar23 &lt;- sjlabelled::read_spss(here(&quot;Dados&quot;, &quot;Latinobarometro_2023_Eng_Spss_v1_0.sav&quot;), drop.labels = TRUE) %&gt;% mutate(S17 = as.Date(as.character(S17), &quot;%Y%m%d&quot;)) %&gt;% janitor::clean_names() ## Invalid date string (length=9): 09 032 23 # get_label(lat_bar23) lat_bar23 &lt;- lat_bar23 %&gt;% mutate(data_base = as.Date(paste(diareal, mesreal, &quot;2023&quot;, sep=&quot;-&quot;), &quot;%d-%m-%Y&quot;), idade = year(as.period(interval(s17,data_base))), econ_12_meses = ifelse(p6stgbs %in% c(1,2), &quot;better&quot;, ifelse(p6stgbs == 8, NA, &quot;other&quot;)), econ_12_meses = relevel(as.factor(econ_12_meses), ref = &quot;other&quot;), aprovacao_presidente = ifelse(p15stgbs == 0, NA, p15stgbs), ideologia = ifelse(p16st %in% c(97, 98, 99), NA, p16st), votaria_governo = ifelse(perpart == 4, NA, ifelse(perpart == 1, 1, 0)), genero = factor(sexo, labels = c(&quot;homem&quot;, &quot;mulher&quot;)), evangelico = ifelse(s1 %in% c(0,98), NA, ifelse(s1 %in% c(2,3,4,5), 1, 0))) # não considera adventista, testemunha Jeová, Mórmon ## Warning: There was 1 warning in `mutate()`. ## ℹ In argument: `idade = year(as.period(interval(s17, data_base)))`. ## Caused by warning in `.local()`: ## ! NAs introduced by coercion to integer range br_latbar_23 &lt;- lat_bar23 %&gt;% mutate(idenpa = remove_all_labels(idenpa)) %&gt;% # haven_labelled problems filter(idenpa == 76) %&gt;% ## seelciona brasil filter(!is.na(votaria_governo) &amp; !is.na(evangelico) &amp; !is.na(ideologia) &amp; !is.na(econ_12_meses)) reg_logistica &lt;- glm(votaria_governo ~ ideologia, data=br_latbar_23, family=binomial(link= &quot;logit&quot;)) reg_logistica %&gt;% tidy() %&gt;% kable() ## Warning in attr(x, &quot;align&quot;): &#39;xfun::attr()&#39; is deprecated. ## Use &#39;xfun::attr2()&#39; instead. ## See help(&quot;Deprecated&quot;) ## Warning in attr(x, &quot;format&quot;): &#39;xfun::attr()&#39; is deprecated. ## Use &#39;xfun::attr2()&#39; instead. ## See help(&quot;Deprecated&quot;) term estimate std.error statistic p.value (Intercept) 3.1399152 0.3377743 9.295898 0 ideologia -0.2850991 0.0385935 -7.387224 0 # plotando # usando base R # código adaptado de Regression and Other Stories (Gelman et. al.) library(arm) n &lt;- nrow(br_latbar_23) ideologia_jitt &lt;- br_latbar_23$ideologia + runif(n, -.2, .2) vote_jitt &lt;- br_latbar_23$votaria_governo + ifelse(br_latbar_23$votaria_governo==0, runif(n, .005, .05), runif(n, -.05, -.005)) curve(invlogit(reg_logistica$coef[1] + reg_logistica$coef[2]*x), from = -5,to=24, ylim=c(0,1), xlim=c(-5, 24), xaxt=&quot;n&quot;, xaxs=&quot;i&quot;, ylab=&quot;Pr (voto governo)&quot;, xlab=&quot;Ideologia&quot;, lwd=.5, yaxs=&quot;i&quot;) curve(invlogit(reg_logistica$coef[1] + reg_logistica$coef[2]*x), 1, 11, lwd=3, add=TRUE) axis(1, 1:11) mtext(&quot;(left)&quot;, side=1, line=1.7, at=1, adj=.5) mtext(&quot;(right)&quot;, 1, 1.7, at=11, adj=.5) points(ideologia_jitt, vote_jitt, pch=20, cex=.1) O gráfico apresenta o modelo ajustado da logística, juntamente com os dados (jittered). Cada ponto na curva representa \\(Pr(Y=1|X=x)\\). A linha mais escura é o modelo ajustado para os pontos da amostra. Podemos observar que há mais dados de pessoas que votariam com o governo entre os indivíduos de esquerda, com mais observações votando contra o governo apenas entre as pessoas mais centristas, o que faz sentido. Também observamos muitas pessoas de direita votando com o governo, indicando que entre as pessoas de direita, o dado não diferencia tanto quem vota a favor ou contra o governo, sugerindo que precisamos considerar outras variáveis. Como a logística é curva, o efeito preditivo de \\(x\\) sobre a probabilidade \\(y=1\\) não é constante (ao contrário de regressões lineares). No caso do nosso gráfico, passar de \\(1\\) para \\(2\\) na ideologia tem um efeito significativamente menor (de 95% para 93%) do que passar de \\(6\\) para \\(7\\) (de 81% para 76%), por exemplo. Para calcular o efeito preditivo nesses casos, fazemos: newdata &lt;- data.frame(ideologia = 1:11) previsao &lt;- predict(reg_logistica, newdata =newdata, type = &quot;response&quot;) print(round(previsao, 2)) ## 1 2 3 4 5 6 7 8 9 10 11 ## 0.95 0.93 0.91 0.88 0.85 0.81 0.76 0.70 0.64 0.57 0.50 A interpretação do coeficiente é um pouco difícil na logística, mas um caminho é pensar que o efeito é máximo no centro da curva, em que \\(\\alpha + \\beta x = 0\\). Nesse caso, a inclinação da curva (a derivada) é dada por \\(\\beta/4\\). Ou seja, podemos dividir o coeficiente estimado por quatro para ter uma ideia do máximo impacto preditivo. Em nosso caso, com um coeficiente aproximado de \\(-0,29\\), temos que no máximo a mudança em uma unidade diminui a probabilidade em no máximo 7,25%. E nos dados, esse efeito máximo é alcançado quando \\(3.14 + -0.29*x = 0\\), ou seja, aproximadamente \\(x = 11\\). 13.2.3 Odds Ratio - Razão de chances Outra forma de interpretar a regressão logística é utilizando a parametrização do logito. O coeficiente assume a forma de razão de chances, onde uma chance é \\(p/(1-p)\\), e razão de chances é a divisão de duas chances (o que quer que isso signifique). Uma vantagem de trabalhar com razão de chances é que a interpretação do coeficiente se torna linear, em vez de não-linear, conforme mostra a equação abaixo. \\[ log(\\frac{Pr(Y=1|X=x)}{Pr(Y=0|X=x)}) = \\alpha + \\beta x \\] Eu acho complicado entender o que é uma razão de chances, então preferimos trabalhar com probabilidade. Mas existe essa outra parametrização. 13.2.4 Coeficientes e erros padrões Como a logística é estimada por máxima verossimilhança, os procedimentos usuais valem e podemos aproximar a interpretação dos coeficientes para uma normal com amostras grandes. 13.3 Logística como variável latente Uma outra forma de interpretar ou justificar a logística é com a formulação de variável latente. Imagine que existe uma variável latente (não observada), \\(z\\), que é contínua e reflete a propensão a votar no governo. Porém, nós só observamos os valores \\(1\\) ou \\(0\\), de tal modo que podemos escrever: \\[\\begin{align} y_i = 1, se &amp; &amp; z_i &gt; 0 \\\\ y_i = 0, c.c. \\\\ z_i = \\alpha + \\beta x_i + e_i, e_i \\sim logistica \\end{align}\\] A distribuição logística é dada por: \\[ Pr(e_i &lt; x) = \\frac{\\exp(x)}{1 + \\exp(x)} \\] Portanto, temos que: \\[\\begin{align} Pr(y_i = 1) = Pr(z_i &gt; 0) = \\\\ Pr(e_i + \\alpha + \\beta x_i &gt; 0) = \\\\ Pr(e_i &gt; -\\alpha - \\beta x_i) \\\\ \\textrm{Pela regra do complemento:} \\\\ Pr(e_i &gt; -\\alpha - \\beta x_i) = 1 - Pr(e_i &lt; -\\alpha - \\beta x_i) = \\\\ 1 - \\frac{\\exp(- \\alpha - \\beta x_i)}{1 + \\exp(- \\alpha - \\beta x_i)} = \\\\ \\frac{1 + \\exp(- \\alpha - \\beta x_i) - \\exp(- \\alpha - \\beta x_i)}{1 + \\exp(- \\alpha - \\beta x_i)} = \\\\ \\frac{1}{1 + \\exp(- \\alpha - \\beta x_i)} \\tag{13.2} \\end{align}\\] Vimos assim que as parametrizações de (13.1) e (13.2) são equivalentes. 13.4 Probit Se, por outro lado, supusermos que o termo de erro tem distribuição normal, em vez de distribuição logística, teremos um modelo probit. \\[ z_i = \\alpha + \\beta x_i + e_i, e_i \\sim N(0, \\sigma^2) \\] Quando \\(\\sigma = 1.6\\), essa formulação é aproximadamente a regressão logística. Isso significa que o coeficiente da probit é aproximadamente o da logística/1.6 e vice-versa, isto é, podemos multiplicar o coeficiente da probit por 1.6. Vamos checar no R. reg_logistica &lt;- glm(votaria_governo ~ ideologia, data=br_latbar_23, family=binomial(link= &quot;logit&quot;)) reg_probit &lt;- glm(votaria_governo ~ ideologia, data=br_latbar_23, family=binomial(link= &quot;probit&quot;)) library(stargazer) stargazer(reg_logistica, reg_probit, type = &quot;html&quot;) Dependent variable: votaria_governo logistic probit (1) (2) ideologia -0.285*** -0.165*** (0.039) (0.021) Constant 3.140*** 1.836*** (0.338) (0.178) Observations 472 472 Log Likelihood -239.376 -239.128 Akaike Inf. Crit. 482.753 482.256 Note: p&lt;0.1; p&lt;0.05; p&lt;0.01 13.5 Logística como GLM Uma outra forma de pensar a logística é como um modelo linear generalizado. Nós vimos que o modelo linear precisou entrar em uma função \\(f\\) que transformasse os intervalos entre \\(0\\) e \\(1\\). Podemos considerar o modelo de regressão linear como um caso particular, em que temos a função identidade \\(f(x) = x\\) e a distribuição da resposta é modelada como Gaussiana. Na logística,a função de ligação é logística, e a resposta modelada como Bernoulli (e para \\(n\\) dados, binomial). Similarmente, se tenho dados discretos \\(\\{1, 2, 3, ..., n\\}\\), posso modelar meus dados com uma Poisson e com uma função \\(f\\) específica, que chamamos de função de ligação. 13.6 Logística com múltiplos preditores reg_logistica1 &lt;- glm(votaria_governo ~ ideologia + evangelico, data=br_latbar_23, family=binomial(link= &quot;logit&quot;)) reg_logistica1 %&gt;% tidy() %&gt;% kable() ## Warning in attr(x, &quot;align&quot;): &#39;xfun::attr()&#39; is deprecated. ## Use &#39;xfun::attr2()&#39; instead. ## See help(&quot;Deprecated&quot;) ## Warning in attr(x, &quot;format&quot;): &#39;xfun::attr()&#39; is deprecated. ## Use &#39;xfun::attr2()&#39; instead. ## See help(&quot;Deprecated&quot;) term estimate std.error statistic p.value (Intercept) 3.3035245 0.3498086 9.443805 0.0000000 ideologia -0.2780476 0.0391497 -7.102157 0.0000000 evangelico -0.7387838 0.2432515 -3.037120 0.0023885 Para facilitar a comparação de coeficientes, pode ser mais fácil padronizar as variáveis. No caso, vamos padronizar apenas ideologia (precisamos supor que é uma variável contínua, medida com valores discretos). br_latbar_23 &lt;- br_latbar_23 %&gt;% mutate(ideologia_pad = (ideologia - mean(ideologia))/sd(ideologia)) reg_logistica2 &lt;- glm(votaria_governo ~ ideologia_pad + evangelico, data=br_latbar_23, family=binomial(link= &quot;logit&quot;)) reg_logistica2 %&gt;% tidy() %&gt;% kable() ## Warning in attr(x, &quot;align&quot;): &#39;xfun::attr()&#39; is deprecated. ## Use &#39;xfun::attr2()&#39; instead. ## See help(&quot;Deprecated&quot;) ## Warning in attr(x, &quot;format&quot;): &#39;xfun::attr()&#39; is deprecated. ## Use &#39;xfun::attr2()&#39; instead. ## See help(&quot;Deprecated&quot;) term estimate std.error statistic p.value (Intercept) 1.4449650 0.1497791 9.647304 0.0000000 ideologia_pad -0.9975056 0.1404511 -7.102157 0.0000000 evangelico -0.7387838 0.2432515 -3.037120 0.0023885 Usando a regra de dividir por quatro, temos que: ideologia tem efeito máximo de redzir a probabilidade em 25%, e evangélico de diminuir em 18%. Plotando graficamente os dois preditores, temos: jitter_binary &lt;- function(a, jitt=0.05){ ifelse(a==0, runif(length(a), 0, jitt), runif(length(a), 1 - jitt, 1)) } br_latbar_23$votaria_governo_jitter &lt;- jitter_binary(br_latbar_23$votaria_governo) ideologia_jitt &lt;- br_latbar_23$ideologia_pad + runif(n, -.05, .05) plot(ideologia_jitt, br_latbar_23$votaria_governo_jitter, xlim=c(0,max(ideologia_jitt))) curve(invlogit(cbind(1, x, 0) %*% coef(reg_logistica2)), add=TRUE) curve(invlogit(cbind(1, x, 1.0) %*% coef(reg_logistica2)), add=TRUE) 13.7 Ajuste do modelo Nós vimos que podemos usar o erro quadrático médio com regressão linear para comparar ajustes do modelo. Porém, para variáveis binárias, há formas melhores de comparar a capacidade preditiva do modelo. A mais tradicional é o log da verossimilhança, dado pela soma para todos as observações de: \\((ylog(p) + (1-y)log(1-p))\\). No limite, um modelo perfeito, que classifica corretmente todas as observações (isto é, coloca probabilidade \\(1\\) de sucesso e \\(0\\) de fracasso) teria um log da verossimilhança igual a zero. Portanto, quanto mais próximo de zero, melhor o ajuste do modelo. E um modelo “burro”, que previsse probabilidade de 50% para cada ponto, teria um total de \\(n \\cdot log(.5) = n \\cdot -0.693\\). Um modelo um pouco melhor seria um que previsse a média, isto é, se 60% dos casos são sucesso, pode prever para todas as observações \\(p=.6\\). Esses modelos simples servem como comparação. No R: y_hat &lt;- predict(reg_logistica, type=&quot;response&quot;) y_obs &lt;- br_latbar_23$votaria_governo log_likelihood &lt;- sum(y_obs*log(y_hat) + (1-y_obs)*log(1 - y_hat)) print(log_likelihood) [1] -239.3764 print(sum(y_obs*log(mean(y_obs)) + (1-y_obs)*log(1 - mean(y_obs)))) [1] -274.8595 print(nrow(br_latbar_23)*log(.5)) [1] -327.1655 stargazer(reg_logistica, reg_logistica1, type = &quot;html&quot;) Dependent variable: votaria_governo (1) (2) ideologia -0.285*** -0.278*** (0.039) (0.039) evangelico -0.739*** (0.243) Constant 3.140*** 3.304*** (0.338) (0.350) Observations 472 472 Log Likelihood -239.376 -234.816 Akaike Inf. Crit. 482.753 475.632 Note: p&lt;0.1; p&lt;0.05; p&lt;0.01 Para mais detalhes sobre regressão logística, nossa referência favorita é o livro de Gelman, Hill e Vehtati, Regression and other Stories, de onde tiramos alguns dos códigos do presente capítulo. Capítulos 13 e 14 contém bastante material a um nível que exige um nível de matemática similar ao que temos usado em nosso livro. 13.8 Regressão com dados Ordenados Até agora tratamos de variáveis respostas binárias. Porém, Há outros tipos de dados categóricos além de binários, e passamos agora a apresentar como tratar desses dados. A aboragem escolhida aqui é aquela de modelos de escolha discreta, que é uma extensão da abordagem de variável latente que vimos acima para regressão logística. A ideia é que existe uma probabilidade que o indivíduo \\(i\\) escolha a alternativa \\(j\\) (de um conjunto discreto e mutuamente excludente de \\(J\\) alternativas, daí o nome de escolhas discretas), \\(p_{ij}\\). Como anteriormente, a relação entre a probabilidade e os preditores é dada por uma função de ligação \\(f(u)\\) e \\(u = X_ij\\beta\\), ou seja, uma modelo linear das variáveis preditoras e \\(u\\). E vamos considerar que podemos ordenar nossa variável resposta, isto é, que as opções possam ser consideradas como um conjunto ordenado. Exemplos de variáveis respostas incluem escalas likerts (discordo muito, discordo pouco, concordo pouco, concordo muito), escalas discretas de ideologia, em que os extremos vão da esquerda à direita, votação em que consideramos não, abstenção e sim, assinar um tratado sem reservas, com reservas ou não assinar, e assim por diante. Seguindo com a formulação de variável latente, podemos pensar que existe uma variável \\(z\\), não observada, tal que: \\(z_i = X_{ij}\\beta + e\\). Supondo, para ilustrar, que tenhamos \\(4\\) categorias ou opções, como no exemplo da escala Likert, então temos \\(J=4\\) categorias ordenadas de \\(1-4\\), e o que nós observamos é uma escolha a dependenter de \\(J-1=3\\) pontos de corte, \\(\\tau_1, \\tau_2, \\tau_3\\): \\[ z = \\begin{cases} 1 &amp; \\text{se } z \\leq \\tau_1 \\\\ 2 &amp; \\text{se } \\tau_1 &lt; z \\leq \\tau_2 \\\\ 3 &amp; \\text{se } \\tau_2 &lt; z \\leq \\tau_3 \\\\ 4 &amp; \\tau_3 &lt; z \\end{cases} \\] Figure 13.1: Mapa entre Z e Y Vejam que nesse modelo, além dos coeficientes da regressão, também temos de estimar os pontos de corte dados pelos parâmetros de \\(\\tau_1\\) a \\(\\tau_3\\). De fato, a probabilidade de escolhermos a primeira categoria, é: \\[Pr(Y_i = 1) = Pr(Z_i \\leq \\tau_1) = Pr(X_{ij}\\beta + e \\leq \\tau_1) = Pr(e \\leq \\tau_1 - X_{ij}\\beta)\\] Similarmente, a probabilidade de um respondente escolher a segunda categoria é: \\[Pr(Y_i = 2) = Pr( \\tau_1 &lt; Z_i \\leq \\tau_2) = Pr(\\tau_1 &lt; X_{ij}\\beta + e \\leq \\tau_1) = Pr(\\tau_1 - X_{ij}\\beta &lt; e \\leq \\tau_1 - X_{ij}\\beta)\\] Se supusermos que \\(e\\) tem distribuição logística com média zero e variância \\(\\frac{\\pi^2}{3}\\), que é a logística padrão, temos o modelo de regressão logística ordenado. Se supusermos que o erro tem distribuição normal, com média zero e variância 1, temos o modelo probit ordenado. Vejamos um exemplo simulado e em seguida uma aplicação empírica. Em nossa simulação, vamos supor que um survey foi rodado e medimos, usando uma escala likert com quatro opções, se a pessoa concorda ou não com uma dada frase. Podemos pensar que a variável latente \\(z\\) é a propensão a concordar com a frase, e só observamos uma das quatro categorias. # library(tidyverse) set.seed(1234) n &lt;- 1000 # número de indivíduos m &lt;- 4 # número de alternativas (discordo muito, pouco, concordo pouco, muito) x_subject &lt;- rnorm(n) erro &lt;- rlogis(n, location = 0, scale = 1) beta_0 &lt;- .5 beta1 &lt;- 2 z &lt;- beta_0 + beta1*x_subject + erro tau1 &lt;- -2 tau2 &lt;- 1 tau3 &lt;- 4 y &lt;- ifelse(z &lt;= tau1, &quot;discordo muito&quot;, ifelse(z &lt;= tau2, &quot;discordo pouco&quot;, ifelse(z &lt;= tau3, &quot;concordo pouco&quot;, &quot;concordo muito&quot;))) df &lt;- data.frame(y=y, x1 = x_subject) df &lt;- df %&gt;% mutate(y = factor(y, ordered=T)) # checando a ordem levels(df$y) ## [1] &quot;concordo muito&quot; &quot;concordo pouco&quot; &quot;discordo muito&quot; &quot;discordo pouco&quot; # ordem trocada (inversa) #vamos corrigir df &lt;- df %&gt;% mutate(y = factor(y, levels = c(&quot;discordo muito&quot;, &quot;discordo pouco&quot;, &quot;concordo pouco&quot;, &quot;concordo muito&quot;), ordered=T)) # checando a ordem levels(df$y) ## [1] &quot;discordo muito&quot; &quot;discordo pouco&quot; &quot;concordo pouco&quot; &quot;concordo muito&quot; # correto # se quiser checar e está ordenado is.ordered(df$y) ## [1] TRUE library(ordinal) reg_ord &lt;- clm(y ~ x1, data = df, link = &quot;logit&quot;) summary(reg_ord) ## formula: y ~ x1 ## data: df ## ## link threshold nobs logLik AIC niter max.grad cond.H ## logit flexible 1000 -896.27 1800.54 6(0) 6.70e-09 7.9e+00 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## x1 2.15892 0.09835 21.95 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Threshold coefficients: ## Estimate Std. Error z value ## discordo muito|discordo pouco -2.62133 0.12144 -21.586 ## discordo pouco|concordo pouco 0.33123 0.08152 4.063 ## concordo pouco|concordo muito 3.55039 0.15692 22.625 A primeira forma de interpretar os coeficientes é que se o \\(\\beta\\) é positivo, quanto maior o valor do preditor, maior a propensão a concordar com a frase e vice-versa se for negativo. Isso é fácil de ver na formulação da variável latente. Para analisar as probabilidades de cada categoria precisamos olhar também os pontos de corte. Vejam que os coeficientes dos pontos de corte estão “errando” por aproximadamente meio em relação aos parâmetros. Isso é porque o modelo não estima intercepto. Se rodarmos a simulação de novo com \\(\\beta_0 = 0\\), iremos nos aproximar dos verdadeiros pontos de corte. A razão é que não é possível identificar um intercepto distinto do valor de \\(\\tau\\), ou seja, é como se tivéssemos subtraído \\(.5\\) de todos os \\(\\tau\\) e rodado o modelo sem intercepto. Em too caso, vamos interpretar os pontos de corte estimados. O que eles nos dizem, em conjunto com o \\(\\beta\\), é a probabilidade de uma pessoa escolher uma dada categoria. Para interpretar os resultados, vamos usar mais uma vez o pacote marginaleffects: 13.9 Referências Gelman, A., Hill, J., &amp; Vehtari, A. (2020). Regression and other stories. Cambridge University Press. "],["apêndice.html", "Capítulo 14 - Apêndice 14.1 Importação 14.2 Gráficos e Tabelas 14.3 Limpeza de dados 14.4 Modelagem de dados 14.5 Outros 14.6 Álgebra Linear 14.7 Econometria de série de tempo 14.8 Dados Espaciais", " Capítulo 14 - Apêndice Iremos tratar aqui de tópicos especiais. Em primeiro lugar, do R. Vamos listar aqui os pacotes do R que eu mais utilizo e as principais funções. 14.1 Importação Para ler arquivos em csv ou formato tabular, prefiro a função fread do data.table. Para ler arquivos de outrossoftwares, como stata ou SPSS, vejam as funções do pacote haven. Para arquivos em formato xls ou xlsx, do Excel, uso a função read_excel do pacote readxl. 14.2 Gráficos e Tabelas Para visualização, uso o ggplot e pacotes que dialogam com o ggplot2. A única exceção é para plotar o gráfico da logística, que usei o R base e a função curve. Para gerar tabelas em Rmarkdown, uso quatro funções (deveria ser mais consistente e usar só uma ou duas), kable do knitr, gt do pacote de mesmo nome, stargazer, também do pacote de mesmo nome e a função etable do pacote fixest, quando rodo regressões desse pacote. 14.3 Limpeza de dados Para limpeza de dados, além do tidyverse e tidyr, uso o pacote janitor (particularmente a função clean_names) e o pacote lubridate para trabalhar com datas. Com relação a textos, o pacote stringr. 14.4 Modelagem de dados Para regressões limpes, uso o R base e suas funções lm e glm. Para dados em painel, prefiro o pacote fixest e suas funções, como a feols, que evita de eu ter de chamar o pacote sandwich para erro padrão robusto. O fixest é melhor que o mais tradicional e antigo pacote plm, que era mais usado para dados em painel antigamente. Para modelos Bayesianos, vario entre rstanarm e brms. 14.5 Outros Checagem do modelo, o pacote performance. Para testes informais de permutação, o pacote nullabor. Para instalação de pacotes direto do gihutb, devtools. Para ajudar na reproducibilidade, uso o pacote here para não vincular aos meus diretórios específicos e evito usar osetwd para apontar o diretório. 14.6 Álgebra Linear Aqui apresentamos alguns tópicos de álgebra linear que podem ser úteis. 14.6.1 Espaço de vetores e subspaço O espaço vetorial de dimensão \\(n\\) é o conjunto infinito de todos os vetores \\(x = [x_1, x_2, \\cdots, x_n]\\), e as coordenadas \\(x-i\\) podem ser quaisquer números reais. Dizemos que um subespaço do espaço vetorial \\(n\\)-dimensional que é gerado por um conjunto de \\(k\\) vetores \\(x = [x_1, x_2, \\cdots, x_k]\\), em que \\(k\\) pode ser eventualmente igual a \\(n\\) (ou menor), é o subconjunto de vetores \\(y\\) naquele espaço que pode ser expresso como uma combinação linear do conjunto gerador: \\(y = a_1x_1 + a_2x_2 + \\cdots + a_kx_k\\) Dizemos que o conjunto de vetores \\(x = [x_1, x_2, \\cdots, x_k]\\) gera o subespaço que ele define. 14.6.2 Projeção linear # Load necessary library library(ggplot2) # Define vectors u and v u &lt;- c(3, 2) v &lt;- c(4, 0) # Calculate the projection of u onto v proj_v_u &lt;- (sum(u * v) / sum(v * v)) * v # Create a data frame for plotting data &lt;- data.frame( x = c(0, u[1], 0, v[1], 0, proj_v_u[1]), y = c(0, u[2], 0, v[2], 0, proj_v_u[2]), label = c(&quot;Origin&quot;, &quot;u&quot;, &quot;Origin&quot;, &quot;v&quot;, &quot;Origin&quot;, &quot;Projection&quot;) ) # Create the plot ggplot(data) + geom_segment(aes(x = x[1], y = y[1], xend = x[2], yend = y[2]), arrow = arrow(length = unit(0.3, &quot;cm&quot;)), color = &quot;blue&quot;, size = 1.2) + geom_segment(aes(x = x[3], y = y[3], xend = x[4], yend = y[4]), arrow = arrow(length = unit(0.3, &quot;cm&quot;)), color = &quot;red&quot;, size = 1.2) + geom_segment(aes(x = x[5], y = y[5], xend = x[6], yend = y[6]), linetype = &quot;dashed&quot;, color = &quot;green&quot;, size = 1.2) + annotate(&quot;text&quot;, x = u[1] + 0.5, y = u[2], label = &quot;u&quot;, color = &quot;blue&quot;) + annotate(&quot;text&quot;, x = v[1] + 0.5, y = v[2], label = &quot;v&quot;, color = &quot;red&quot;) + annotate(&quot;text&quot;, x = proj_v_u[1] + 0.5, y = proj_v_u[2], label = &quot;proj_v_u&quot;, color = &quot;green&quot;) + xlim(0, max(u[1], v[1], proj_v_u[1]) + 1) + ylim(0, max(u[2], v[2], proj_v_u[2]) + 1) + theme_minimal() + labs(title = &quot;Projection of Vector u onto Vector v&quot;, x = &quot;X-axis&quot;, y = &quot;Y-axis&quot;) ## Warning in geom_segment(aes(x = x[1], y = y[1], xend = x[2], yend = y[2]), : All aesthetics have length 1, but the data has 6 rows. ## ℹ Please consider using `annotate()` or provide this layer with data ## containing a single row. ## Warning in geom_segment(aes(x = x[3], y = y[3], xend = x[4], yend = y[4]), : All aesthetics have length 1, but the data has 6 rows. ## ℹ Please consider using `annotate()` or provide this layer with data ## containing a single row. ## Warning in geom_segment(aes(x = x[5], y = y[5], xend = x[6], yend = y[6]), : All aesthetics have length 1, but the data has 6 rows. ## ℹ Please consider using `annotate()` or provide this layer with data ## containing a single row. The plot above shows the vectors \\(\\mathbf{u}\\) (in blue) and \\(\\mathbf{v}\\) (in red), along with the projection of \\(\\mathbf{u}\\) onto \\(\\mathbf{v}\\) (in green, dashed line). The projection is the closest point on the line defined by \\(\\mathbf{v}\\) to the point represented by \\(\\mathbf{u}\\). This visualization helps in understanding how projections work in a geometric sense. 14.7 Econometria de série de tempo Uma série de tempo é caracterizada por um vetor do tipo \\(\\{y_t, y_{t+1}, y_{t+2}, \\dots + t_{t+k}\\}\\). Em uma análise de corte transversal, nós tipicamente assumimos que a amostra era aleatória e iid. Aqui, essa suposição não é plausível devido à provável correlação temporal. A razão pela qual assumimos iid é porque precisamos de alguma forma de constância para que possamos aprender algo sobre a nossa variável. Se o passado não se assemelhar ao futuro, nada poderá ser aprendido. Tipicamente, a constância assumida é estacionariedade. Ela implica que a distribuição de \\(y_t\\) é estável ao longo do tempo. Em particular, que a distribuição de \\(y_t\\) é independente de \\(t\\). Isso vai implicar que a média de \\(y\\) não depende de \\(t\\), bem como a variância. 14.7.1 Modelos básicos O modelo mais básico de série de tempo é do tipo: \\[ y_t = \\mu + e_t + \\theta e_{t-1} \\] Nesse modelo, vemos que o \\(y\\) do presente depende do erro presente e do erro do passado imediato. Nós dizemos que o termo de erro segue uma Moving Average de ordem 1, denotado por \\(MA(1)\\). O termo Média Móvel vem do fato de que o \\(y_t\\) é uma média ponderada dos erros \\(t\\) e \\(t-1\\). Tipicamente assumimos que o termo de erro \\(e_t\\) possui \\(\\mathbb{E}[e_t] = 0\\) e variância igual a \\(\\sigma^2\\), ou seja, \\(e_t \\sim N(0, \\sigma^2)\\). Esse erro é chamado de ruído branco, para indicar que não possui sinal. Vamos agora provar alguns fatos. Variância de \\(y_t\\) \\[\\begin{align*} Var(y_t) = Var(\\mu + e_t + \\theta e_{t-1}) \\\\ \\text{como } \\mu \\text{ é constante, não altera a variância} \\\\ Var(y_t) = Var(e_t + \\theta e_{t-1}) \\\\ Var(y_t) = Var(e_t) + Var(\\theta e_{t-1}) \\text{, usando que o erro é iid} \\\\ Var(y_t) = Var(e_t) + \\theta^2 Var(e_{t-1}) \\\\ Var(y_t) = \\sigma^2 + \\theta^2 \\sigma^2 \\\\ Var(y_t) = \\sigma^2(1 + \\theta^2 ) \\\\ \\end{align*}\\] Covariância de ordem 1. \\[\\begin{align*} Cov(y_t, y_{t-1}) = Cov(\\mu + e_t + \\theta e_{t-1}, \\mu + e_{t-1} + \\theta e_{t-2} ) &amp;= \\\\ \\text{como } \\mu \\text{ é constante, não altera a covariância} \\\\ Cov(e_t + \\theta e_{t-1}, e_{t-1} + \\theta e_{t-2} ) &amp;= \\\\ Cov(e_t, e_{t-1}) + Cov(e_t, \\theta e_{t-2}) + Cov(\\theta e_{t-1}, e_{t-1}) + Cov(\\theta e_{t-1},\\theta e_{t-2} ) &amp;= \\\\ Cov(e_t, e_{t-1}) + \\theta Cov(e_t, e_{t-2}) + \\theta Cov( e_{t-1}, e_{t-1}) + \\theta^2Cov( e_{t-1}, e_{t-2} ) &amp;= \\\\ \\text{ lembrando que os erros são iid} \\\\ 0 + \\theta 0 + \\theta \\sigma^2 + \\theta^2 0 &amp;= \\\\ \\theta \\sigma^2 \\\\ \\end{align*}\\] Correlação de ordem 1 \\[\\begin{align*} cor(y_t, y_{t-1}) = \\frac{Cov(y_t, y_{t-1})}{sd(y_t) sd(y_{t-1})} \\\\ cor(y_t, y_{t-1}) = \\frac{\\theta \\sigma^2}{sd(y_t) sd(y_{t})} \\\\ cor(y_t, y_{t-1}) = \\frac{\\theta \\sigma^2}{\\sigma^2(1 + \\theta^2 )} \\\\ cor(y_t, y_{t-1}) = \\frac{\\theta}{(1 + \\theta^2 )} \\\\ \\end{align*}\\] E também é possível mostrar que a correlação de ordem maior que 1 é zero. Em geral, precisamos especificar o que acontece no primeiro período da série, e tipicamente estabelecemos algo como \\(y_{t0} = y_0\\). Vale escrever a equação acima com o termo de erro do lado direito e notar que existe autocorrelação serial: \\[\\begin{align} y_{t} = \\mu + e_{t} + \\theta e_{t-1} \\\\ e_{t} = \\theta e_{t-1} - y_{t} \\\\ \\text{vejamos agora a equação para o período } t+1 e_{t+1} = \\theta e_{t} - y_{t+1} \\\\ \\text{substituindo } e_{t} \\\\ e_{t+1} = \\theta (\\theta e_{t-1} - y_{t}) - y_{t+1} \\\\ e_{t+1} = \\theta \\theta e_{t-1} - \\theta y_{t} - y_{t+1} \\\\ \\end{align}\\] De maneira geral, temos que o erro em \\(t\\) possui correlação com o erro em \\(t-1\\) dada por \\(\\theta\\) e para \\(t-2\\) com \\(\\theta^2\\) e assim por diante para períodos maiores. Isso é o que configura a correlação serial de um processo \\(AR(1)\\). Vejamos um exemplo rápido no R. 14.7.2 AR1 Um outro modelo básico é um Processo Auto Regressivo de Primeira ordem, denotado de AR(1). \\[ y_t = \\alpha_0 + \\alpha_1 y_{t-1} + e_t \\] ## [1] 0.499343 Um modelo que combina um AR de ordem \\(p\\) com MA de ordem \\(q\\) é chamado de \\(ARMA(p,q)\\). 14.7.3 TSCS É raro que em ciência política analisamos um modelo de série de tempo sem preditores e para uma única unidade. Em geral, temos vários preditores e várias unidades. Esses dados são chamados em ciência política de Série de Tempo com Corte Tranversal, tipicamente abreviados em inglês para TSCS (Time Series Cross Section). Em economia esses dados são chamados de painel. Na epidemiologia, são dados longitudinais. A forma básica desses dados requer que tenhamos medidas das mesmas unidades \\(i\\) ao longo de vários períodos de tempo \\(t\\). 14.7.4 Modelo estático e dinâmico Esses modelos podem ser estáticos ou dinâmicos. Modelos estáticos incluem apenas relações contemporâneas (no mesmo período de tempo) entre variáveis. Esse tipo de modelo é apropriado quando um preditor ou múltiplos preditores produzem mudanças instantâneas (no período considerado). Uma equação típica seria: \\[ y_{it} = \\alpha + \\beta_1x_{1it} + \\beta_2x_{2it} + \\cdots + \\beta_px_{pit} + e_{it} \\] Já um modelo dinâmico captura relação entre os valores correntes e defasados (lagged) dos preditores e a variável resposta. Um exemplo simples com \\(m\\) termos defasados seria: \\[ y_{it} = \\alpha + \\beta_1x_{1it} + \\beta_2x_{2it-1} + \\cdots + \\beta_2x_{2it-m} e_{it} \\] Esses modelos trazem a complicação adicional de que temos de considerar possibilidade de termos um componente autoregresivo ou de média móvel na dimensão temporal, como vimos acima. E isso altera as condições para que um estimador não seja viesado. Não temos tempo de aprofundar isso aqui, mas eu quero pelo menos introduzir o principal tipo de modelo que é utilizado, que é o chamado modelo de efeitos fixos. 14.7.5 Efeitos Fixos Vamos supor que queremos medir o efeito do número efetivo de partidos sobre percentual de mulheres eleitas. Podemos esperar que quanto maior a fragmentação partidária, mais difícil mulheres serem eleitas. Um problema de uma regressão com fragmentação partidária e percentual de mulheres eleitas, do ponto de vista causal, é que podemos ter viés de seleção. Por exemplo, pode acontecer de estados que no passado tiveram mais mobilização feminista tenha menos partidos e também eleja mais mulheres. Vamos chamar essa variável de \\(U\\), para indicar que é não observada (unobserved, em inglês). \\[ y_{it} = \\alpha + \\beta x_{it} + u_i + e_{it} \\] Em que \\(x_t\\) é o número de partido (ou alguma medida de fragmentação partidária) em \\(t\\), e \\(u_i\\) é a presença ou não de mobilização feminista no passado (pré-período \\(t\\)). Considere que temos dois períodos, \\(1\\) e \\(2\\). Temos então uma equação de regressão para cada período. A ideia do modelo de efeitos fixos é simplesmente rodar uma regressão em que computamos a média de todas as variáveis do modelo e subtraímos essa média. Formalmente: \\[\\begin{align} \\text{Sejam } \\bar{y}_i = \\frac{y_{i1} + y_{i2}}{2}, \\bar{x}_i = \\frac{x_{i1} + x_{i2}}{2}, \\bar{u}_i = \\frac{u_{i} + u_{i}}{2} = u_i \\text{ e } \\bar{e}_i = \\frac{e_{i1} + e_{i2}}{2} \\text{ as médias no tempo para cada indivíduo de cada variável} \\\\ \\end{align}\\] Obtemos assim uma equação de regressão alternativa com a média: \\[ \\bar{y}_i = \\alpha + \\beta \\bar{x}_i + \\bar{u}_i + \\bar{e}_i \\] Agora, subtraindo da equação principal, temos: \\[\\begin{align} y_{it} - \\bar{y}_i = \\alpha - \\alpha + \\beta x_{it} - \\bar{x}_i + u_i -\\bar{u_i} + e_{it} - \\bar{e}_i &amp;= \\\\ \\tilde{y}_{it} = \\beta \\tilde{x}_{it} + \\tilde{e}_{it} \\end{align}\\] Uma formulação alternativa, com dois períodos, é dada pela primeira diferença, isto é, \\(y_{i, 2} - y_{i,1}\\): \\[\\begin{align} y_{i, 2} - y_{i,1} = \\alpha - \\alpha + \\beta x_{i,2} - \\beta x_{i,1} + u_i -u_i + e_{i,2} - e_{i,1} &amp;= \\\\ \\beta x_{i,2} - x_{i,1} + e_{i,2} - e_{i,1} \\end{align}\\] Isso significa que conseguimos eliminar a variável de confusão não observada que gerava viés de seleção, efetivamente controlando para ela. Para isso, precisamos subtrair a média de cada variável, ou, equivalentemente, computar a primeira diferença. Note que calculamos a média para cada indivíduo. Poderíamos ter também calculado a média para cada período de tempo. Nesse caso, teríamos o chamado efeito fixo de tempo. Quando usamos os dois efeitos fixos (de tempo e unidade), temos o chamado “two-way fixed effects”. E no caso de dois períodos é equivalente ao método conhecido como diferença-em-diferenças. O ponto fundamental desse método é que ele elimina confoundings de efeitos invariantes no tempo por unidade e efeitos comuns a todas as unidades no tempo (invariantes por unidade). 14.7.6 Erro como AR ou MA Os modelos de série de tempo anteriores consideram que o \\(y\\) segue um AR ou MA. Mas podemos pensar que o erro segue um AR ou MA. Quais as consequências? Tipicamente, o estimador continua não-viesado (se for exógeno aos erros em todos os períodos de tempo) e consistente, mas se torna ineficiente (o erro padrão está errado). Por exemplo, suponha que o modelo correto é: \\[ y_t = \\alpha + \\beta x_t + et, e_t = \\mu_t + \\theta \\mu_{t-1} \\] Se houver autocorrelação temporal no termo de erro, em geral é necessário calcular um erro padrão corrigido para autocorrelação. O mais usado, Newey-West entra na categoria HAC, Heterocedasticity-Autocorrelation-Consistent. Quando tenhos dados em formato TSCS \\(y_{it} = \\alpha + \\beta x_{it} + e{it}, e_{it} = \\mu_{it} + \\theta \\mu_{i,t-1}\\), a sugestão é usar outros erros padrão HAC, como Driscoll-Kraay e Panel-Corrected SE (de Beck–Katz). E soluções como correção de Prais–Winsten AR(1). Driscoll-Kraay é uma generalização do Newey-West para dados de painel, mas requer um T grande (idealmente mais que 30, mas já vi usando em T = 20). Inclui correlação entre unidade (além de no tempo). Correção de Prais–Winsten AR(1) envolve remover o termo de erro AR(1) da equação, em vez de ajustar o erro padrão. O procedimento é calcular a primeira diferença nas VDs e VIs. PCSE foi introduzido pelos cientistas políticos Beck e Katz. Desenvolvido para T grande e N pequeno (ex. países da OCDE por 50 anos). Modela correlação entre unidades, mas não serial (ou corrigida com Prais-Winsten). Artigos em journals top internacionais usam o erro padrão apropriado. Em revistas brasileiras, ainda é comum trabalhos usarem apenas erro padrão robusto (HC) em vez de HAC. Vamos rodar uma simulação e ver como fica: set.seed(1) library(lmtest) library(sandwich) library(parameters) # parâmetros R &lt;- 5000 N &lt;- 500 cover_c &lt;- numeric() cover_hac &lt;- numeric() cover_hc &lt;- numeric() beta_true &lt;- 0.5 for (i in 1:R) { eps &lt;- arima.sim(n = N, model = list(ma = 0.8)) X &lt;- arima.sim(n = N, model = list(ar = 0.5)) Y &lt;- beta_true* X + eps fit &lt;- lm(Y ~ X) se_c &lt;- summary(fit)$coef[2,2] b1 &lt;- coef(fit)[2] ci_c &lt;- b1 + c(-1,1) * qt(0.975, df = N - 2) * se_c cover_c[i] &lt;- (beta_true &gt;= ci_c[1] &amp;&amp; beta_true &lt;= ci_c[2]) results &lt;- model_parameters(fit, vcov = &quot;HC3&quot;) se_hc &lt;- results$SE[2] ci_hc &lt;- b1 + c(-1,1) * qt(0.975, df = N - 2) * se_hc cover_hc[i] &lt;- (beta_true &gt;= ci_hc[1] &amp;&amp; beta_true &lt;= ci_hc[2]) Vh &lt;- NeweyWest(fit, lag = 3, prewhite = FALSE, adjust = TRUE) se_h &lt;- sqrt(diag(Vh))[2] ci_h &lt;- b1 + c(-1,1) * qt(0.975, df = N - 2) * se_h cover_hac[i] &lt;- (beta_true &gt;= ci_h[1] &amp;&amp; beta_true &lt;= ci_h[2]) } mean(cover_c) ## [1] 0.8876 mean(cover_hc) ## [1] 0.8876 mean(cover_hac) ## [1] 0.9292 Referência útil sobre erros padrão em um contexto de dados de painel (caso mais comum): https://cran.r-project.org/web/packages/fixest/vignettes/standard_errors.html 14.8 Dados Espaciais Há dois tipos de dados espaciais diferentes que se traduzem em visualizações distintas. Em primeiro lugar, temos dados espaciais em pontos, que podem ser plotados como conjuntos de pontos em um mapa. Designam localizações específicas, como cidades ou quaisquer objetos discretos definidos em um espaço. E em segundo lugar, temos dados espaciais de polígonos, que representam uma sequência de pontos conectados espacialmente (em geral fronteiras) que formam um área. Dados em formato de polígonos usualmente são distribuídos na forma de shapefiles. 14.8.1 Regressão espacial Quando falamos de regressão espacial, tipicamente estamos tratando de dados espaciais em pontos, isto é, unidades que se localizam no espaço, como cidades, bairros ou distritos. E de maneira análoga ao que tratamos em série de tempo de que os dados podem ter autocorrelação temporal, também podem ter autocorrelação espacial. Só que, ao contrário do tempo, em que a direção de causalidade vai sempre do passado para o futuro, no caso espacial, unidades vizinhas podem causar umas as outras, de modo que a modelagem é mais complexa, o que impede-nos de aprofundar aqui no curso esse tópico. De todo modo, vale a pena apresentar dois modelos básicos: 14.8.1.1 Spatial lag (Outcome) model O modelo espacial defasado tipicamente ocorre quando a variável resposta \\(y\\) com aspecto espacial é incluída como preditor. Isso acontece quando queremos modelar algum processo de difusão ou contágio da variável resposta. Um exemplo comum em Relações Internacionais é o de estudos sobre o efeito de assinar tratados de proteção a investimento sobre o volume de investimento estrangeiro direto (IED). Se o país vizinho recebe mais investimento (por exemplo OpenAi decide construir um data-center na Argentina) isso impacta o volume de investimentos no Brasil e vice-versa. 14.8.1.2 Spatially lagged (predictor) variable Aqui, estamos interessados na difusão ou spillover do tratamento. Considerando o exemplo acima, se um país vizinho assina um tratado de proteção a investimento isso impacta não apenas a atração de investimento no país, mas também no vizinho. 14.8.1.3 Spatial error model Por fim, também podemos concluir que variáveis não observadas podem induzir difusão ou contágio, de modo que o termo de erro terá um cumponente espacial. A especificação da regressão para cada tipo de modelo (ou uma combinação de um ou mais tipos pode ser feita) é dada por: Resposta defasada espacialmente. \\[ y_{i} = \\alpha + \\rho Wy_{-i} + \\beta x_i + e_i \\] Preditor defasado espacialmente. \\[ y_{i} = \\alpha + \\rho Wx_{-i} + \\beta x_i + e_i \\] 3. Termo de erro defasado espacialmente. \\[\\begin{align} y_{i} = \\alpha + \\beta x_i + e_i \\\\ e_i = \\lambda We_{-i} + \\epsilon_i \\end{align}\\] Em todos os casos, \\(W\\) é a matriz de pesos espaciais. Se apenas vizinhos diretos podem influenciar uns aos outros, então ela terá peso zero para países que não são vizinhos. Tipicamente este é o caso, pois o número de parâmetros aumenta muito se todos os países puderem influenciar todos os países e pode acabar sendo maior do que o número de observações. Por fim, é possível combinar aspectos temporais e espaciais em uma única análise. Na prática isso é raro, pois requer conhecimentos especializados de duas sub-áreas distintas. No geral a maioria dos trabalhos em ciência política e Relações Internacionais ignora as correlações espaciais. "],["teoremas.html", "Capítulo 15 - Teoremas 15.1 Independência e Correlação 15.2 Propriedade de estimadores 15.3 Exercícios para o leitor 15.4 Lei das Esperanças Iteradas 15.5 Teorema do Condicionamento", " Capítulo 15 - Teoremas Apresentamos aqui alguns teoremas importantes para um aprofundamento em Regressão e Estatística 15.1 Independência e Correlação Sejam \\(X\\) e \\(Z\\) duas variáveis aleatórias, com médias \\(\\mu_x\\) e \\(\\mu_z\\) respectivamente. Definição 15.1 (independência): Dizemos que \\(X\\) e \\(Z\\) são independentes, denotado por \\(X \\perp Z\\), se \\(Z\\) não traz nenhuma informação sobre \\(X\\) e vice-versa. Formalmente, isso quer dizer que: \\(f(x,z) = f_x(x)f_z(x)\\), isto é, a função densidade conjunta é o produto das marginais. Proposição 15.1 (independência na média): Se \\(X\\) e \\(Z\\) são independentes, então são independentes na média (o reverso não é verdade). Prova: \\(\\mathbb{E}[X|Z] = \\sum xP_{x|z}(x) = \\sum x \\frac{P_x(x)P_Z(z)}{P_Z(z)} = \\sum x P_x(x) = \\mu_x\\) Da segunda igualdade para a terceira usamos a definição de probabilidade condicional. Definição 15.1.2 (independência linear): Dizemos que \\(X\\) e \\(Z\\) são não-correlacionados ou linearmente independentes se \\(\\mathbb{E}[XZ] = \\mu_x\\mu_z \\iff cov(X,Z) = 0\\) Proposição 15.1.2 (independência na média para independência linear): Se \\(X\\) e \\(Y\\) são independentes na média, então são linearmente independentes. O reverso não é verdade. \\[\\begin{aligned} \\mathbb{E}[XZ] &amp;= \\mathbb{E}\\!\\big[\\mathbb{E}[XZ \\mid Z]\\big] \\quad \\text{(lei das esperanças iteradas)}\\\\ &amp;= \\mathbb{E}\\!\\big[ Z\\,\\mathbb{E}[X \\mid Z] \\big] \\quad \\text{(condicionado em $Z$, ele é constante)}\\\\ &amp;= \\mathbb{E}\\!\\big[ Z\\,\\mathbb{E}[X] \\big] \\quad \\text{(independência na média)}\\\\ &amp;= \\mathbb{E}[X]\\,\\mathbb{E}[Z] \\quad \\text{(constante sai da esperança)}\\\\ &amp;= \\mu_X \\mu_Z, \\end{aligned}\\] logo \\(\\mathrm{cov}(X,Z)=\\mathbb{E}[XZ]-\\mu_X\\mu_Z=0\\). Observação: O reverso não vale: por exemplo, se \\(Z\\) é uniforme em \\((-1,1)\\) e \\(X = Z^2\\), então \\(\\mathrm{cov}(X,Z) = 0\\), mas \\(\\mathbb{E}[X \\mid Z] \\neq \\mathbb{E}[X]\\). Para ver que \\(\\mathrm{cov}(X,Z) = 0\\), considere: \\[\\begin{aligned} \\mathbb{E}[XZ]-\\mu_X\\mu_Z &amp;= \\mathbb{E}[XZ]-\\mu_X 0 &amp;= \\mathbb{E}[XZ] &amp;= \\mathbb{E}[Z^3] = 0 \\end{aligned}\\] 15.2 Propriedade de estimadores 15.2.1 Viés Definição 15.2 (viés). Seja \\(\\theta_0\\) o verdadeiro valor de um parâmetro de uma variável aleatória e seja \\(\\hat{\\theta}\\) um um estimador de \\(\\theta_0\\). O viés de um estimador é o desvio absoluto entre o verdadeiro valor do parâmetro e a esperança do estimador. \\(|\\theta_0 - \\mathbb{E}[\\hat{\\theta}]|\\). Dizemos que um estimador é não-viesado se e somente se o viés é zero. Lembrem-se que se e somente se é uma relação de bi-implicação, ou seja, se é não viesado, o viés é zero. Se o viés é zero, é não viesado. Observação: o estimador é que pode ser viesado ou não. Isso não se aplica a uma estimativa. Incorremos em erro quando dizemos que a estimativa é viesada (ou não viesada). O que é viesado ou não é o estimador. Similarmente, não cabe falar em amostra viesada ou não-viesada. 15.2.2 Variância Definição 15.3. (variância do estimador). Seja \\(\\theta_0\\) o verdadeiro valor de um parâmetro de uma variável aleatória e seja \\(\\hat{\\theta}\\) um um estimador de \\(\\theta_0\\). A variância de um estimador é o valor esperado do quadrado dos desvios amostrais. \\(Var[\\hat{\\theta}] = \\mathbb{E}[\\left(\\hat{\\theta} - \\mathbb{E}[\\hat{\\theta}] \\right)^2]\\) Proposição 15.3. Seja \\(Z_1, Z_2, ...\\) uma sequência de variáveis aleatórias i.i.d tal que, para todo \\(i\\), \\(\\mathbb{E}[Z_i] = \\mu\\) e \\(Var[Z_i] = \\sigma^2\\). Seja \\(X_n\\) a média amostral das primeiras \\(n\\) variáveis. A variância amostral é igual à variância de \\(Z\\) dividida por \\(n\\). \\(Var[X_n] = \\frac{Var[Z]}{n} = \\frac{\\sigma^2}{n}\\) 15.2.3 Eficiência Entre um número de estimadores da mesma classe (por exemplo, todos são não-viesados e têm variância finita), o estimador com a menor variância é chamado de estimador (relativamente mais) eficiente. 15.2.4 Consistência Definição: Um estimador \\(\\hat\\theta_n\\) é consistente para \\(\\theta\\) se \\(\\hat\\theta_n \\xrightarrow{p} \\theta\\), i.e., \\(\\forall \\varepsilon&gt;0,\\ \\lim_{n\\to\\infty}\\Pr(|\\hat\\theta_n-\\theta|&gt;\\varepsilon)=0\\). Proposição: Condição suficiente via Chebyshev Se \\(\\hat\\theta_n\\) é não-viesado e \\(\\mathrm{Var}(\\hat\\theta_n)\\to 0\\) quando \\(n\\to\\infty\\), então \\(\\hat\\theta_n\\) é consistente. Pela desigualdade de Chebyshev, \\(\\Pr(|\\hat\\theta_n-\\theta|&gt;\\varepsilon)\\le \\mathrm{Var}(\\hat\\theta_n)/\\varepsilon^2 \\to 0\\). 15.3 Exercícios para o leitor Suponha que \\(Z\\) é uma distribuição uniforme \\(Z \\sim U(-1,1)\\). Mostre que se \\(X = Z^2\\), então são independentes na média, mas não independentes entre si. Mostre que \\(X\\) e \\(Z\\) são independentes na média, mas não independentes entre si no seguinte caso: Seja \\(Y\\) uma variável aleatória uniforme \\(Y \\sim U(-1,1)\\). Defina \\(X = Y+ \\epsilon_1\\) e \\(Z = Y+ \\epsilon_2\\), em que \\(\\epsilon_1\\) e \\(\\epsilon_2\\) são ruídos aleatórios com média zero, independentes entre si, \\(X\\), \\(Z\\) e \\(Y\\). Dica: em algum momento, use a definição (15.2) de que se \\(Cov(X,Z) \\neq 0\\), então \\(X\\) e \\(Z\\) não são independentes. Mostre que a média amostral é um estimador não-viesado para a média populacional. 15.4 Lei das Esperanças Iteradas Teorema: Lei simples das Esperanças Iteradas Se \\(\\mathbb{E}[Y] &lt; \\infty\\) então para qualquer vetor de variáveis aleatórias \\(X\\), \\(\\mathbb{E}[\\mathbb{E}[Y|X]] = \\mathbb{E}[Y]\\). Prova: Para \\(X\\) discreto: Como a esperença condicional é uma função de \\(X\\) apenas, a esperança é calculada somando apenas com respeito a \\(X\\). \\[\\begin{align*} 1.\\mathbb{E}[\\mathbb{E}[Y|X]] &amp;= \\sum_{j=1}^{\\infty}\\mathbb{E}[Y|X=x_j]P[X=x_j] \\\\ 2. &amp;= \\sum_{j=1}^{\\infty}\\left(\\sum_{i=1}^{\\infty} y_i P[Y=y_i|X=x_j]\\right)P[X=x_j] \\\\ 2.5. &amp;= \\sum_{j=1}^{\\infty}y_1 P[Y=y_1|X=x_j]P[X=x_j] + y_2 P[Y=y_2|X=x_j]) P[X=x_j] + \\cdots \\\\ 3. &amp;= \\sum_{j=1}^{\\infty}\\sum_{i=1}^{\\infty} y_i P[Y=y_i|X=x_j]P[X=x_j] \\\\ 4. &amp;= \\sum_{j=1}^{\\infty}\\sum_{i=1}^{\\infty} y_i P[Y=y_i, X=x_j] \\\\ 5. \\ &amp;= \\sum_{i=1}^{\\infty} y_i \\color{red}{\\sum_{j=1}^{\\infty}} P[Y=y_i, X=x_j] \\quad \\text{(Troca na ordem dos somatórios)} \\\\ 6. &amp;= \\sum_{i=1}^{\\infty} y_i P[Y=y_i] \\\\ 7. &amp;= \\mathbb{E}[Y] \\end{align*}\\] Teorema: Lei das Esperanças Iteradas: Se \\(\\mathbb{E}[Y] &lt; \\infty\\) então para quaisquer vetores de variáveis aleatórias \\(X_1\\) e \\(X_2\\), \\(\\mathbb{E}[\\mathbb{E}[Y|X_1,X_2]|X_1] = \\mathbb{E}[Y|X_1]\\). A esperança interna condiciona em \\(X_1\\) e \\(X_2\\), e a externa apenas em \\(X_1\\). E o resultado é o condiionamento apenas em \\(X_1\\). Uma forma de memorizar o teorema é dizer que “o menor conjunto de informações ganha”. 15.5 Teorema do Condicionamento Propisição: Se \\(\\mathbb{E}[Y] &lt; \\infty\\), então \\(\\mathbb{E}[g(x)Y|X] = g(x) \\mathbb{E}[Y|X]\\). Se, além disso, \\(\\mathbb{E}[g(x)] &lt; \\infty\\), então \\(\\mathbb{E}[g(x)Y] = \\mathbb{E}[g(x) \\mathbb{E}[Y|X]]\\) Prova 1: \\(\\mathbb{E}[g(x)Y|X] = \\sum_{y} g(x) y P(Y=y|X=x) = g(x) \\sum_{y} y P(Y=y|X=x) = g(x)\\mathbb{E}[Y|X]\\). Como estamos somando em \\(Y\\), \\(g(x)\\) age como uma constante com respeito à somatória de \\(Y\\). Depois é só aplicar a definição de esperança condicional. Prova 2. Basta aplicar a lei das esperanças iteradas e a prova acima: \\(\\mathbb{E}[g(x)Y] = \\mathbb{E}[\\mathbb{E}[g(x)Y|X]] = \\mathbb{E}[g(x)\\mathbb{E}[Y|X]]\\) "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
