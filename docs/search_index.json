[["index.html", "Introdução à Regresssão para Ciências Sociais Capítulo 1 - Prefácio", " Introdução à Regresssão para Ciências Sociais Manoel Galdino 2023-08-09 Capítulo 1 - Prefácio Esse livro é, por enquanto, apenas uma repositório de notas de aula do curso de graduação da Ciências Sociais da USP, FLP0468 Métodos Quantitativos de Pesquisa na Ciência Política IV, de introdução à regressão, bem como o curso de pós-graduação, Métodos II. Agradeço aos alunos do curso pelos feedbacks (futuros e presentes) sobre esse material, bem como ao monitor do curso, Davi Veronese. Futuramente, pretendo transformar as notas de aulas em um livro, que poderá ser utilizado pela comunidade brasileira de ciências sociais interessada em aprender mais sobre métodos quantitativos, em particular regressão com R. A motivação para disponibilizar as notas de aula em formato de livro é a quase inexistência de um material bom de econometria básica, em português, voltado para a área de ciências sociais, em particular a ciência política. Existem bons manuais em inglês e bons manuais em português, mas voltado para a economia. Os livros existentes nem sempre fazem um bom trabalho de diferenciar regressão estatística e modelo estrutural (causal) de regressão. Uma abordagem moderna de inferência causal requer que essa distinção seja ensinada ao aluno e esse livro também pretende preencher essa lacuna. Por fim, a utilização do R é uma forma de dar ênfase à parte prática. Embora a teoria seja importante e a boa compreensão dos fundamentos é que permita o aprofundamento dos temas cobertos no livro, estatística é uma disciplina aplicada, cujo maior valor está na sua aplicação prática. Assim, boa parte do curso e do conteúdo do livro é dedicado a implementar a teoria no R, bem como na interpretação dos dados. O livro começa com capítulos iniciais de introdução/revisão do R e de estatística básica (incluindo probabilidade). Assim, pressupomos que um estudante que utilize esse curso tenha conhecimentos de estatística e probabilidade básica. Não é necessário conhecimento previo de R, nem de cálculo ou álgebra linerar. "],["revisão-de-r.html", "Capítulo 2 - Revisão de R 2.1 R e Rstudio 2.2 R como calculadora 2.3 Objetos no R 2.4 Tipos de objetos 2.5 Armazenando dados 2.6 Bibliotecas/pacotes 2.7 importando dados 2.8 Data wrangling 2.9 Visualização", " Capítulo 2 - Revisão de R 2.1 R e Rstudio O R é uma linguagem de programação voltada para análise de dados. O Rstudio é uma IDE (interface de desenvolvimento), que nos ajuda a programar em R. No curso utilizaremos o Rstudio para facilitar programar em R. Normalmente, iremos escrever um comando aqui no Script, clicar em executar (run) ou apertar ctrl + enter, e o Rstudio vai copiar o comando, colar no console e executá-los para nós. 2.2 R como calculadora o R pode funcionar como calculadora. 2+2 [1] 4 3*4 [1] 12 10/2 [1] 5 2.3 Objetos no R Tudo no R é um objeto. Isso significa que um número é um objeto. pi [1] 3.141593 Isso significa que funções (comandos) também são objetos sum(c(1,2,3)) [1] 6 sum function (…, na.rm = FALSE) .Primitive(“sum”) E nós podemos criar nossos próprios objetos, dando os nomes que quisermos (exceto se já existe um objeto no R com aquele nome, como por exemplo o objeto “sum”). x &lt;- 3 y &lt;- 7 x+y [1] 10 2.4 Tipos de objetos O R tem muitos tipos de objetos. Vamos listar aqui apenas os mais básicos. 2.4.1 Numeric Objetos do tipo numeric são … números (“reais”). # Exemplos pi [1] 3.141593 1/3 [1] 0.3333333 4 [1] 4 2.4.2 character Objetos do tipo character são do tipo texto. Sempre são escritos entre aspas (simples ou duplas, tanto faz) # Exemplos &quot;Manoel Galdino&quot; [1] “Manoel Galdino” &#39;abc&#39; [1] “abc” &quot;7&quot; [1] “7” 2.5 Armazenando dados Para armazenar dados, usualmente teremos 4 tipos de objetos: 1. vetor, 2. Matriz. 3. data.frame, 4. lista. Não vou falar de lista agora (nem de array, que é uma generalização da matriz para mais de duas dimensões). 2.5.1 Vetor Um vetor é uma sequência de objetos. # Exemplos c(1,2,3) [1] 1 2 3 1:3 [1] 1 2 3 c(&quot;Manoel&quot;, &quot;Hugo&quot;, &quot;Lia&quot;, &quot;Juliana&quot;, &quot;Jéssica&quot;) [1] “Manoel” “Hugo” “Lia” “Juliana” “Jéssica” c(c(1,2,3), c(2,3,1)) [1] 1 2 3 2 3 1 os elementos de um vetor devem ser todos do mesmo tipo: # Exemplos x &lt;- c(&quot;1&quot;, 1) x[2] [1] “1” # sedundo elemento fica armazenado como character # não é possível somar texto # x[2] + x[2] # erro 2.5.2 Matriz Uma matriz são vetores organizados por coluna, todas as colunas (vetores) só podem ser de um tipo, ou seja, não posso ter uma coluna numeric e outra de character, por exemplo. # Exemplos matrix(1:6, nrow=3, ncol=2) [,1] [,2] [1,] 1 4 [2,] 2 5 [3,] 3 6 2.5.3 Data Frame O data.frame é uma tabela/planilha, e é onde normalmente armazenamos nossos bancos de dados no R. # Exemplos data.frame(x=1:3, y=c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;)) x y 1 1 a 2 2 b 3 3 c 2.5.3.1 datas ## explicando data rapidamente data_ex &lt;- &quot;2020-10-23&quot; data_ex1 &lt;- as.Date(data_ex) data_ex1 + 0:9 [1] “2020-10-23” “2020-10-24” “2020-10-25” “2020-10-26” “2020-10-27” “2020-10-28” “2020-10-29” “2020-10-30” “2020-10-31” [10] “2020-11-01” ## fim da explicação rápida de data library(knitr) library(kableExtra) ## criando data de forma repetitiva e tediosa. O que queremos evitar! minha_data &lt;- c(as.Date(&#39;2009-01-01&#39;), as.Date(&#39;2009-01-02&#39;), as.Date(&#39;2009-01-03&#39;), as.Date(&#39;2009-01-04&#39;), as.Date(&#39;2009-01-05&#39;), as.Date(&#39;2009-01-06&#39;), as.Date(&#39;2009-01-07&#39;), as.Date(&#39;2009-01-08&#39;), as.Date(&#39;2009-01-09&#39;), as.Date(&#39;2009-01-10&#39;)) acoes &lt;- data.frame( tempo = minha_data, X = rnorm(10, 0, 1), Y = rnorm(10, 0, 2), Z = rnorm(10, 0, 4) ) kable(acoes) tempo X Y Z 2009-01-01 -0.5942169 -2.5462534 -5.8756087 2009-01-02 2.2381340 1.3045714 -6.3549874 2009-01-03 0.6212960 0.3913685 -0.0843070 2009-01-04 -2.0664466 0.5990876 -0.3469913 2009-01-05 -0.8950288 -0.0482699 -6.0501447 2009-01-06 -0.9732650 -1.0547529 -4.4589942 2009-01-07 0.3799902 -2.8811420 1.8220280 2009-01-08 1.0445470 -0.4754305 12.3860064 2009-01-09 0.0214847 -1.2360396 1.2510656 2009-01-10 1.1353331 2.2757195 2.8245165 # criando data.frame de maneira mais inteligente acoes &lt;- data.frame( tempo = as.Date(&#39;2009-01-01&#39;) + 0:9, X = rnorm(10, 0, 1), Y = rnorm(10, 0, 2), Z = rnorm(10, 0, 4) ) kable(acoes) tempo X Y Z 2009-01-01 -1.3956945 -1.5691115 2.5593133 2009-01-02 -0.1338684 1.5393867 -4.8258934 2009-01-03 0.2753938 0.9741069 4.1496071 2009-01-04 -1.5046480 -3.8632062 -8.5287340 2009-01-05 0.6459368 -1.4767597 -0.8104898 2009-01-06 -0.3713497 -0.2747539 1.1455279 2009-01-07 -1.0837592 0.7822565 -3.5126754 2009-01-08 -0.5593102 -1.0192248 3.6812021 2009-01-09 0.8856205 0.0341255 5.0712964 2009-01-10 1.4783638 0.2114132 -2.6515043 2.6 Bibliotecas/pacotes O R permite que a gente importe comandos que não vêm por padrão no R. Em gerla esses comandos es~toa agrupados sob um pacote. PAra usar esses comandos, primeiro a gente instala o pacote, e depois carrega a biblioteca. # Exemplos #install.packages(&quot;data.table&quot;) #library(data.table) 2.7 importando dados Para importar dados, vamos usar a bilioteca “data.table” Então, instalem ela se ainda não instalaram (usando o comando install.packages(“data.table”)) E depois carreguem a biblioteca: library(data.table) Para importar, usaremos o comando fread do pacote data.table. # vamos carregar base de dados do ibge # install.packages(&quot;devtools&quot;) # devtools::install_github(&quot;tbrugz/ribge&quot;) library(ribge) # para descobrir quais funções existem nesse pacote, podemos usar: ??ribge::ribge # vamos usar uma que retorna o pib municipal # pib_cid &lt;- pib_municipios(2013) pib_cid &lt;- readRDS(&quot;dados/pib_cid.RDS&quot;) # para visualizar os dados que forma importados, temos várias funções # glimpse, head e View library(dplyr) # para glimpse # glimpse(pib_cid) # head(pib_cid) # View(pib_cid) 2.8 Data wrangling Para manipulação, limpeza e processamento de dados, iremos utilizar o chamado “tidyverse”. library(tidyverse) # digamos que quero o pib total médio e o pib per capita médio # basta usar o comando summarise, que resume os dados e escolher a função mean. df &lt;- pib_cid %&gt;% summarise(pib_medio = mean(pib_total), pib_per_capita_medio = mean(pib_per_capita)) kable(df) pib_medio pib_per_capita_medio 957202.7 17388.86 # se eu quiser a soma dos pibs municipais df &lt;- pib_cid %&gt;% summarise(soma_pib = sum(pib_total)) %&gt;% head() kable(df) soma_pib 5331618957 # maior e menos pibs e pibs per capita entre municípios df &lt;- pib_cid %&gt;% summarise(pib_max = max(pib_total), pib_min = min(pib_total), pib_per_capita_max = max(pib_per_capita), pib_per_capita_min = min(pib_per_capita)) %&gt;% head() kable(df) pib_max pib_min pib_per_capita_max pib_per_capita_min 582079726 4198.94 717343.7 301.6 # se eu quiser apenas dos municípios od estado de SP? # basta filtrar pelo estado de SP, com o comando filter df &lt;- pib_cid %&gt;% filter(sigla_uf == &quot;SP&quot;) %&gt;% summarise(soma_pib = sum(pib_total)) %&gt;% head() kable(df) soma_pib 1715238417 df &lt;- pib_cid %&gt;% filter(sigla_uf == &quot;SP&quot;) %&gt;% summarise(pib_medio = mean(pib_total), pib_per_capita_medio = mean(pib_per_capita)) %&gt;% head() kable(df) pib_medio pib_per_capita_medio 2659284 24827.14 # se eu quiser esse cálculo por uf (or cada umas das ufs?) # Aí é melhor aguprar por uf # ideia é: split by, apply (function), combine (summarise?) df &lt;- pib_cid %&gt;% group_by(sigla_uf) %&gt;% summarise(pib_medio = mean(pib_total), pib_per_capita_medio = mean(pib_per_capita)) %&gt;% head() kable(df) sigla_uf pib_medio pib_per_capita_medio AC 521542.3 11450.979 AL 365515.0 7931.148 AM 1339536.0 8975.314 AP 797717.9 14947.829 BA 491233.3 8814.841 CE 592590.0 7157.360 # agora, quero criar uma nova variável, que é o pib estadual df &lt;- pib_cid %&gt;% group_by(sigla_uf) %&gt;% mutate(pib_uf = sum(pib_total)) %&gt;% head() kable(df) ano codigo_regiao nome_regiao codigo_uf sigla_uf nome_uf cod_municipio nome_munic nome_metro codigo_meso nome_meso codigo_micro nome_micro codigo_reg_geo_imediata nome_reg_geo_imediata mun_reg_geo_imediata codigo_reg_geo_intermediaria nome_reg_geo_intermediaria mun_reg_geo_intermediaria codigo_concentracao_urbana nome_concentracao_urbana tipo_concentracao_urbana codigo_arranjo_populacional nome_arranjo_populacional hierarquia_urbana hierarquia_urbana_principais codigo_regiao_rural nome_regiao_rural regiao_rural_classificacao amazonia_legal semiarido cidade_de_sao_paulo vab_agropecuaria vab_industria vab_servicos_exclusivo vab_adm_publica vab_total impostos pib_total pib_per_capita atividade_vab1 atividade_vab2 atividade_vab3 pib_uf 2013 1 Norte 11 RO Rondônia 1100015 Alta Floresta D’Oeste NA 1102 Leste Rondoniense 11006 Cacoal 110005 Cacoal do Entorno 1102 Ji-Paraná do Entorno NA NA NA NA NA Centro Local Centro Local 1101 Região Rural da Capital Regional de Porto Velho Região Rural de Capital Regional Sim Não Não 110850.84 20336.994 73025.07 120335.59 324548.50 16776.193 341324.69 13266.66 Administração, defesa, educação e saúde públicas e seguridade social Pecuária, inclusive apoio à pecuária Demais serviços 31121413 2013 1 Norte 11 RO Rondônia 1100023 Ariquemes NA 1102 Leste Rondoniense 11003 Ariquemes 110002 Ariquemes Polo 1101 Porto Velho do Entorno NA NA NA NA NA Centro Sub-regional B Centro Sub-regional 1101 Região Rural da Capital Regional de Porto Velho Região Rural de Capital Regional Sim Não Não 93249.75 354733.212 694832.17 466732.80 1609547.93 190304.571 1799852.51 17772.99 Administração, defesa, educação e saúde públicas e seguridade social Demais serviços Comércio e reparação de veículos automotores e motocicletas 31121413 2013 1 Norte 11 RO Rondônia 1100031 Cabixi NA 1102 Leste Rondoniense 11008 Colorado do Oeste 110006 Vilhena do Entorno 1102 Ji-Paraná do Entorno NA NA NA NA NA Centro Local Centro Local 1101 Região Rural da Capital Regional de Porto Velho Região Rural de Capital Regional Sim Não Não 38259.43 3412.205 17787.45 32838.64 92297.72 4066.819 96364.54 14836.73 Administração, defesa, educação e saúde públicas e seguridade social Pecuária, inclusive apoio à pecuária Agricultura, inclusive apoio à agricultura e a pós colheita 31121413 2013 1 Norte 11 RO Rondônia 1100049 Cacoal NA 1102 Leste Rondoniense 11006 Cacoal 110005 Cacoal Polo 1102 Ji-Paraná do Entorno NA NA NA NA NA Centro Sub-regional B Centro Sub-regional 5105 Região Rural do Centro Sub-regional de Vilhena (RO) e Cacoal (RO) Região Rural de Centro Sub-regional Sim Não Não 140658.88 140288.317 599519.83 395842.23 1276309.26 156944.241 1433253.51 16692.33 Administração, defesa, educação e saúde públicas e seguridade social Demais serviços Comércio e reparação de veículos automotores e motocicletas 31121413 2013 1 Norte 11 RO Rondônia 1100056 Cerejeiras NA 1102 Leste Rondoniense 11008 Colorado do Oeste 110006 Vilhena do Entorno 1102 Ji-Paraná do Entorno NA NA NA NA NA Centro de Zona B Centro de Zona 1101 Região Rural da Capital Regional de Porto Velho Região Rural de Capital Regional Sim Não Não 45153.64 19889.818 149569.44 83089.55 297702.45 55567.232 353269.68 19581.49 Administração, defesa, educação e saúde públicas e seguridade social Comércio e reparação de veículos automotores e motocicletas Demais serviços 31121413 2013 1 Norte 11 RO Rondônia 1100064 Colorado do Oeste NA 1102 Leste Rondoniense 11008 Colorado do Oeste 110006 Vilhena do Entorno 1102 Ji-Paraná do Entorno NA NA NA NA NA Centro Local Centro Local 1101 Região Rural da Capital Regional de Porto Velho Região Rural de Capital Regional Sim Não Não 51029.59 23179.131 66099.41 86090.50 226398.63 16368.611 242767.24 12650.72 Administração, defesa, educação e saúde públicas e seguridade social Demais serviços Pecuária, inclusive apoio à pecuária 31121413 Coisas estranhas. O maior pib per capita municipal deu muito alto. vamos ver qual município é? Vamos filtrar e depois selecionar apenas algumas colunas # agora, quero criar uma nova variável, que é o pib estadual df &lt;- pib_cid %&gt;% filter(pib_per_capita &gt; 700000) %&gt;% select(sigla_uf, nome_munic, pib_per_capita, pib_total) %&gt;% head() kable(df) sigla_uf nome_munic pib_per_capita pib_total ES Presidente Kennedy 717343.7 7984035 Vamos entrar na Wiki do município ou perguntar pra chatGPT o que explica isso aí? Veremos que “faz sentido”, embora na verdade não faça. Exercício em sala de aula: veja os impostos desse município. 2.9 Visualização Para visualizarmos os dados com gráficos, utilizaremos a biblioteca ggplot2 # gráficos library(ggplot2) pib_cid %&gt;% ggplot(aes(y=pib_total, x=impostos)) + geom_point() A lgócia geral de um grtáfico com ggplot2 é como no exemplo acima. Primeiro passamos as variáveis por meio do comando ggplot, dentro de aes (de aesthetics), depois combinamos com o tipo de plot que queremos faze,r nesse caso, pontos, com geom_point. É possívle customizar o gráfico para ele ficar mais bonito. Vamos fazer isso agora. # gráficos mais bonitos pib_cid %&gt;% ggplot(aes(y=pib_total, x=impostos)) + geom_point() + scale_y_continuous(labels = scales::dollar) + theme_light() + xlab(&quot;impostos municipais&quot;) + ggtitle(&quot;PIB municipal de 2013 x impostos municipais&quot;) Podemos usar vários temas feitos pela comunidade. Por exemplo, Barbie: # gráficos mais bonitos # install.packages(&quot;remotes&quot;) #remotes::install_github(&quot;MatthewBJane/theme_park&quot;) library(ThemePark) pib_cid %&gt;% ggplot(aes(y=pib_total, x=impostos)) + geom_point() + scale_y_continuous(labels = scales::dollar) + theme_barbie() + xlab(&quot;impostos municipais&quot;) + ggtitle(&quot;PIB municipal de 2013 x impostos municipais&quot;) Vocês podem ver outros temas de filmes no github do autor do pacote: https://github.com/MatthewBJane/theme_park E, claro, há muito mais na internet. Para fazer outro tipo de gráfico, é só variar o geom. Por exemplo, um histograma do PIB per capita. # Histograma pib_cid %&gt;% ggplot(aes(y=pib_per_capita)) + geom_histogram() + theme_light() + ggtitle(&quot;PIB per capita municipal&quot;) "],["revisão-de-estatística-e-probabilidade.html", "Capítulo 3 - Revisão de estatística e probabilidade 3.1 Variável Aleatória 3.2 Esperança matemática 3.3 Variância 3.4 Algebra com Esperança, Variância e Covariância 3.5 Distribuição de Probabilidade Conjunta", " Capítulo 3 - Revisão de estatística e probabilidade A média de um conjunto de valores é dada pela soma dos valores, dividdo pelo número de observações. Matematicamente: \\(media = \\sum_{i=1}^n{x_i}/n\\) para observações \\(\\{ x_1, x_2, x_3, ..., x_n \\}\\) Em geral, as observações são uma amostra, e falamos de média amostral, \\(\\overline x\\). Ou seja: \\(\\overline x = \\sum_{i=1}^n{x_i}/n\\) Exercício 1. Vamos calcular, no R, a média das seguintes amostras: \\(\\{1,2,3,4,5,6,7,8,9,10\\}\\) \\(\\{5,5,5,5,5,5,5\\}\\) \\(\\{1,3,5,7,9,11\\}\\) \\(\\{-5,-4,-3,-2,-1,1,2,3,4,5\\}\\) Código no R x &lt;- c(1,2,3,4,5,6,7,8,9,10) (media_x &lt;- sum(x)/length(x)) ## [1] 5.5 x &lt;- c(5,5,5,5,5,5,5) (media_x &lt;- sum(x)/length(x)) ## [1] 5 x &lt;- c(1,3,5,7,9,11) (media_x &lt;- sum(x)/length(x)) ## [1] 6 x &lt;- c(-5,-4,-3,-2,-1,1,2,3,4,5) (media_x &lt;- sum(x)/length(x)) ## [1] 0 # ou podemos simplemsnte usar mean(x) mean(x) ## [1] 0 3.1 Variável Aleatória Uma variável aleatória (v.a.) é uma função que mapeia eventos (aleatórios) de um espaço amostral aos números reais, em geral uma probabilidade entre \\(0\\) e \\(1\\). Por exemplo, um dado de \\(6\\) faces possui um espaço amostral de eventos possíveis dados pelos números \\(\\{1,2,3,4,5,6\\}\\) e cada evento pode sair com uma probabiidade, por exemplo, \\(1/6\\). 3.2 Esperança matemática A esperança de uma variável aleatória discreta \\(X\\), cuja probabilidade de massa de \\(x \\in X\\) é dada por \\(p(x)\\), é definida por: \\(\\sum(x*p(x))\\). A esperança de uma v.a. contínua X, cuja densidade é \\(f(x)\\), é definida por: \\(\\int f(x)*x\\,dx\\). Similarmente, para a mesma v.a. X acima, a esperança de uma função \\(h(X)\\) é dada por \\(\\int f(x)*h(x)\\,dx\\) e analogamente para o caso discreto. 3.3 Variância A variância de uma variável aleatória \\(X\\) é dada por: Definição 1. \\(Var(X) = \\mathbb{E}[(X - \\mathbb{E}[X])^2]\\). A Covariância de duas v.a. \\(X\\) e \\(Y\\) é definida como: \\(Cov(X,Y) = \\mathbb{E}[(X - \\mathbb{E}[X])*(Y - \\mathbb{E}[Y])]\\). Notem que \\(Cov(X,X) = Var(X)\\). A covariância é positiva quando ambos X e Y tendem a ter valores acima (ou abaixo) de sua média simultaneamente, enquanto ela é negativa quando uma v.a. tende a ter valores acima da sua média e a outra abaixo. 3.4 Algebra com Esperança, Variância e Covariância Sejam \\(a\\) e \\(b\\) constantes. Linearidade da Esperença \\(\\mathbb{E}[aX + bY] =\\mathbb{E}[aX] + \\mathbb{E}[by] = a*\\mathbb{E}[X] + b*\\mathbb{E}[Y]\\) Exercício: verifique, com exemplos, que isso é verdade. Identidade da variância \\(Var(X) = \\mathbb{E}[(X - \\mathbb{E}[X])^2] = \\mathbb{E}[X^2] - \\mathbb{E}^2[X]\\) A prova será demonstrada mais adiante. Identidade da Covariância \\(Voc(X,Y) = \\mathbb{E}[X*Y] - \\mathbb{E}[X]*\\mathbb{E}[Y] = \\mathbb{E}[(X - \\mathbb{E}[X])*(Y - \\mathbb{E}[Y])]\\) Exercício para o leitor. Prove que isso é verdade. 4, Covariância é simétrica \\(Cov(X,Y) = Cov(Y,X)\\) Variância não é linear \\(Var(a*X + b) = a^2*Var(x)\\) Covariância não é linear \\(Cov(a*X + b,Y) = a*Cov(Y,X)\\) # Prova da identidade da variância Vamos mostrar que \\(\\mathbb{E}[(X - \\mathbb{E}[X])^2] = \\mathbb{E}[X^2] - \\mathbb{E}^2[X]\\) Começamos expandido o quadrado da esperança: \\(Var(X) = \\mathbb{E}[(X - \\mathbb{E}[X])^2] = \\mathbb{E}[(X - \\mathbb{E}[X]) * (X - \\mathbb{E}[X])]\\). Aplicando a regra do quadrado, temos: \\(\\mathbb{E}[(X - \\mathbb{E}[X]) * (X - \\mathbb{E}[X])] = \\mathbb{E}[(X^2 - 2* \\mathbb{E}[X]*X + \\mathbb{E}[X]^2)]\\) Pela propriedade da experança, sabemos que, sejam \\(A\\) e \\(B\\) duas v.a. independentes, então \\(\\mathbb{E}[A + B] = \\mathbb{E}[A] + \\mathbb{E}[B]\\). Então: \\(Var(X) = \\mathbb{E}[X^2] - \\mathbb{E}[2*\\mathbb{E}[X]*X] + \\mathbb{E}[\\mathbb{E}[X]^2]]\\) Outra propriedade da esperança é que, seja \\(a\\) uma constante e \\(X\\) uma v.a., então \\(\\mathbb{E}[a*X] = a*\\mathbb{E}[X]\\). \\(Var(X) = \\mathbb{E}[X^2] - 2*\\mathbb{E}[\\mathbb{E}[X]*X] + \\mathbb{E}[\\mathbb{E}[X]^2]]\\) Nós sabemos que \\(\\mathbb{E}[X]\\) é uma constante (é uma média da v.a.). E a média de uma constante é a própria constante. Portanto, \\(\\mathbb{E}[\\mathbb{E}[X]] = \\mathbb{E}[X]\\). E usaremos também que \\(\\mathbb{E}[a*X] = a*\\mathbb{E}[X]\\) e, por fim, o fato de que uma constante ao quadrado é em si uma constante. \\(Var(X) = \\mathbb{E}[X^2] - 2*\\mathbb{E}[X] * \\mathbb{E}[\\mathbb{E}[X]] + \\mathbb{E}[X]^2]\\) \\(Var(X) = \\mathbb{E}[X^2] - 2*\\mathbb{E}[X] * \\mathbb{E}[X] + \\mathbb{E}[X]^2]\\) \\(Var(X) = \\mathbb{E}[X^2] - 2*\\mathbb{E}[X]^2 + \\mathbb{E}[X]^2]\\) \\(Var(X) = \\mathbb{E}[X^2] - \\mathbb{E}[X]^2\\) Como Queriamos Demonstrar (CQD). 3.5 Distribuição de Probabilidade Conjunta A Distribuição de probabilidade conjunta de \\(X\\) e \\(Y\\) (definida no mesmo espaço de probabilidade) é uma distribuição de probabilidade dos pares \\((x,y)\\) e descreve como os valores de \\(X\\) e \\(Y\\) variam conjuntamente. Cada uma das distribuições \\(X\\) e \\(Y\\) sozinhas são chamadas de distribuições marginais. Uma distribuição conjunta é como uma máquina que, de acordo com certas regras de probabilidade, retorna dois pares de valores. ex. 1. Roleta. Em um casino, um jogo comum é a roleta. Ela consiste normalmente de 32 números (0 a 31), e cada número tem uma cor (preto, vermelho ou verde). Ao girar a roleta, ela solta um número e uma cor. Portanto, podemos pensar que a roleta é uma distribuição conjunta de duas variáveis (números e cores). Ex. 2.: Considere um dado de 4 faces \\(( 1, 2, 3, 4 )\\). Seja \\(X\\) a soma dos números dos dois dados, e \\(Y\\) o maior valor dos dois dados. O espaço amostral é dado pela tabela abaixo. library(knitr) library(dplyr) library(kableExtra) #Definir o espaço amostral espaco_amostral &lt;- expand.grid(1:4, 1:4) espaco_amostral$X &lt;- espaco_amostral$Var1 + espaco_amostral$Var2 espaco_amostral$Y &lt;- pmax(espaco_amostral$Var1, espaco_amostral$Var2) # Criar a tabela kable(espaco_amostral, col.names = c(&quot;resultado do primeiro dado&quot;, &quot;resultado do segundo dado&quot;, &quot;X&quot;, &quot;Y&quot;), caption = &quot;Tabela 2.5: Tabela representando a soma (X) and o maior valor (Y) do lançamento de dois dados de quatro lados&quot;) %&gt;% kable_styling(bootstrap_options = &quot;striped&quot;) Table 3.1: Tabela 2.5: Tabela representando a soma (X) and o maior valor (Y) do lançamento de dois dados de quatro lados resultado do primeiro dado resultado do segundo dado X Y 1 1 2 1 2 1 3 2 3 1 4 3 4 1 5 4 1 2 3 2 2 2 4 2 3 2 5 3 4 2 6 4 1 3 4 3 2 3 5 3 3 3 6 3 4 3 7 4 1 4 5 4 2 4 6 4 3 4 7 4 4 4 8 4 Se supusermos que todos os números possuem a mesma chance de sair quando jogamos os dados, então, a distribuição conjunta de \\(X\\) e \\(Y\\) pode ser dada por: library(knitr) library(kableExtra) # Definir os valores de (x, y) e P(X = x, Y = y) valores &lt;- c(&quot;(2, 1)&quot;, &quot;(3, 2)&quot;, &quot;(4, 2)&quot;, &quot;(4, 3)&quot;, &quot;(5, 3)&quot;, &quot;(5, 4)&quot;, &quot;(6, 3)&quot;, &quot;(6, 4)&quot;, &quot;(7, 4)&quot;, &quot;(8, 4)&quot;) probabilidades &lt;- c(0.0625, 0.1250, 0.0625, 0.1250, 0.1250, 0.1250, 0.0625, 0.1250, 0.1250, 0.0625) # Criar a tabela tabela &lt;- data.frame(&quot;(x, y)&quot; = valores, &quot;P(X = x, Y = y)&quot; = probabilidades) # Formatar a tabela kable(tabela, caption = &quot;Tabela 2.26: Tabela representando a distribuição conjunta da soma (X) e o maior (Y) de dois lançamentos de um dado de quatro faces&quot;, col.names = c(&quot;$(X,Y)$&quot;, &quot;$P(X=x, Y=y)$&quot;)) %&gt;% kable_styling(bootstrap_options = &quot;striped&quot;) Table 3.2: Tabela 2.26: Tabela representando a distribuição conjunta da soma (X) e o maior (Y) de dois lançamentos de um dado de quatro faces \\((X,Y)\\) \\(P(X=x, Y=y)\\) (2, 1) 0.0625 (3, 2) 0.1250 (4, 2) 0.0625 (4, 3) 0.1250 (5, 3) 0.1250 (5, 4) 0.1250 (6, 3) 0.0625 (6, 4) 0.1250 (7, 4) 0.1250 (8, 4) 0.0625 "],["introdução-à-simulação.html", "Capítulo 4 Introdução à Simulação 4.1 Análise de sensibilidade", " Capítulo 4 Introdução à Simulação Simulação é uma ferramenta extremamente importante em probabilidade e estatística. Dois são seus principais usos. De um lado, é possível usar um modelo de probabilidade para recriar, artificialmente, no computador, amostras desse modelo, quantas vezes quisermos. Tudo se passa como se tivéssemos o poder de criar um mundo, totalmente controlado por nós, e podemos simular esse mundo e verificar o que acontece. Isso é útil quando não podemos ou não queremos verificar matematicamente o que deveria acontecer com base no modelo matemático de probabilidade. De outro lado, é possível usar simulações para aproximar quantidades matemáticas que não podemos ou não queremos calcular analiticamente. Por calcular analiticamente, é fazendo a conta nós mesmos. Um exemplo simples disso é, em vez de calcular os resultados de alguma análise combinatória usando as fórmulas de combinação e simularmos os valores e contar quantas combinações resultam. No geral usamos simulação para o primeiro tipo de uso. Mas, aqui no curso, será útil que vocês verifiquem resultados matemáticos por meio de simulações. Assim, se escrevemos que \\(\\sum_{i=1}^{10} i = 55\\), vocês podem verificar com código no R essa soma, por exemplo, rodando: sum(1:10). Sempre que vocês tiverem dúvida de algum passo matemático, podem fazer uma simulação para entender melhor o que está acontecendo. Isso é encorajado, para melhorar a intuição do que está acontecendo e lhe assegurar que de fato você está entendendo o que está acontecendo. Podemos usar simulações para aproximar quantidades. Um exemplo clássico em probabilidade é a fórmula de Stirling, dada por \\(n! \\sim \\sqrt{2\\pi}n^{n + 1/2}*e^{-n}\\). Então, por exemplo, \\(10! = 10*9*8*7*6*5*4*3*2*1\\) com os computadores modernos pode ser calculada diretamente para números não tão grandes. No caso, \\(10! = 3628800\\). Ou pode ser aproximada pela fórmula de Stirling: Exemplo 4.1 # especificando semente, para simulação ser reproduzível set.seed(2) # número de amostras stirling_aprox &lt;- function(n) { sqrt(2*pi)*n^(n+1/2)*exp(-n) } print(stirling_aprox(10)) [1] 3598696 # razão da aproximação para o valor correto stirling_aprox(10)/3628800 [1] 0.991704 # erro percentual 1 - stirling_aprox(10)/3628800 # 0,8% [1] 0.00829596 Como se vê, a fórmula de Stirling é bem precisa para aproximar fatorial e muito mais fácil de computar. Dito isso, voltemos ao primeiro caso, de simulação de modelos de probabilidade. Comecemos lembrando que probabilidades podem ser interpretadas como frequências relativas de longo prazo. Portanto, a probabilidade de um evento pode ser aproximada por simulações, na medida em que aproximamos a frequência relativa com que o fenômeno acontece em nossas simulações. Vamos dar um exemplo simples desse tipo de aplicação. Suponha que quero simular a probabilidade do número 6 sair em um dado de seis lados. Exemplo 4.1.1 # especificando semente, para simulação ser reproduzível set.seed(234) # número de amostras n &lt;- 10000 # 1000 amostras de uma lançamento de dado de 6 lados resultado &lt;- sample(1:6, n, TRUE) # frequência relativade 6 é dada por número de 6 / total de amostras prob_6 &lt;- sum(resultado == 6)/n # 16,89% # 1/6 = 16.6666 Podemos também ver como a aproximação converge para o verdadeiro valor à medida que \\(n\\) cresce. # especificando semente, para simulação ser reproduzível set.seed(234) # número de amostras vec_amostra &lt;- c(100, 1000, 10000, 100000, 1000000) # lista vazia para armazenar os resultados das simulações resultado_lista &lt;- list() # vetor vazio para armazenar a frequência relativa de 6 vec_prob6 &lt;- numeric() set.seed(234) # loop sobre os tamanhos das amostrar for ( i in 1:length(vec_amostra)) { # n amostras de uma lançamento de dado de 6 lados resultado_lista[[i]] &lt;- sample(1:6, vec_amostra[i], TRUE) # frequência relativade 6 é dada por número de 6 / total de amostras vec_prob6[i] &lt;- sum(resultado_lista[[i]] == 6)/vec_amostra[i] } print(vec_prob6) [1] 0.150000 0.189000 0.164700 0.169250 0.166257 Se supusermos que todos os números possuem a mesma chance de sair quando jogamos os dados, então, a distribuição conjunta de \\(X\\) e \\(Y\\) pode ser dada por: library(knitr) library(kableExtra) # Definir os valores de (x, y) e P(X = x, Y = y) valores &lt;- c(&quot;(2, 1)&quot;, &quot;(3, 2)&quot;, &quot;(4, 2)&quot;, &quot;(4, 3)&quot;, &quot;(5, 3)&quot;, &quot;(5, 4)&quot;, &quot;(6, 3)&quot;, &quot;(6, 4)&quot;, &quot;(7, 4)&quot;, &quot;(8, 4)&quot;) probabilidades &lt;- c(0.0625, 0.1250, 0.0625, 0.1250, 0.1250, 0.1250, 0.0625, 0.1250, 0.1250, 0.0625) # Criar a tabela tabela &lt;- data.frame(&quot;(x, y)&quot; = valores, &quot;P(X = x, Y = y)&quot; = probabilidades) # Formatar a tabela kable(tabela, caption = &quot;Tabela 2.26: Tabela representando a distribuição conjunta da soma (X) e o maior (Y) de dois lançamentos de um dado de quatro faces&quot;, col.names = c(&quot;$(X,Y)$&quot;, &quot;$P(X=x, Y=y)$&quot;)) %&gt;% kable_styling(bootstrap_options = &quot;striped&quot;) Table 4.1: Tabela 2.26: Tabela representando a distribuição conjunta da soma (X) e o maior (Y) de dois lançamentos de um dado de quatro faces \\((X,Y)\\) \\(P(X=x, Y=y)\\) (2, 1) 0.0625 (3, 2) 0.1250 (4, 2) 0.0625 (4, 3) 0.1250 (5, 3) 0.1250 (5, 4) 0.1250 (6, 3) 0.0625 (6, 4) 0.1250 (7, 4) 0.1250 (8, 4) 0.0625 como podemos ver, à medida que \\(n\\) cresce, a simulação converge para o verdadeiro valor do parâmetro, embora isso não seja linear. Por isso que é importante variar a configuração para ter certeza de que a simulação está próxima do verdadeiro valor do parâmetro. De maneira geral, uma simulação envolve os seguintes passos. 4.0.1 Configuraçao Definir o modelo de probabilidade, variáveis aleatórias e eventos a serem modelados. O modelo de probabilidade codifica todas as suposições do modelo. No exemplo acima, de que todos os lados têm a mesma probabilidade de sair, que existem apenas seis possibilidades, numeradas de \\(1-6\\) e assim por diante. 4.0.2 Simular 4.0.3 Resumir 4.1 Análise de sensibilidade Um dos resultados mais contraintuitivos de modelos de probabilidade está relacionado à chamada “falácia do jogador”. A falácia é a crença de que, porque saiu um número par (ou preto) na última jogada em uma roleta no cassino, então aumenta a chance de sair ímpar (vermelho) na próxima rodada, para que os números se equilibrem. Porém, cada jogada é independente da anterior. Um problema relacionado é o chamado problema da ruína do jogador. Exemplo 1.2. Ballot theorem (toerema da urna de votação? teorema da votação?) Esse teorema, provado em 1878 por W. A. Whitworth e independentemente em 1887 por J. Bertrand estabelece a probabilidade de que, durante a apuração dos votos em uma urna, sempre haja mais votos contabilizados para o candidato \\(P\\) do que para o candidato \\(Q\\), quando \\(P\\) possui \\(p\\) votos e \\(Q\\) possui \\(q\\) votos, com \\(p &gt; q\\). A fórmula para essa probabilidade é \\((p-q)/(p+q)\\). Vamos verificar esse teorema por meio de simulação? Configuração: suponha que \\(p = .51\\) e \\(q = .49\\). O que sigifnica que é \\((p-q)/(p+q) = .02/1 = 2\\%\\) Primeiro, faremos uma simulação, e depois repetiremos k vezes a simulação para obter a frequência relativa. set.seed(234) # número de votos na urna n = 1000 # 1 é o voto para P, -1 o voto para Q. # então geramos n votos e armazenamos em votos # vamos usar rbinom, que gera 0 ou 1. votos &lt;- rbinom(n, 1, p=.51) # transformado 0 em -1 votos &lt;- ifelse(votos == 0, -1, votos) # Agora, iremos contar o primeiro voto e registrar para quem vai. Depois o segundo e o terceiro etc. até não sobrarem votos a serem apurados. Vamso guardar essa sequência em um objeto apuração. apuracao &lt;- numeric() # em seguida, contamos se em algum momento Q está na frente. Se estiver, então não aconteceu de P liderar toda a apuração. contagem_lideranca &lt;- numeric() for ( i in 1:n) { apuracao[i] &lt;- votos[i] contagem_lideranca[i] &lt;- sum(apuracao) } # se a soma dos votos for positiva, tivemos mais votos 1 que -1. Se for negativa ou igual a zero, P não esteve liderando (esteve atrás ou empatando) # então, basta contar quantos casos aconteceram da soma dos votos apurados é menor ou igual a zero. if (sum(contagem_lideranca &lt;= 0)) print(&quot;p não liderou sempre&quot;) [1] “p não liderou sempre” #agora, vamos fazer a simulação mil vezes. Esperamos que aprox. 2% das vezes P lidere sempre. # simulando k = 1000 vezes apuracao &lt;- numeric() contagem_lideranca &lt;- numeric() freq_lideranca &lt;- numeric() # armazanear se liderou ou não em cada sim k for (k in 1:1000) { for ( i in 1:n) { apuracao[i] &lt;- votos[i] contagem_lideranca[i] &lt;- sum(apuracao) } freq_lideranca[k] &lt;- as.numeric(sum(contagem_lideranca &lt;= 0)) if( k%% 50 == 0) print(k) # para vermos a evolução da simulação a cada 50 passos } [1] 50 [1] 100 [1] 150 [1] 200 [1] 250 [1] 300 [1] 350 [1] 400 [1] 450 [1] 500 [1] 550 [1] 600 [1] 650 [1] 700 [1] 750 [1] 800 [1] 850 [1] 900 [1] 950 [1] 1000 # se a soma dos votos for positiva, tivemos mais votos 1 que -1. Se for negativa ou igual a zero, P não esteve liderando (esteve atrás ou empatando) # então, basta contar quantos casos aconteceram da soma dos votos apurados é menor ou igual a zero. print(sum(freq_lideranca)/k) [1] 0.281 # 0,281 ou 2,8%. Próximo dovalor verdadeiro de 2%. Se aumentarmos k, irá convergir para o valor verdadeiro. Suponha agora um problema diferente. Um dos dois candidatos vai ter a primeira liderança. Após \\(n\\) votos apurados, qual a probabilidade de a contagem ter empatado pela primeira vez? Dito de outro modo, após uma das candidatas assumir a liderança, quanto “tempo” (isto é, após quantos votos apurados) em média devemos esperar até que uma reversão de liderança ocorra (ou pelo menos um empate)? A intuição que as pessoas têm, talvez a partir da lei dos grandes números, é que deveríamos esperar que, se \\(p = q = 1/2\\), ambas candidatas deveriam liderar por 50% do tempo em uma apuração suficientemente longa (com \\(n\\) votos grande) e deveria haver troca constante de liderança, em vez de uma candidata liderar po um longo tempo. Essa intuição é errada, como iremos mostrar agora por simulação. A leitora interessada na demonstração matemática utilizando apenas análise combinatória (portanto, apenas estatística básica) deve consultar o clássico livro de Feller (1968). "],["introdução-à-regressão-linear-univariada.html", "Capítulo 5 Introdução à regressão linear univariada 5.1 Esperança Condicional 5.2 CEF 5.3 Objetivos de aprendizagem ao final do capítulo", " Capítulo 5 Introdução à regressão linear univariada O modelo de regressão (não confundir com regressão linear) é uma forma bem ampla de modelar os dados para prever uma variável de interesse, usualmente designada pela letra \\(Y\\). Se eu quero prever os votos de candidatas em uma eleição, a votação de cada candidata é minha variável de interesse, \\(Y\\). Digamos que eu tenho uma amostra da intenção de votos das candidatas, obtidas por meio de uma pesquisa eleitoral. Há muitas formas de apresentar ou motivar regressão linear. O método mais tradicional é pensar que a regressão linear é uma reta que é ajustada aos pontos observados. Porém, não tomaremos esse caminho aqui. Nós iremos tomar aqui o caminho de considerar que a regressão linear é uma formade aproximar a chamada “Conditional Regression Function” (CEF, na sigla em inglês). O objetivo é entender “as far as possible with the available data how the conditional distribution of some response y varies across subpopulations determined by the possible values of the predictor or predictors” (Cook and Weisberg, apud Berk, p. 4). ## # A tibble: 1 × 1 ## `mean(pib_per_capita)` ## &lt;dbl&gt; ## 1 17389. ## # A tibble: 2 × 2 ## amazonia_legal `mean(pib_per_capita)` ## &lt;chr&gt; &lt;dbl&gt; ## 1 Não 17953. ## 2 Sim 13883. Portanto, vamos retomrar o conceito de esperança condicional, para introduzir em seguida a função de regressão condicional (CEF, em inglÊs) e então como a regressão pode ser pensada como uma forma de aproximar a CEF. 5.1 Esperança Condicional Uma das primeiras distinções que temos de fazer é sobre previsão e explicação (causal). Quando queremos prever, estamos interessados em saber quais os provaveis valores de variáveis no futuro, a parte de informações sobre a própria variável e outras no passado. Nesse sentido, é preciso algum tipo de suposição de que o futuro se assemelha ao passado de algum modo. Esse tipo de suposição usualmente toma a forma de um modelo probabilísitco, mas não apenas. Quando estamos interessados em explicações causais, temos dois tipos de perguntas de pesquisa possíveis. Uma sobre a chamada causa dos efeitos e outra sobre o efeito das causas (Gelman &amp; Imbens, 2013). A causa dos efeitos são perguntas do tipo: o que causal a II Grande Guerra? Ou qual a causa da eleição de Trump ou Bolsonaro? O que explica a desigualdade de renda no Brasil? São todas perguntas em que queremos explicar um efeito, isto é, identificar as causas de um fenômeno (efeito). Já o efeito das causas ão perguntas do tipo: qual o efeito da vacina de covid-19 sobre a mortalidade por Covid-19? Qual o efeito de checagem de notícias sobre a crença de pessoas em desinformação? Qual o efeito da magnitude eleitoral sobre fragmentação partidária? E assim por diante. Aqui, estamos interessados em entender o efeito causal de uma variável sobre outra, sem pretender esgotar todas as explicações de causa possíveis. A maior parte dos métodos quantitativos existentes são bons para responder perguntas de previsão e de causa dos efeitos. Grosso modo, não há método quantitativo para estimação do efeito das causas, exceto realizar uma série de estudos independentes sobre várias causas dos efeitos, olhando uma causa distinta do mesmo efeito por vez e esperar que isso gere um conhecimento combinado sobre essas múltiplas causas. Mas não há, contudo, uma metodologia bem definida de como combinar esses estudos independentes em um único conhecimento do efeito conjunto das causas. Assim, nosso curso será dedicado apenas a modelos de previsão e modelos de causa dos efeitos, que é o que temos de metodologias já desenvolvidas e consolidadas. Começamos por essa explicação porque uma perspectiva mais antiga, e ainda comum nas ciências sociais, é que modelos de regressão múltiplas permitem estimar o efeito de várias causas. Isso raramente é o caso e não adotaremos essa perspecitva aqui 1. 5.2 CEF O que consgtitui uma boa previsão? Tradicionalmente, empregamos a noção de Erro Quadrátco Médio (EQM) para quantificar boas previsões. Quanto menor o EQM, melhor uma previsão. Se o objetivo é, portanto, fazer previsões que minimizem o EQM, iremos apresertar e mostrar que a Função de Esperança Condicional (CEF, na sigla em inglês) é o melhor preditor global possível. Vamos dizer em outras palavras, porque esse resultado é verdadeiramente icnrível. A CEF é o melhor preditor possível dentre todos que existam ou possam vir a existir, entendendo melhor como ter o menor EQM. Por isso que a CEF é o ponto de partida de qualquer preditor que exista, seja uma regressão simples ou algoritmos de aprendizens de máquinas como “random forest” ou mesmo algorítimos de deep learning de redes neurais por traz dos recentes avanços na inteligência artificial. Mesmo os algorítmos mais avançados de inteleigência artificial, como os Large Language Models, que estão na base de ferramentas como ChatGPT, não podem ter desempenho melhor que a função de experança condicional, CEF, ao fazer uma previsão. Naturalmente, se esse é o caso, a próxima pergunta que todos nós iremos fazer é: por que não aprender apenas a usar a CEF, que é o melhor preditor possível, e ser feliz para sempre? Porque a natureza não nos diz qual é a CEF. Nós nunca sabemos qual a verdadeira função de esperança condicional. Então tentamos aproximar o melhor possível a CEF, a partir de simplificações da realidade. Em particular, nosso curso pode ser pensado em torno das seguintes perguntas: como aproximar a CEF por meio de regressão linear (combinação lineares de preditores)? Quais as propridades dessa aproximação? Em que condições ela é uma boa aproximação e em que sentido (quantitativo e preciso) podemos falar de boa aproximação? Mais para o final do curso faremos a conexão entre a CEF, modelos preditivos e modelos causais. 5.3 Objetivos de aprendizagem ao final do capítulo Estudantes deverão ter aprendido ao final do capítulo: Reconhecer que a função de esperança condicional, a CEF, em sua forma pura, é o melhor preditor possível para uma variável alvo, dadas as informações de outras variáveis. Memorizar que todos os outros predidores, sejam lineares ou não-lineares, incluindo preditores de deep learning, são tentativas de aproximar a CEF. Apreciar que o melhor predito linear (isto é, considerando apenas preditores que incluem combinação lineares de variáveis) produz previsões razoáveis, e antecipar que esse preditor noe leva à regressão linear. t A maior parte do curso será dedicadas a modelos preditivos, e apenas pontualmente falaremos de modelos causais↩︎ "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
