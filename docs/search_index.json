[["estimação.html", "Capítulo 7 - Estimação 7.1 Plug-in estimators 7.2 Simulando de uma Normal Bivariada 7.3 Suposições amostrais de MQO 7.4 Modelo de Média amostral 7.5 Modelo de Regressão Linear com preditores", " Capítulo 7 - Estimação Nós já vimos que é possível mostrar que o preditor linear ótimo com um único regressor possui intercepto \\(\\alpha = \\mathbb{E}[Y] - \\beta*\\mathbb{E}[X]\\) e inclinação \\(\\beta = Cov(X,Y)/Var(X)\\). Isso significa que, se nós considerarmos o modelo de regressão \\(y_i = \\alpha + \\beta*x_i + e_i\\), e usarmos essas fórmulas para calcular os valores de \\(\\alpha\\) e \\(\\beta\\) em uma população, obteríamos uma reta ajustada que é o melhor preditor linear. Até o momento, estávamos trabalhando com a população. Porém, na prática estaremos sempre lidando com uma amostra, de forma que precisamos entender como funciona a estimação de uma regressão a partir de uma amostra. 7.1 Plug-in estimators Se tivermos uma amostra, e não a população, é razoável pensar que uma boa estimativa para os valores populacionais de \\(\\alpha\\) e \\(\\beta\\) são justamente essas fórmulas, calculadas para os dados amostrais. Esse estimador dos parâmetros populacionais nós chamamos de plug-in estimates, pois sem maiores teorias, assumimos que o que vale para a população vale para a amostra. Para diferenciar as estimativas amostrais dos valores populacionais, é comum usarmos \\(\\hat{\\beta}\\) em vez de \\(\\beta\\), ou então letras latinas \\(b\\) em vez das gregas \\(\\beta\\). Vou usar letras latinas, mas outros textos utilizam letras gregas com o “chapéu”. 7.2 Simulando de uma Normal Bivariada Uma distribuição de probabilidade de \\(X\\) e \\(Y\\) é Normal bivariada quando \\(aX + bY\\) é Normal também, para quaisquer \\(a\\) e \\(b\\). Para nós, importa aqui que \\(X\\) e \\(Y\\) são normais cada uma, e a conjunta deles também é Normal. E diferentemente da Normal univariada, preciso representar os parâmetros por meio de vetores. A média, que antes era um escalar, agora será representada por um vetor: \\(\\mu = (\\mu_x, \\mu_y)\\). Mesmo que \\(\\mu_x = \\mu_y\\), ainda assim preciso representar por um vetor. E a variância também não consegue especificartudo que preciso saber sobre a variação nos dados. Eu preciso saber, a variância de \\(X\\), dada por \\(\\sigma_x\\), a variância de \\(Y\\), \\(\\sigma_x\\) e a covariância entre \\(X\\) e \\(Y\\) e \\(Y\\) e \\(X\\), dadas respectivamente por \\(\\mathbb{Cov}[X,Y]\\) e \\(\\mathbb{Cov}[Y,X]\\), que obviamente serão iguais. E nós representamos essas quatro informações por meio de uma matriz, chamada de matriz de covariância ou matriz de variância-covariância: \\[ \\Sigma = \\begin{bmatrix} \\sigma_x &amp; \\mathbb{Cov}[Y,X] \\\\ \\mathbb{Cov}[X,Y] &amp; \\sigma_y \\end{bmatrix} \\] Essa matriz precisa ser positiva-definida, que é uma característica de matrizes, mas que basicamente tem a ver com o fato de que não existe variância negativa e dadas as variâncias de \\(X\\) e \\(Y\\), existem limites para o que a covariância pode ser. Como escolher os valores para essa matriz não é nada intuitivo, vamos trabalhar com a correlação, que temos ma noção melhor. Para isso, utilizaremos a seguinte relação: \\[ \\Sigma = \\begin{bmatrix} \\sigma_x^2 &amp; \\rho \\cdot \\sigma_x \\cdot \\sigma_y \\\\ \\rho \\cdot \\sigma_x\\cdot \\sigma_y &amp; \\sigma_y \\end{bmatrix} \\] Ou seja, a matriz de variância-covariância pode ser parametrizada apenas em função dos desvios-padrões e da correlação entre as duas variáveis. library(ggplot2) library(tidyverse) # vetor de médias vetor_media &lt;- c(0,0) sigma_x &lt;- 2 sigma_y &lt;- 2 rho &lt;- .6 matriz_var_cov &lt;- matrix(c(sigma_x^2, rho*sigma_x*sigma_y, rho*sigma_x*sigma_y, sigma_y^2 ), byrow=T, nrow=2) matriz_var_cov %&gt;% kable() 4.0 2.4 2.4 4.0 set.seed(345) norm_bivariada &lt;- MASS::mvrnorm(n = 10000, mu = vetor_media, Sigma = matriz_var_cov) bivariada_tibble1 &lt;- as_tibble(norm_bivariada, .name_repair = &quot;universal&quot;) %&gt;% rename(x = &#39;...1&#39;, y = &#39;...2&#39;) bivariada_tibble1 %&gt;% head() %&gt;% kable() x y -2.4157713 -0.3924015 -1.4771034 0.4770824 -0.6117056 0.0340563 -1.2569654 0.2172955 -1.1136253 0.8720169 -2.3546882 0.0881367 bivariada_tibble %&gt;% ggplot(aes(x)) + geom_density() bivariada_tibble %&gt;% ggplot(aes(y)) + geom_density() bivariada_tibble %&gt;% ggplot(aes(x, y)) + geom_density_2d() bivariada_tibble %&gt;% ggplot(aes(x, y)) + geom_point() Vamos usar esses dados para fazer uma regressão. reg1 &lt;- lm( y ~x , data = bivariada_tibble1) summary(reg1) ## ## Call: ## lm(formula = y ~ x, data = bivariada_tibble1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.7942 -1.0639 -0.0067 1.0740 6.4187 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.014913 0.015907 -0.937 0.349 ## x 0.588264 0.008073 72.870 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.591 on 9998 degrees of freedom ## Multiple R-squared: 0.3469, Adjusted R-squared: 0.3468 ## F-statistic: 5310 on 1 and 9998 DF, p-value: &lt; 2.2e-16 A fórmula do \\(\\beta\\) nós sabemos que depende da covariância entre as variáveis e o desvio-padrão de \\(X\\). Vamos então simular os dados novamente, mudando as correlações e variâncias, para entender como isso impacta a regressão a partir da fórmula da inclinação da reta. library(ggplot2) library(tidyverse) # vetor de médias vetor_media &lt;- c(0,0) # cov menor, dp o mesmo sigma_x &lt;- 2 sigma_y &lt;- 2 rho &lt;- .3 matriz_var_cov &lt;- matrix(c(sigma_x^2, rho*sigma_x*sigma_y, rho*sigma_x*sigma_y, sigma_y^2 ), byrow=T, nrow=2) matriz_var_cov %&gt;% kable() 4.0 1.2 1.2 4.0 set.seed(345) norm_bivariada &lt;- MASS::mvrnorm(n = 10000, mu = vetor_media, Sigma = matriz_var_cov) bivariada_tibble2 &lt;- as_tibble(norm_bivariada, .name_repair = &quot;universal&quot;) %&gt;% rename(x = &#39;...1&#39;, y = &#39;...2&#39;) bivariada_tibble2 %&gt;% ggplot(aes(x, y)) + geom_point() # cov igual, dp de x maior rho &lt;- .3 sigma_x &lt;- 4 matriz_var_cov &lt;- matrix(c(sigma_x^2, rho*sigma_x*sigma_y, rho*sigma_x*sigma_y, sigma_y^2 ), byrow=T, nrow=2) matriz_var_cov %&gt;% kable() 16.0 2.4 2.4 4.0 set.seed(345) norm_bivariada &lt;- MASS::mvrnorm(n = 10000, mu = vetor_media, Sigma = matriz_var_cov) bivariada_tibble3 &lt;- as_tibble(norm_bivariada, .name_repair = &quot;universal&quot;) %&gt;% rename(x = &#39;...1&#39;, y = &#39;...2&#39;) bivariada_tibble3 %&gt;% ggplot(aes(x, y)) + geom_point() O R mostra os resultados da regressão desse jeito, e vamos aprender a interpretá-lo, mas vamos também apresentar em um formato mais amigável e, caso vocês utilizem o rmarkdown, poderão utilizar em trabalhos acadêmicos. Para isso, utilizaremos o pacote stargazer reg2 &lt;- lm( y ~x , data = bivariada_tibble2) reg3 &lt;- lm( y ~x , data = bivariada_tibble3) library(&quot;stargazer&quot;) stargazer::stargazer(list(reg1, reg2, reg3), type = &quot;html&quot;, title = &quot;stargazer table&quot;) ## ## &lt;table style=&quot;text-align:center&quot;&gt;&lt;caption&gt;&lt;strong&gt;stargazer table&lt;/strong&gt;&lt;/caption&gt; ## &lt;tr&gt;&lt;td colspan=&quot;4&quot; style=&quot;border-bottom: 1px solid black&quot;&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style=&quot;text-align:left&quot;&gt;&lt;/td&gt;&lt;td colspan=&quot;3&quot;&gt;&lt;em&gt;Dependent variable:&lt;/em&gt;&lt;/td&gt;&lt;/tr&gt; ## &lt;tr&gt;&lt;td&gt;&lt;/td&gt;&lt;td colspan=&quot;3&quot; style=&quot;border-bottom: 1px solid black&quot;&gt;&lt;/td&gt;&lt;/tr&gt; ## &lt;tr&gt;&lt;td style=&quot;text-align:left&quot;&gt;&lt;/td&gt;&lt;td colspan=&quot;3&quot;&gt;y&lt;/td&gt;&lt;/tr&gt; ## &lt;tr&gt;&lt;td style=&quot;text-align:left&quot;&gt;&lt;/td&gt;&lt;td&gt;(1)&lt;/td&gt;&lt;td&gt;(2)&lt;/td&gt;&lt;td&gt;(3)&lt;/td&gt;&lt;/tr&gt; ## &lt;tr&gt;&lt;td colspan=&quot;4&quot; style=&quot;border-bottom: 1px solid black&quot;&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style=&quot;text-align:left&quot;&gt;x&lt;/td&gt;&lt;td&gt;0.588&lt;sup&gt;***&lt;/sup&gt;&lt;/td&gt;&lt;td&gt;0.284&lt;sup&gt;***&lt;/sup&gt;&lt;/td&gt;&lt;td&gt;0.148&lt;sup&gt;***&lt;/sup&gt;&lt;/td&gt;&lt;/tr&gt; ## &lt;tr&gt;&lt;td style=&quot;text-align:left&quot;&gt;&lt;/td&gt;&lt;td&gt;(0.008)&lt;/td&gt;&lt;td&gt;(0.010)&lt;/td&gt;&lt;td&gt;(0.005)&lt;/td&gt;&lt;/tr&gt; ## &lt;tr&gt;&lt;td style=&quot;text-align:left&quot;&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;/tr&gt; ## &lt;tr&gt;&lt;td style=&quot;text-align:left&quot;&gt;Constant&lt;/td&gt;&lt;td&gt;-0.015&lt;/td&gt;&lt;td&gt;-0.019&lt;/td&gt;&lt;td&gt;0.014&lt;/td&gt;&lt;/tr&gt; ## &lt;tr&gt;&lt;td style=&quot;text-align:left&quot;&gt;&lt;/td&gt;&lt;td&gt;(0.016)&lt;/td&gt;&lt;td&gt;(0.019)&lt;/td&gt;&lt;td&gt;(0.019)&lt;/td&gt;&lt;/tr&gt; ## &lt;tr&gt;&lt;td style=&quot;text-align:left&quot;&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;/tr&gt; ## &lt;tr&gt;&lt;td colspan=&quot;4&quot; style=&quot;border-bottom: 1px solid black&quot;&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style=&quot;text-align:left&quot;&gt;Observations&lt;/td&gt;&lt;td&gt;10,000&lt;/td&gt;&lt;td&gt;10,000&lt;/td&gt;&lt;td&gt;10,000&lt;/td&gt;&lt;/tr&gt; ## &lt;tr&gt;&lt;td style=&quot;text-align:left&quot;&gt;R&lt;sup&gt;2&lt;/sup&gt;&lt;/td&gt;&lt;td&gt;0.347&lt;/td&gt;&lt;td&gt;0.081&lt;/td&gt;&lt;td&gt;0.085&lt;/td&gt;&lt;/tr&gt; ## &lt;tr&gt;&lt;td style=&quot;text-align:left&quot;&gt;Adjusted R&lt;sup&gt;2&lt;/sup&gt;&lt;/td&gt;&lt;td&gt;0.347&lt;/td&gt;&lt;td&gt;0.081&lt;/td&gt;&lt;td&gt;0.085&lt;/td&gt;&lt;/tr&gt; ## &lt;tr&gt;&lt;td style=&quot;text-align:left&quot;&gt;Residual Std. Error (df = 9998)&lt;/td&gt;&lt;td&gt;1.591&lt;/td&gt;&lt;td&gt;1.892&lt;/td&gt;&lt;td&gt;1.904&lt;/td&gt;&lt;/tr&gt; ## &lt;tr&gt;&lt;td style=&quot;text-align:left&quot;&gt;F Statistic (df = 1; 9998)&lt;/td&gt;&lt;td&gt;5,309.975&lt;sup&gt;***&lt;/sup&gt;&lt;/td&gt;&lt;td&gt;879.742&lt;sup&gt;***&lt;/sup&gt;&lt;/td&gt;&lt;td&gt;929.207&lt;sup&gt;***&lt;/sup&gt;&lt;/td&gt;&lt;/tr&gt; ## &lt;tr&gt;&lt;td colspan=&quot;4&quot; style=&quot;border-bottom: 1px solid black&quot;&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style=&quot;text-align:left&quot;&gt;&lt;em&gt;Note:&lt;/em&gt;&lt;/td&gt;&lt;td colspan=&quot;3&quot; style=&quot;text-align:right&quot;&gt;&lt;sup&gt;*&lt;/sup&gt;p&lt;0.1; &lt;sup&gt;**&lt;/sup&gt;p&lt;0.05; &lt;sup&gt;***&lt;/sup&gt;p&lt;0.01&lt;/td&gt;&lt;/tr&gt; ## &lt;/table&gt; 7.3 Suposições amostrais de MQO Quando temos uma amostra, é comum supormos que os dados são independentes e identicamente distribuídos, i.i.d. Esse tipo de amostra às vezes é chamada de amostra aleatória simples. Isso é tipicamente válido em dados de corte transversal (cross-section) como uma pesquisa de opinião (survey) de intenção de voto, amostra pequena (mil, dois mil) de alunos do Brasil a partir de dados doe Censo Escolar e assim por diante. O ponto é que os dados precisam ser dispersos. Se você coletar alunos de uma mesma escola, eles não tenderão a ser independentes. Se amostrar eleitores de um mesmo bairro, tampouco haverá independência. Há formas de lidar com isso, e aprenderemos mais tarde. Por enquanto, tratemos do caso mais simples. A suposição de que dados são i.i.d significa que, em uma amostra com duas variáveis, \\(X\\) e \\(Y\\), o par \\(x_i, y_i\\) é independente de \\(x_j, y_j\\), com \\(i \\neq j\\) e identicamente distribuídos, isto é, com a mesma distribuição. Essa suposição é importante para calcular a variância dos nossos estimadores, não para a estimativa pontual deles. 7.4 Modelo de Média amostral Como vimos, o modelo de regressão mais simples é aquele sem preditores, em que \\(Y = \\mu + e\\). O estimador de mínimos quadrados para \\(\\hat{\\mu}\\) é a média amostral, \\(\\bar{y}\\). Notem que \\(\\mathbb{E}[Y]\\) é a esperança populacional e \\(\\mu\\) é o valor da esperança, já que a esperança do erro \\(e\\) é zero. Vamos então calcular a média de nosso estimador \\(\\bar{y}\\) e sua variância. \\[ \\mathbb{E}[\\bar{y}] = \\mathbb{E}[\\sum{y_i}/n] = (1/n) * \\sum{\\mathbb{E}[y_i]} = (1/n)*\\mu*n = \\mu \\] Portanto, o valor esperando do estimador de mínimos quadrados (a média amostral) é igual à média populacional. Quando isso acontece, isto é, quando a esperança de um estimador é igual ao parâmetro populacional dizemos que o estimador é não-viesado. Definição 7.1. Um estimador \\(\\hat{\\theta}\\) é não-viesado quando \\(\\mathbb{E}[\\hat{\\theta}] = \\theta\\). Agora, vamos calcular a variância da média amostral sob a suposição de que a amostra é i.i.d. Considerando \\(Y_i = \\mu + e_i\\), temos: \\[ \\bar{y} = \\sum{y_i}/n = \\sum{(\\mu + e_i)}/n = \\sum{\\mu}/n + \\sum{e_i}/n = \\mu + \\sum{e_i}/n \\] Rearranjando, temos: \\(\\bar{y} - \\mu = \\sum{e_i}/n\\) A variância então pode ser calculada como: \\[ \\mathbb{Var}[\\bar{y}] = \\mathbb{E}[(\\bar{y} - mu)^2] \\] \\[ = \\mathbb{E}[( \\sum\\limits_{i=1}^n {e_i}/n)*( \\sum\\limits_{j=1}^n{e_j}/n)] = (1/n^2)* \\sum\\limits_{i=1}^n{} \\sum\\limits_{j=1}^n{}\\mathbb{E}[e_ie_j] \\] Se nós somarmos o somatório indexado no \\(j\\), o primeiro erro fica constante (indexado no i) e vamos passar por todo os erros de \\(1\\) até \\(n\\). Eventualmente, vamos passar por \\(i\\) e todos os outros diferentes de \\(i\\). Sabendo que os erros são i.i.d, isso significa que erros diferentes são não correlacionados e possuem esperança zero, ou seja, \\(\\mathbb{E}[e_ie_j] = 0, i \\neq j\\), e erros iguais, por serem da mesma distribuição, têm \\(\\mathbb{E}[e_ie_i] = \\sigma^2\\). Então o resultado do último somatório é uma soma de zeros, exceto para o caso em que \\(i = j\\), em que dá \\(\\sigma^2\\). De forma que a equação fica: \\[ (1/n^2)* \\sum\\limits_{i=1}^n{}\\sigma^2 = (1/n^2)*n*\\sigma^2 = \\sigma^2/n \\] Ou seja, a variância de nosso estimador é igual à variância populacional dividida pelo tamanho da amostra. A variância amostral do estimador também é chamada de erro padrão. 7.5 Modelo de Regressão Linear com preditores Tendo mostrado que o estimador é não-viesado e calculado a variância amostral para o caso mais simples, sem preditor, vamos agora fazer o mesmo cálculo com um preditor. Vamos assumir que o modelo de regressão linear é uma boa aproximação para CEF. Portanto, vamos supor que: O modelo linear \\(y_i = \\alpha + \\beta*x_i + e_i\\) é adequado. \\(\\mathbb{E}[e|X] = 0\\), isto é, o erro é não correlacionado com x. Para essa demonstração, vamos utilizar uma parametrização alternativa para o modelo de regressão linear em torno da média de \\(x\\). Para isso, vamos usar nosso truque matemático de adicionar e subtrair Lembremos que a fórmula para o \\(\\hat{\\beta}\\) é \\(\\mathbb{Cov}[X,Y]/\\mathbb{Var}[X]\\). Substituindo nossa equação de regressão na fórmula, temos: \\[ \\hat{\\beta} = \\sum({x_i - \\bar{x}})*({y_i - \\bar{y}})/\\mathbb{Var}[x] = (\\sum({x - \\bar{x}})*y_i + \\sum({x -\\bar{x}})*\\bar{y})/\\mathbb{Var}[x] \\] Vamos agora usar o fato de que, para qualquer variável \\(z\\), a diferença média de \\(z\\) para a média amostral de \\(z\\) é zero, isto é, \\(n^{-1}*\\sum\\limits_{i=1}^n{}z - \\bar{z} = 0\\) Por fim, segue-se disso que, para qualquer constante \\(w\\) que não varia com \\(i\\), podemos escrever: \\(n^{-1}*\\sum\\limits_{i=1}^n{}(z - \\bar{z})*w = 0\\) Portanto, \\(\\sum({x -\\bar{x}})*\\bar{y}) = 0\\). Logo, \\[ \\hat{\\beta} = n^{-1}*\\sum({x - \\bar{x}})*y_i/\\mathbb{Var}[x] \\] Lembremos que, \\(y_i = \\alpha + \\beta*X_i + e_i\\). Se eu adicionar e subtrair \\(\\beta*\\bar{x}\\) da equação, não altero ela. Utilizando esse truque e rearranjando, temos: \\[ y_i = \\alpha + \\beta*X_i + e_i = \\alpha + \\beta*\\bar{x} + \\beta*X_i - \\beta*\\bar{x} + e_i = \\alpha + \\beta*\\bar{x} + \\beta*(x_i - \\bar{x}) + e_i \\] Substituindo na equação anterior, temos: \\[ \\hat{\\beta} = n^{-1}*\\sum({x - \\bar{x}})*(\\alpha + \\beta*\\bar{x} + \\beta*(x_i - \\bar{x}) + e_i)/\\mathbb{Var}[x] \\] \\[ \\hat{\\beta} = n^{-1}*(\\alpha + \\beta)\\bar{x}\\sum({x - \\bar{x}})*(\\beta*(x_i - \\bar{x}) + e_i)/\\mathbb{Var}[x] \\] \\[ \\hat{\\beta} = n^{-1}*(\\alpha + \\beta)\\bar{x}\\sum({x - \\bar{x}}) + n^{-1}\\beta\\sum(x_i - \\bar{x})^2 + n^{-1}\\sum({x - \\bar{x}})*e_i)/\\mathbb{Var}[x] \\] A priemria soma é uma constante, vezes o somatório de \\(x_i\\) menos sua média, o que é zero. A segunda soma é a variância de x multiplicada por \\(\\beta\\). Então, simplifcando, temos: \\[ \\hat{\\beta} = \\beta + n^{-1}\\sum({x - \\bar{x}})*e_i)/\\mathbb{Var}[x] \\] Portanto, nós mostramos que o estimador \\(\\hat{\\beta}\\) pode ser decomposto no parâmetro populacional \\(\\beta\\) mais uma soma ponderada do termo de erro. Uma vez que nossa amostra é i.i.d., sabemos que é uma média ponderada dos erros não-correlacionados. Podemos agora mostrar que o estimador é não-viesado. \\[ \\mathbb{E}[\\hat{\\beta}] = \\beta + n^{-1}\\mathbb{E}[\\sum({x - \\bar{x}})*e_i)]/\\mathbb{Var}[x] = \\beta +\\mathbb{E}[e]*\\mathbb{E}[\\sum({x - \\bar{x}})]\\mathbb{Var}[x] = \\beta \\] Podemos similarmente calcular a variância do nosso estimador, e mostrar que ela é a variância do erro dividida pela variância de x vezes o tamanho da amostra, \\(n\\). \\[ \\mathbb{Var}[\\hat{\\beta}] = \\mathbb{Var}[\\beta + n^{-1}\\sum({x - \\bar{x}})*e_i)/\\mathbb{Var}[x]] \\] Sabemos que \\(\\mathbb{Var}[x + a] = \\mathbb{Var}[x]\\). Então, podemos eliminar o \\(\\beta\\) da equação. \\[ \\mathbb{Var}[\\hat{\\beta}] = \\mathbb{Var}[n^{-1}\\sum({x - \\bar{x}})*e_i)/\\mathbb{Var}[x]] \\] Todas as observações do \\(x\\) foram observadas e, portanto, a soma da diferença para a média é uma constante. Sabemos que \\(\\mathbb{Var}[x*a] = a^2\\mathbb{Var}[x]\\) \\[ \\mathbb{Var}[\\hat{\\beta}] = (n^{-1}\\sum({x - \\bar{x}}))^2*\\mathbb{Var}[e_i/\\mathbb{Var}[x]] \\] Supondo homocedasticidade, [e_i] = ^2$ \\[ \\mathbb{Var}[\\hat{\\beta}] = (n^{-1}\\sum({x - \\bar{x}}))^2*\\mathbb{Var}[e_i/\\mathbb{Var}[x]] \\] Vamos começar com um exemplo, importando dados do site Base de Dados, que possui muitas bases de dados públicas, já tratadas. Para tanto, precisaremos criar um projeto no Google cloud, conforme os passos aqui: https://basedosdados.github.io/mais/access_data_packages/ Depois, só escolher quais dos dados iremos olhar https://basedosdados.org/ Escolhi mexer com dados do censo escolar # instalando a biblioteca # install.packages(&#39;basedosdados&#39;) # carregando a biblioteca na sessão library(basedosdados) # para importar os dadosdiretamente no R, precisamos criar um id para acessar a base de dados set_billing_id(&quot;aula-reg-manoel&quot;) # checando que deu certo get_billing_id() ## [1] &quot;aula-reg-manoel&quot; # importando dados knitr::include_graphics(here(&quot;imagens&quot;, &quot;autenticacao base dados.jpg&quot;)) query &lt;- &quot;SELECT * FROM `basedosdados.br_inep_censo_escolar.escola` where sigla_uf = &#39;AL&#39; and id_municipio = &#39;2704302&#39; and ano = 2019&quot; escola &lt;- read_sql(query) # query &lt;- &quot;SELECT count(*) as contagem, id_municipio FROM `basedosdados.br_inep_censo_escolar.escola` where sigla_uf = &#39;AL&#39; and ano = 2019 group by id_municipio&quot; # escola &lt;- read_sql(query) query &lt;- &quot;SELECT * FROM `basedosdados.br_inep_censo_escolar.dicionario`&quot; dicionario &lt;- read_sql(query) query &lt;- &quot;SELECT * FROM `basedosdados.br_inep_censo_escolar.matricula` where sigla_uf = &#39;AL&#39; and id_municipio = &#39;2704302&#39; and ano = 2019&quot; aluno &lt;- read_sql(query) # dicionário de gênero e raça # dicionario %&gt;% # dplyr::filter(id_tabela == &quot;matricula&quot;, nome_coluna %in% c(&quot;sexo&quot;, &quot;raca_cor&quot;)) aluno_gen &lt;- aluno %&gt;% dplyr::filter(regular == 1) %&gt;% group_by(id_escola, sexo) %&gt;% summarise(num_aluno_gen = n()) %&gt;% mutate(sexo = gsub(&quot;1&quot;, &quot;Male&quot;, sexo), sexo = gsub(&quot;2&quot;, &quot;Female&quot;, sexo)) %&gt;% mutate(total = sum(num_aluno_gen), percent = num_aluno_gen/total) %&gt;% filter(sexo == &quot;Female&quot;) %&gt;% rename(percent_female = percent) %&gt;% dplyr::select(id_escola, percent_female) aluno_raca &lt;- aluno %&gt;% dplyr::filter(regular == 1) %&gt;% group_by(id_escola, raca_cor) %&gt;% summarise(num_aluno_raca = n()) %&gt;% mutate(raca_cor = gsub(&quot;1&quot;, &quot;Branca&quot;, raca_cor), raca_cor = gsub(&quot;2&quot;, &quot;Preta&quot;, raca_cor), raca_cor = gsub(&quot;3&quot;, &quot;Parda&quot;, raca_cor)) %&gt;% mutate(total = sum(num_aluno_raca), percent = num_aluno_raca/total) %&gt;% filter(raca_cor %in% c(&quot;Branca&quot;, &quot;0&quot;, &quot;Preta&quot;, &quot;Parda&quot;)) %&gt;% mutate(raca_cor = gsub(&quot;0&quot;,&quot;não_declarado&quot;, raca_cor)) %&gt;% select(id_escola, raca_cor, percent) %&gt;% pivot_wider(names_from = raca_cor , values_from = percent ) aluno_escola &lt;- aluno_gen %&gt;% inner_join(aluno_raca, by = &quot;id_escola&quot;) %&gt;% inner_join(escola, by = &quot;id_escola&quot;) aluno_escola_reg &lt;- aluno_escola %&gt;% ungroup() %&gt;% mutate(negra = Preta + Parda) %&gt;% select(negra, percent_female, tipo_localizacao, agua_potavel, esgoto_rede_publica, lixo_servico_coleta, area_verde, biblioteca, quantidade_profissional_psicologo) %&gt;% filter(across(everything(), ~!is.na(.))) %&gt;% mutate_if(bit64::is.integer64, as.factor) reg &lt;- lm(negra ~ percent_female + tipo_localizacao + agua_potavel + esgoto_rede_publica + lixo_servico_coleta + area_verde + biblioteca + quantidade_profissional_psicologo, data= aluno_escola_reg) summary(reg) ## ## Call: ## lm(formula = negra ~ percent_female + tipo_localizacao + agua_potavel + ## esgoto_rede_publica + lixo_servico_coleta + area_verde + ## biblioteca + quantidade_profissional_psicologo, data = aluno_escola_reg) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.46731 -0.16217 -0.01934 0.17534 0.55176 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.070584 0.204310 0.345 0.72991 ## percent_female 0.400443 0.256195 1.563 0.11880 ## tipo_localizacao2 -0.251352 0.224599 -1.119 0.26373 ## agua_potavel1 0.067306 0.052684 1.278 0.20212 ## esgoto_rede_publica1 -0.069037 0.022951 -3.008 0.00279 ** ## lixo_servico_coleta1 0.144155 0.158554 0.909 0.36378 ## area_verde1 0.047138 0.025681 1.836 0.06714 . ## biblioteca1 0.009462 0.022531 0.420 0.67473 ## quantidade_profissional_psicologo1 -0.069969 0.033593 -2.083 0.03787 * ## quantidade_profissional_psicologo2 -0.120828 0.092870 -1.301 0.19396 ## quantidade_profissional_psicologo3 -0.306991 0.130754 -2.348 0.01935 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2222 on 418 degrees of freedom ## Multiple R-squared: 0.07237, Adjusted R-squared: 0.05017 ## F-statistic: 3.261 on 10 and 418 DF, p-value: 0.0004498 "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
