[["index.html", "Introdução à Regresssão para Ciências Sociais Capítulo 1 - Prefácio", " Introdução à Regresssão para Ciências Sociais Manoel Galdino 2023-10-03 Capítulo 1 - Prefácio Esse livro é, por enquanto, apenas uma repositório de notas de aula do curso de graduação da Ciências Sociais da USP, FLP0468 Métodos Quantitativos de Pesquisa na Ciência Política IV, de introdução à regressão, bem como o curso de pós-graduação, Métodos II. Agradeço aos alunos do curso pelos feedbacks (futuros e presentes) sobre esse material, bem como ao monitor do curso, Davi Veronese. Futuramente, pretendo transformar as notas de aulas em um livro, que poderá ser utilizado pela comunidade brasileira de ciências sociais interessada em aprender mais sobre métodos quantitativos, em particular regressão com R. A motivação para disponibilizar as notas de aula em formato de livro é a quase inexistência de um material bom de econometria básica, em português, voltado para a área de ciências sociais, em particular a ciência política. Existem bons manuais em inglês e bons manuais em português, mas voltado para a economia. Os livros existentes nem sempre fazem um bom trabalho de diferenciar regressão estatística e modelo estrutural (causal) de regressão. Uma abordagem moderna de inferência causal requer que essa distinção seja ensinada ao aluno e esse livro também pretende preencher essa lacuna. Por fim, a utilização do R é uma forma de dar ênfase à parte prática. Embora a teoria seja importante e a boa compreensão dos fundamentos é que permita o aprofundamento dos temas cobertos no livro, estatística é uma disciplina aplicada, cujo maior valor está na sua aplicação prática. Assim, boa parte do curso e do conteúdo do livro é dedicado a implementar a teoria no R, bem como na interpretação dos dados. O livro começa com capítulos iniciais de introdução/revisão do R e de estatística básica (incluindo probabilidade). Assim, pressupomos que um estudante que utilize esse curso tenha conhecimentos de estatística e probabilidade básica. Não é necessário conhecimento previo de R, nem de cálculo ou álgebra linerar. "],["revisão-de-r.html", "Capítulo 2 - Revisão de R 2.1 R e Rstudio 2.2 R como calculadora 2.3 Objetos no R 2.4 Tipos de objetos 2.5 Armazenando dados 2.6 Bibliotecas/pacotes 2.7 importando dados 2.8 Data wrangling 2.9 Visualização", " Capítulo 2 - Revisão de R 2.1 R e Rstudio O R é uma linguagem de programação voltada para análise de dados. O Rstudio é uma IDE (interface de desenvolvimento), que nos ajuda a programar em R. No curso utilizaremos o Rstudio para facilitar programar em R. Normalmente, iremos escrever um comando aqui no Script, clicar em executar (run) ou apertar ctrl + enter, e o Rstudio vai copiar o comando, colar no console e executá-los para nós. 2.2 R como calculadora o R pode funcionar como calculadora. 2+2 [1] 4 3*4 [1] 12 10/2 [1] 5 2.3 Objetos no R Tudo no R é um objeto. Isso significa que um número é um objeto. pi [1] 3.141593 Isso significa que funções (comandos) também são objetos sum(c(1,2,3)) [1] 6 sum function (…, na.rm = FALSE) .Primitive(“sum”) E nós podemos criar nossos próprios objetos, dando os nomes que quisermos (exceto se já existe um objeto no R com aquele nome, como por exemplo o objeto “sum”). x &lt;- 3 y &lt;- 7 x+y [1] 10 2.4 Tipos de objetos O R tem muitos tipos de objetos. Vamos listar aqui apenas os mais básicos. 2.4.1 Numeric Objetos do tipo numeric são … números (“reais”). # Exemplos pi [1] 3.141593 1/3 [1] 0.3333333 4 [1] 4 2.4.2 character Objetos do tipo character são do tipo texto. Sempre são escritos entre aspas (simples ou duplas, tanto faz) # Exemplos &quot;Manoel Galdino&quot; [1] “Manoel Galdino” &#39;abc&#39; [1] “abc” &quot;7&quot; [1] “7” 2.5 Armazenando dados Para armazenar dados, usualmente teremos 4 tipos de objetos: 1. vetor, 2. Matriz. 3. data.frame, 4. lista. Não vou falar de lista agora (nem de array, que é uma generalização da matriz para mais de duas dimensões). 2.5.1 Vetor Um vetor é uma sequência de objetos. # Exemplos c(1,2,3) [1] 1 2 3 1:3 [1] 1 2 3 c(&quot;Manoel&quot;, &quot;Hugo&quot;, &quot;Lia&quot;, &quot;Juliana&quot;, &quot;Jéssica&quot;) [1] “Manoel” “Hugo” “Lia” “Juliana” “Jéssica” c(c(1,2,3), c(2,3,1)) [1] 1 2 3 2 3 1 os elementos de um vetor devem ser todos do mesmo tipo: # Exemplos x &lt;- c(&quot;1&quot;, 1) x[2] [1] “1” # sedundo elemento fica armazenado como character # não é possível somar texto # x[2] + x[2] # erro 2.5.2 Matriz Uma matriz são vetores organizados por coluna, todas as colunas (vetores) só podem ser de um tipo, ou seja, não posso ter uma coluna numeric e outra de character, por exemplo. library(knitr) library(kableExtra) # Exemplos mat &lt;- matrix(1:6, nrow=3, ncol=2) kable(mat) 1 4 2 5 3 6 2.5.3 Data Frame O data.frame é uma tabela/planilha, e é onde normalmente armazenamos nossos bancos de dados no R. # Exemplos df &lt;- data.frame(x=1:3, y=c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;)) kable(df) x y 1 a 2 b 3 c 2.5.3.1 datas ## explicando data rapidamente data_ex &lt;- &quot;2020-10-23&quot; data_ex1 &lt;- as.Date(data_ex) data_ex1 + 0:9 [1] “2020-10-23” “2020-10-24” “2020-10-25” “2020-10-26” “2020-10-27” “2020-10-28” “2020-10-29” “2020-10-30” “2020-10-31” “2020-11-01” ## fim da explicação rápida de data ## criando data de forma repetitiva e tediosa. O que queremos evitar! minha_data &lt;- c(as.Date(&#39;2009-01-01&#39;), as.Date(&#39;2009-01-02&#39;), as.Date(&#39;2009-01-03&#39;), as.Date(&#39;2009-01-04&#39;), as.Date(&#39;2009-01-05&#39;), as.Date(&#39;2009-01-06&#39;), as.Date(&#39;2009-01-07&#39;), as.Date(&#39;2009-01-08&#39;), as.Date(&#39;2009-01-09&#39;), as.Date(&#39;2009-01-10&#39;)) acoes &lt;- data.frame( tempo = minha_data, X = rnorm(10, 0, 1), Y = rnorm(10, 0, 2), Z = rnorm(10, 0, 4) ) tempo X Y Z 2009-01-01 -0.6788076 4.0839109 4.971996 2009-01-02 0.5743127 1.2289625 7.928839 2009-01-03 -0.7045145 0.8438623 -2.585767 2009-01-04 -0.5339841 -0.9928433 3.864757 2009-01-05 0.7743846 0.9819228 -5.709070 2009-01-06 -0.4756214 -1.0039643 -1.829935 2009-01-07 -0.0244274 0.5763396 3.781867 2009-01-08 1.0190081 -1.3732520 -2.953557 2009-01-09 -1.2055804 1.5768076 1.382563 2009-01-10 1.5952939 1.3827377 -3.601779 # criando data.frame de maneira mais inteligente acoes &lt;- data.frame( tempo = as.Date(&#39;2009-01-01&#39;) + 0:9, X = rnorm(10, 0, 1), Y = rnorm(10, 0, 2), Z = rnorm(10, 0, 4) ) tempo X Y Z 2009-01-01 -0.3703507 -4.7473417 0.7853014 2009-01-02 -0.0407969 2.2491497 2.5152126 2009-01-03 -0.6123188 -3.5590955 3.4437886 2009-01-04 -1.9458521 -0.6891007 -3.8929894 2009-01-05 0.2430963 -2.2183462 3.7501722 2009-01-06 0.4749001 -1.2602116 -5.5808231 2009-01-07 0.1367146 2.6337675 6.9549721 2009-01-08 -0.4887477 1.0690268 -3.1945372 2009-01-09 0.9002037 0.9877962 3.0600975 2009-01-10 1.0775357 -4.1559849 1.2716454 2.6 Bibliotecas/pacotes O R permite que a gente importe comandos que não vêm por padrão no R. Em gerla esses comandos es~toa agrupados sob um pacote. PAra usar esses comandos, primeiro a gente instala o pacote, e depois carrega a biblioteca. # Exemplos #install.packages(&quot;data.table&quot;) #library(data.table) 2.7 importando dados Para importar dados, vamos usar a bilioteca “data.table” Então, instalem ela se ainda não instalaram (usando o comando install.packages(“data.table”)) E depois carreguem a biblioteca: library(data.table) Para importar, usaremos o comando fread do pacote data.table. # vamos importar uma base de dados de pib municipais de 2013, do IBGE # o arquivo está em formato RDS, que é um formato do R, e disponível no meu github. Para importá-lo direto no R, vamos ler o arquivo com a função url e depois import´-lo com a função readRDS. pib_cid &lt;- readRDS(url(&quot;https://github.com/mgaldino/book-regression/raw/main/dados/pib_cid.RDS&quot;)) # para visualizar os dados que forma importados, temos várias funções # glimpse, head e View library(dplyr) # para glimpse ## Warning: package &#39;dplyr&#39; was built under R version 4.2.3 # glimpse(pib_cid) # head(pib_cid) # View(pib_cid) 2.8 Data wrangling Para manipulação, limpeza e processamento de dados, iremos utilizar o chamado “tidyverse”. library(tidyverse) # digamos que quero o pib total médio e o pib per capita médio # basta usar o comando summarise, que resume os dados e escolher a função mean. df &lt;- pib_cid %&gt;% summarise(pib_medio = mean(pib_total), pib_per_capita_medio = mean(pib_per_capita)) kable(df) pib_medio pib_per_capita_medio 957202.7 17388.86 # se eu quiser a soma dos pibs municipais df &lt;- pib_cid %&gt;% summarise(soma_pib = sum(pib_total)) %&gt;% head() kable(df) soma_pib 5331618957 # maior e menos pibs e pibs per capita entre municípios df &lt;- pib_cid %&gt;% summarise(pib_max = max(pib_total), pib_min = min(pib_total), pib_per_capita_max = max(pib_per_capita), pib_per_capita_min = min(pib_per_capita)) %&gt;% head() kable(df) pib_max pib_min pib_per_capita_max pib_per_capita_min 582079726 4198.94 717343.7 301.6 # se eu quiser apenas dos municípios od estado de SP? # basta filtrar pelo estado de SP, com o comando filter df &lt;- pib_cid %&gt;% filter(sigla_uf == &quot;SP&quot;) %&gt;% summarise(soma_pib = sum(pib_total)) %&gt;% head() kable(df) soma_pib 1715238417 df &lt;- pib_cid %&gt;% filter(sigla_uf == &quot;SP&quot;) %&gt;% summarise(pib_medio = mean(pib_total), pib_per_capita_medio = mean(pib_per_capita)) %&gt;% head() kable(df) pib_medio pib_per_capita_medio 2659284 24827.14 # se eu quiser esse cálculo por uf (or cada umas das ufs?) # Aí é melhor aguprar por uf # ideia é: split by, apply (function), combine (summarise?) df &lt;- pib_cid %&gt;% group_by(sigla_uf) %&gt;% summarise(pib_medio = mean(pib_total), pib_per_capita_medio = mean(pib_per_capita)) %&gt;% head() kable(df) sigla_uf pib_medio pib_per_capita_medio AC 521542.3 11450.979 AL 365515.0 7931.148 AM 1339536.0 8975.314 AP 797717.9 14947.829 BA 491233.3 8814.841 CE 592590.0 7157.360 # agora, quero criar uma nova variável, que é o pib estadual df &lt;- pib_cid %&gt;% group_by(sigla_uf) %&gt;% mutate(pib_uf = sum(pib_total)) %&gt;% head() kable(df) ano codigo_regiao nome_regiao codigo_uf sigla_uf nome_uf cod_municipio nome_munic nome_metro codigo_meso nome_meso codigo_micro nome_micro codigo_reg_geo_imediata nome_reg_geo_imediata mun_reg_geo_imediata codigo_reg_geo_intermediaria nome_reg_geo_intermediaria mun_reg_geo_intermediaria codigo_concentracao_urbana nome_concentracao_urbana tipo_concentracao_urbana codigo_arranjo_populacional nome_arranjo_populacional hierarquia_urbana hierarquia_urbana_principais codigo_regiao_rural nome_regiao_rural regiao_rural_classificacao amazonia_legal semiarido cidade_de_sao_paulo vab_agropecuaria vab_industria vab_servicos_exclusivo vab_adm_publica vab_total impostos pib_total pib_per_capita atividade_vab1 atividade_vab2 atividade_vab3 pib_uf 2013 1 Norte 11 RO Rondônia 1100015 Alta Floresta D’Oeste NA 1102 Leste Rondoniense 11006 Cacoal 110005 Cacoal do Entorno 1102 Ji-Paraná do Entorno NA NA NA NA NA Centro Local Centro Local 1101 Região Rural da Capital Regional de Porto Velho Região Rural de Capital Regional Sim Não Não 110850.84 20336.994 73025.07 120335.59 324548.50 16776.193 341324.69 13266.66 Administração, defesa, educação e saúde públicas e seguridade social Pecuária, inclusive apoio à pecuária Demais serviços 31121413 2013 1 Norte 11 RO Rondônia 1100023 Ariquemes NA 1102 Leste Rondoniense 11003 Ariquemes 110002 Ariquemes Polo 1101 Porto Velho do Entorno NA NA NA NA NA Centro Sub-regional B Centro Sub-regional 1101 Região Rural da Capital Regional de Porto Velho Região Rural de Capital Regional Sim Não Não 93249.75 354733.212 694832.17 466732.80 1609547.93 190304.571 1799852.51 17772.99 Administração, defesa, educação e saúde públicas e seguridade social Demais serviços Comércio e reparação de veículos automotores e motocicletas 31121413 2013 1 Norte 11 RO Rondônia 1100031 Cabixi NA 1102 Leste Rondoniense 11008 Colorado do Oeste 110006 Vilhena do Entorno 1102 Ji-Paraná do Entorno NA NA NA NA NA Centro Local Centro Local 1101 Região Rural da Capital Regional de Porto Velho Região Rural de Capital Regional Sim Não Não 38259.43 3412.205 17787.45 32838.64 92297.72 4066.819 96364.54 14836.73 Administração, defesa, educação e saúde públicas e seguridade social Pecuária, inclusive apoio à pecuária Agricultura, inclusive apoio à agricultura e a pós colheita 31121413 2013 1 Norte 11 RO Rondônia 1100049 Cacoal NA 1102 Leste Rondoniense 11006 Cacoal 110005 Cacoal Polo 1102 Ji-Paraná do Entorno NA NA NA NA NA Centro Sub-regional B Centro Sub-regional 5105 Região Rural do Centro Sub-regional de Vilhena (RO) e Cacoal (RO) Região Rural de Centro Sub-regional Sim Não Não 140658.88 140288.317 599519.83 395842.23 1276309.26 156944.241 1433253.51 16692.33 Administração, defesa, educação e saúde públicas e seguridade social Demais serviços Comércio e reparação de veículos automotores e motocicletas 31121413 2013 1 Norte 11 RO Rondônia 1100056 Cerejeiras NA 1102 Leste Rondoniense 11008 Colorado do Oeste 110006 Vilhena do Entorno 1102 Ji-Paraná do Entorno NA NA NA NA NA Centro de Zona B Centro de Zona 1101 Região Rural da Capital Regional de Porto Velho Região Rural de Capital Regional Sim Não Não 45153.64 19889.818 149569.44 83089.55 297702.45 55567.232 353269.68 19581.49 Administração, defesa, educação e saúde públicas e seguridade social Comércio e reparação de veículos automotores e motocicletas Demais serviços 31121413 2013 1 Norte 11 RO Rondônia 1100064 Colorado do Oeste NA 1102 Leste Rondoniense 11008 Colorado do Oeste 110006 Vilhena do Entorno 1102 Ji-Paraná do Entorno NA NA NA NA NA Centro Local Centro Local 1101 Região Rural da Capital Regional de Porto Velho Região Rural de Capital Regional Sim Não Não 51029.59 23179.131 66099.41 86090.50 226398.63 16368.611 242767.24 12650.72 Administração, defesa, educação e saúde públicas e seguridade social Demais serviços Pecuária, inclusive apoio à pecuária 31121413 Coisas estranhas. O maior pib per capita municipal deu muito alto. vamos ver qual município é? Vamos filtrar e depois selecionar apenas algumas colunas # agora, quero criar uma nova variável, que é o pib estadual df &lt;- pib_cid %&gt;% filter(pib_per_capita &gt; 700000) %&gt;% select(sigla_uf, nome_munic, pib_per_capita, pib_total) %&gt;% head() kable(df) sigla_uf nome_munic pib_per_capita pib_total ES Presidente Kennedy 717343.7 7984035 Vamos entrar na Wiki do município ou perguntar pra chatGPT o que explica isso aí? Veremos que “faz sentido”, embora na verdade não faça. Exercício em sala de aula: veja os impostos desse município. 2.9 Visualização Para visualizarmos os dados com gráficos, utilizaremos a biblioteca ggplot2 # gráficos library(ggplot2) pib_cid %&gt;% ggplot(aes(y=pib_total, x=impostos)) + geom_point() A lgócia geral de um grtáfico com ggplot2 é como no exemplo acima. Primeiro passamos as variáveis por meio do comando ggplot, dentro de aes (de aesthetics), depois combinamos com o tipo de plot que queremos faze,r nesse caso, pontos, com geom_point. É possívle customizar o gráfico para ele ficar mais bonito. Vamos fazer isso agora. # gráficos mais bonitos pib_cid %&gt;% ggplot(aes(y=pib_total, x=impostos)) + geom_point() + scale_y_continuous(labels = scales::dollar) + theme_light() + theme(text=element_text(size=20)) + xlab(&quot;impostos municipais&quot;) + ggtitle(&quot;PIB municipal de 2013 x impostos municipais&quot;) Podemos usar vários temas feitos pela comunidade. Por exemplo, Barbie: # gráficos mais bonitos # install.packages(&quot;remotes&quot;) #remotes::install_github(&quot;MatthewBJane/theme_park&quot;) library(ThemePark) pib_cid %&gt;% ggplot(aes(y=pib_total, x=impostos)) + geom_point() + scale_y_continuous(labels = scales::dollar) + theme(text=element_text(size=20)) + theme_barbie() + xlab(&quot;impostos municipais&quot;) + ggtitle(&quot;PIB municipal de 2013 x impostos municipais&quot;) Vocês podem ver outros temas de filmes no github do autor do pacote: https://github.com/MatthewBJane/theme_park E, claro, há muito mais na internet. Para fazer outro tipo de gráfico, é só variar o geom. Por exemplo, um histograma do PIB per capita. # Histograma pib_cid %&gt;% ggplot(aes(x=pib_per_capita)) + geom_histogram() + theme_light() + theme(text=element_text(size=20)) + ggtitle(&quot;PIB per capita municipal&quot;) "],["revisão-de-estatística-e-probabilidade.html", "Capítulo 3 - Revisão de estatística e probabilidade 3.1 Variável Aleatória 3.2 Esperança matemática 3.3 Variância 3.4 Algebra com Esperança, Variância e Covariância 3.5 Distribuição de Probabilidade Conjunta 3.6 Pobabilidade Condicional 3.7 Esperança Condicional", " Capítulo 3 - Revisão de estatística e probabilidade A média de um conjunto de valores é dada pela soma dos valores, dividdo pelo número de observações. Matematicamente: \\(media = \\sum_{i=1}^n{x_i}/n\\) para observações \\(\\{ x_1, x_2, x_3, ..., x_n \\}\\) Em geral, as observações são uma amostra, e falamos de média amostral, \\(\\overline x\\). Ou seja: \\(\\overline x = \\sum_{i=1}^n{x_i}/n\\) Exercício 1. Vamos calcular, no R, a média das seguintes amostras: \\(\\{1,2,3,4,5,6,7,8,9,10\\}\\) \\(\\{5,5,5,5,5,5,5\\}\\) \\(\\{1,3,5,7,9,11\\}\\) \\(\\{-5,-4,-3,-2,-1,1,2,3,4,5\\}\\) Código no R x &lt;- c(1,2,3,4,5,6,7,8,9,10) (media_x &lt;- sum(x)/length(x)) ## [1] 5.5 x &lt;- c(5,5,5,5,5,5,5) (media_x &lt;- sum(x)/length(x)) ## [1] 5 x &lt;- c(1,3,5,7,9,11) (media_x &lt;- sum(x)/length(x)) ## [1] 6 x &lt;- c(-5,-4,-3,-2,-1,1,2,3,4,5) (media_x &lt;- sum(x)/length(x)) ## [1] 0 # ou podemos simplemsnte usar mean(x) mean(x) ## [1] 0 3.1 Variável Aleatória Uma variável aleatória (v.a.) mede numericamente resultad0s de eventos aleatórios. dPor exemplo, um dado de \\(6\\) faces possui um espaço amostral de eventos possíveis dados pelos números \\(\\{1,2,3,4,5,6\\}\\). Ao atribuirmos uma probabilidade a cada resultado possível do espaço amostral por exemplo, \\(1/6\\), temos uma distribuição de probabilidade. Variáveis aleatórias podem ser discretas ou contínuas. Uma v.a. discreta pode assumir um número finito (contável) de valores. Já uma contínua pode assumir infinitos (não-contáveis) valores. Um conjunto é contável se ele for finito ou se puder ser estabelecida uma correspondência um para um com o conjunto (infinito) dos números naturais. 3.2 Esperança matemática A esperança de uma variável aleatória discreta \\(X\\), cuja probabilidade de massa de \\(x \\in X\\) é dada por \\(p(x)\\), é definida por: \\(\\sum(x*p(x))\\). A esperança de uma v.a. contínua X, cuja densidade é \\(f(x)\\), é definida por: \\(\\int f(x)*x\\,dx\\). Similarmente, para a mesma v.a. X acima, a esperança de uma função \\(h(X)\\) é dada por \\(\\int f(x)*h(x)\\,dx\\) e analogamente para o caso discreto. 3.3 Variância A variância de uma variável aleatória \\(X\\) é dada por: Definição 1. \\(Var(X) = \\mathbb{E}[(X - \\mathbb{E}[X])^2]\\). A Covariância de duas v.a. \\(X\\) e \\(Y\\) é definida como: \\(Cov(X,Y) = \\mathbb{E}[(X - \\mathbb{E}[X])*(Y - \\mathbb{E}[Y])]\\). Notem que \\(Cov(X,X) = Var(X)\\). A covariância é positiva quando ambos X e Y tendem a ter valores acima (ou abaixo) de sua média simultaneamente, enquanto ela é negativa quando uma v.a. tende a ter valores acima da sua média e a outra abaixo. 3.4 Algebra com Esperança, Variância e Covariância Sejam \\(a\\) e \\(b\\) constantes. Linearidade da Esperença \\(\\mathbb{E}[aX + bY] =\\mathbb{E}[aX] + \\mathbb{E}[by] = a*\\mathbb{E}[X] + b*\\mathbb{E}[Y]\\) Exercício: verifique, com exemplos, que isso é verdade. Identidade da variância \\(Var(X) = \\mathbb{E}[(X - \\mathbb{E}[X])^2] = \\mathbb{E}[X^2] - \\mathbb{E}^2[X]\\) A prova será demonstrada mais adiante. Identidade da Covariância \\(Cov(X,Y) = \\mathbb{E}[X*Y] - \\mathbb{E}[X]*\\mathbb{E}[Y] = \\mathbb{E}[(X - \\mathbb{E}[X])*(Y - \\mathbb{E}[Y])]\\) Exercício para o leitor. Prove que isso é verdade. 4, Covariância é simétrica \\(Cov(X,Y) = Cov(Y,X)\\) Variância não é linear \\(Var(a*X + b) = a^2*Var(x)\\) Covariância não é linear \\(Cov(a*X + b,Y) = a*Cov(Y,X)\\) # Prova da identidade da variância Vamos mostrar que \\(\\mathbb{E}[(X - \\mathbb{E}[X])^2] = \\mathbb{E}[X^2] - \\mathbb{E}^2[X]\\) Começamos expandido o quadrado da esperança: \\(Var(X) = \\mathbb{E}[(X - \\mathbb{E}[X])^2] = \\mathbb{E}[(X - \\mathbb{E}[X]) * (X - \\mathbb{E}[X])]\\). Aplicando a regra do quadrado, temos: \\(\\mathbb{E}[(X - \\mathbb{E}[X]) * (X - \\mathbb{E}[X])] = \\mathbb{E}[(X^2 - 2* \\mathbb{E}[X]*X + \\mathbb{E}[X]^2)]\\) Pela propriedade da experança, sabemos que, sejam \\(A\\) e \\(B\\) duas v.a. independentes, então \\(\\mathbb{E}[A + B] = \\mathbb{E}[A] + \\mathbb{E}[B]\\). Então: \\(Var(X) = \\mathbb{E}[X^2] - \\mathbb{E}[2*\\mathbb{E}[X]*X] + \\mathbb{E}[\\mathbb{E}[X]^2]]\\) Outra propriedade da esperança é que, seja \\(a\\) uma constante e \\(X\\) uma v.a., então \\(\\mathbb{E}[a*X] = a*\\mathbb{E}[X]\\). \\(Var(X) = \\mathbb{E}[X^2] - 2*\\mathbb{E}[\\mathbb{E}[X]*X] + \\mathbb{E}[\\mathbb{E}[X]^2]]\\) Nós sabemos que \\(\\mathbb{E}[X]\\) é uma constante (é uma média da v.a.). E a média de uma constante é a própria constante. Portanto, \\(\\mathbb{E}[\\mathbb{E}[X]] = \\mathbb{E}[X]\\). E usaremos também que \\(\\mathbb{E}[a*X] = a*\\mathbb{E}[X]\\) e, por fim, o fato de que uma constante ao quadrado é em si uma constante. \\(Var(X) = \\mathbb{E}[X^2] - 2*\\mathbb{E}[X] * \\mathbb{E}[\\mathbb{E}[X]] + \\mathbb{E}[X]^2]\\) \\(Var(X) = \\mathbb{E}[X^2] - 2*\\mathbb{E}[X] * \\mathbb{E}[X] + \\mathbb{E}[X]^2]\\) \\(Var(X) = \\mathbb{E}[X^2] - 2*\\mathbb{E}[X]^2 + \\mathbb{E}[X]^2]\\) \\(Var(X) = \\mathbb{E}[X^2] - \\mathbb{E}[X]^2\\) Como Queriamos Demonstrar (CQD). 3.5 Distribuição de Probabilidade Conjunta A Distribuição de probabilidade conjunta de \\(X\\) e \\(Y\\) (definida no mesmo espaço de probabilidade) é uma distribuição de probabilidade dos pares \\((x,y)\\) e descreve como os valores de \\(X\\) e \\(Y\\) variam conjuntamente. Cada uma das distribuições \\(X\\) e \\(Y\\) sozinhas são chamadas de distribuições marginais. Uma distribuição conjunta é como uma máquina que, de acordo com certas regras de probabilidade, retorna dois pares de valores. ex. 1. Roleta. Em um casino, um jogo comum é a roleta. Ela consiste normalmente de 32 números (0 a 31), e cada número tem uma cor (preto, vermelho ou verde). Ao girar a roleta, ela solta um número e uma cor. Portanto, podemos pensar que a roleta é uma distribuição conjunta de duas variáveis (números e cores). Ex. 2.: Considere um dado de 4 faces \\(( 1, 2, 3, 4 )\\). Seja \\(X\\) a soma dos números dos dois dados, e \\(Y\\) o maior valor dos dois dados. O espaço amostral é dado pela tabela abaixo. library(knitr) library(dplyr) library(kableExtra) #Definir o espaço amostral espaco_amostral &lt;- expand.grid(1:4, 1:4) espaco_amostral$X &lt;- espaco_amostral$Var1 + espaco_amostral$Var2 espaco_amostral$Y &lt;- pmax(espaco_amostral$Var1, espaco_amostral$Var2) # Criar a tabela kable(espaco_amostral, col.names = c(&quot;resultado do primeiro dado&quot;, &quot;resultado do segundo dado&quot;, &quot;X&quot;, &quot;Y&quot;), caption = &quot;Tabela representando a soma (X) and o maior valor (Y) do lançamento de dois dados de quatro lados&quot;) %&gt;% kable_styling(bootstrap_options = &quot;striped&quot;) Table 3.1: Tabela representando a soma (X) and o maior valor (Y) do lançamento de dois dados de quatro lados resultado do primeiro dado resultado do segundo dado X Y 1 1 2 1 2 1 3 2 3 1 4 3 4 1 5 4 1 2 3 2 2 2 4 2 3 2 5 3 4 2 6 4 1 3 4 3 2 3 5 3 3 3 6 3 4 3 7 4 1 4 5 4 2 4 6 4 3 4 7 4 4 4 8 4 Se supusermos que todos os números possuem a mesma chance de sair quando jogamos os dados, então, a distribuição conjunta de \\(X\\) e \\(Y\\) pode ser dada por: # Definir os valores de (x, y) e P(X = x, Y = y) valores &lt;- c(&quot;(2, 1)&quot;, &quot;(3, 2)&quot;, &quot;(4, 2)&quot;, &quot;(4, 3)&quot;, &quot;(5, 3)&quot;, &quot;(5, 4)&quot;, &quot;(6, 3)&quot;, &quot;(6, 4)&quot;, &quot;(7, 4)&quot;, &quot;(8, 4)&quot;) probabilidades &lt;- c(0.0625, 0.1250, 0.0625, 0.1250, 0.1250, 0.1250, 0.0625, 0.1250, 0.1250, 0.0625) # Criar a tabela tabela &lt;- data.frame(&quot;(x, y)&quot; = valores, &quot;P(X = x, Y = y)&quot; = probabilidades) # Formatar a tabela kable(tabela, caption = &quot;Tabela representando a distribuição conjunta da soma (X) e o maior (Y) de dois lançamentos de um dado de quatro faces&quot;, col.names = c(&quot;$(X,Y)$&quot;, &quot;$P(X=x, Y=y)$&quot;)) %&gt;% kable_styling(bootstrap_options = &quot;striped&quot;) Table 3.2: Tabela representando a distribuição conjunta da soma (X) e o maior (Y) de dois lançamentos de um dado de quatro faces \\((X,Y)\\) \\(P(X=x, Y=y)\\) (2, 1) 0.0625 (3, 2) 0.1250 (4, 2) 0.0625 (4, 3) 0.1250 (5, 3) 0.1250 (5, 4) 0.1250 (6, 3) 0.0625 (6, 4) 0.1250 (7, 4) 0.1250 (8, 4) 0.0625 3.6 Pobabilidade Condicional Nós vimos o que é uma distribuição de probabilidade conjunta. É comum que nós tenhamos apenas uma observação parcial sobre uma das variáveis. Digamos, que \\(y = 2\\). De posse dessa informação, como podemos atualizar nossa tabela de probabilidades? Se o maior valor foi \\(2\\), então a soma dos dados \\(X\\) só pode ser 3 ou 4. Se a soma for 3, temos algo como \\((1,2)\\) ou \\((2,1)\\). Se a soma for 4, então deve ter saído \\((2,2)\\). Esse é o novo espaço amostral dado que \\(y=2\\).Todos os outros números não podem ocorrer e, portanto, possuem probabilidade zero. Logo, \\(P(X=1|Y=2) = 2/3\\) e \\(P(X=2|Y=2) = 1/3\\). Isso é o que chamamos de probabilidade condicional. No caso, a probabilidade condicional de \\(X\\), dado \\(Y=y\\). A probabilidade condicional é em si uma distribuição de probabilidade. Do mesmo jeito que temos as distribuições de probabilidade de \\(X\\), de \\(Y\\) e de \\(X,Y\\), também temos a de \\(X|y=y\\) (e, claro, a de \\(Y|X=x\\)). A tabela abaixo apresenta essa distribuição de probabilidade condicional para o caso de \\(Y=2\\). # Definir os valores de (x, y) e P(X = x, Y = y) valores &lt;- c(&quot;(2, 1)&quot;, &quot;(3, 2)&quot;, &quot;(4, 2)&quot;, &quot;(4, 3)&quot;, &quot;(5, 3)&quot;, &quot;(5, 4)&quot;, &quot;(6, 3)&quot;, &quot;(6, 4)&quot;, &quot;(7, 4)&quot;, &quot;(8, 4)&quot;) probabilidades &lt;- c(0, 2/3, 1/3, 0, 0, 0, 0, 0, 0, 0) # Criar a tabela tabela &lt;- data.frame(&quot;(x, y)&quot; = valores, &quot;P(X = x| Y = 2)&quot; = probabilidades) # Formatar a tabela kable(tabela, caption = &quot;Tabela representando a distribuição conjunta da soma (X) e o maior (Y) de dois lançamentos de um dado de quatro faces&quot;, col.names = c(&quot;$(X,Y)$&quot;, &quot;$P(X=x| Y=2)$&quot;)) %&gt;% kable_styling(bootstrap_options = &quot;striped&quot;) Table 3.3: Tabela representando a distribuição conjunta da soma (X) e o maior (Y) de dois lançamentos de um dado de quatro faces \\((X,Y)\\) \\(P(X=x| Y=2)\\) (2, 1) 0.0000000 (3, 2) 0.6666667 (4, 2) 0.3333333 (4, 3) 0.0000000 (5, 3) 0.0000000 (5, 4) 0.0000000 (6, 3) 0.0000000 (6, 4) 0.0000000 (7, 4) 0.0000000 (8, 4) 0.0000000 3.7 Esperança Condicional Se a probabilidade condicional é uma distribuição de probabilidade no seu próprio direito, então podemos calcular a esperança dessa distribuição, exatamente como fazíamos antes. No caso, falamos de esperança condicional. Qual o valor médio de \\(X\\), quando \\(Y=2\\)? \\(3*(2/3) + 4*(1/3)\\) \\(2 + 4/3 = 6/3 + 4/3 = 10/3 = 3.333\\) A notação matemática para a esperança condicional, nesse caso, é: \\(E[X|Y=2]\\). "],["introdução-à-simulação.html", "Capítulo 4 Introdução à Simulação 4.1 Configuração 4.2 Simular 4.3 Resumir 4.4 Análise de sensibilidade 4.5 Aplicação de simulação", " Capítulo 4 Introdução à Simulação Simulação é uma ferramenta extremamente importante em probabilidade e estatística. Dois são seus principais usos. De um lado, é possível usar um modelo de probabilidade para recriar, artificialmente, no computador, amostras desse modelo, quantas vezes quisermos. Tudo se passa como se tivéssemos o poder de criar um mundo, totalmente controlado por nós, e podemos simular esse mundo e verificar o que acontece. Isso é útil quando não podemos ou não queremos verificar matematicamente o que deveria acontecer com base no modelo matemático de probabilidade. De outro lado, é possível usar simulações para aproximar quantidades matemáticas que não podemos ou não queremos calcular analiticamente. Por calcular analiticamente, é fazendo a conta nós mesmos. Um exemplo simples disso é, em vez de calcular os resultados de alguma análise combinatória usando as fórmulas de combinação e simularmos os valores e contar quantas combinações resultam. No geral usamos simulação para o primeiro tipo de uso. Mas, aqui no curso, será útil que vocês verifiquem resultados matemáticos por meio de simulações. Assim, se escrevemos que \\(\\sum_{i=1}^{10} i = 55\\), vocês podem verificar com código no R essa soma, por exemplo, rodando: sum(1:10). Sempre que vocês tiverem dúvida de algum passo matemático, podem fazer uma simulação para entender melhor o que está acontecendo. Isso é encorajado, para melhorar a intuição do que está acontecendo e lhe assegurar que de fato você está entendendo o que está acontecendo. Podemos usar simulações para aproximar quantidades. Um exemplo clássico em probabilidade é a fórmula de Stirling, dada por \\(n! \\sim \\sqrt{2\\pi}n^{n + 1/2}*e^{-n}\\). Então, por exemplo, \\(10! = 10*9*8*7*6*5*4*3*2*1\\) com os computadores modernos pode ser calculada diretamente para números não tão grandes. No caso, \\(10! = 3628800\\). Ou pode ser aproximada pela fórmula de Stirling: Exemplo 4.1 # especificando semente, para simulação ser reproduzível set.seed(2) # número de amostras stirling_aprox &lt;- function(n) { sqrt(2*pi)*n^(n+1/2)*exp(-n) } print(stirling_aprox(10)) [1] 3598696 # razão da aproximação para o valor correto stirling_aprox(10)/3628800 [1] 0.991704 # erro percentual 1 - stirling_aprox(10)/3628800 # 0,8% [1] 0.00829596 Como se vê, a fórmula de Stirling é bem precisa para aproximar fatorial e muito mais fácil de computar. Dito isso, voltemos ao primeiro caso, de simulação de modelos de probabilidade. Comecemos lembrando que probabilidades podem ser interpretadas como frequências relativas de longo prazo. Portanto, a probabilidade de um evento pode ser aproximada por simulações, na medida em que aproximamos a frequência relativa com que o fenômeno acontece em nossas simulações. Vamos dar um exemplo simples desse tipo de aplicação. Suponha que quero simular a probabilidade do número 6 sair em um dado de seis lados. Exemplo 4.1.1 # especificando semente, para simulação ser reproduzível set.seed(234) # número de amostras n &lt;- 10000 # 1000 amostras de uma lançamento de dado de 6 lados resultado &lt;- sample(1:6, n, TRUE) # frequência relativade 6 é dada por número de 6 / total de amostras prob_6 &lt;- sum(resultado == 6)/n # 16,89% # 1/6 = 16.6666 Podemos também ver como a aproximação converge para o verdadeiro valor à medida que \\(n\\) cresce. # especificando semente, para simulação ser reproduzível set.seed(234) # número de amostras vec_amostra &lt;- c(100, 1000, 10000, 100000, 1000000) # lista vazia para armazenar os resultados das simulações resultado_lista &lt;- list() # vetor vazio para armazenar a frequência relativa de 6 vec_prob6 &lt;- numeric() set.seed(234) # loop sobre os tamanhos das amostrar for ( i in 1:length(vec_amostra)) { # n amostras de uma lançamento de dado de 6 lados resultado_lista[[i]] &lt;- sample(1:6, vec_amostra[i], TRUE) # frequência relativade 6 é dada por número de 6 / total de amostras vec_prob6[i] &lt;- sum(resultado_lista[[i]] == 6)/vec_amostra[i] } print(vec_prob6) [1] 0.150000 0.189000 0.164700 0.169250 0.166257 Se supusermos que todos os números possuem a mesma chance de sair quando jogamos os dados, então, a distribuição conjunta de \\(X\\) e \\(Y\\) pode ser dada por: library(knitr) library(kableExtra) # Definir os valores de (x, y) e P(X = x, Y = y) valores &lt;- c(&quot;(2, 1)&quot;, &quot;(3, 2)&quot;, &quot;(4, 2)&quot;, &quot;(4, 3)&quot;, &quot;(5, 3)&quot;, &quot;(5, 4)&quot;, &quot;(6, 3)&quot;, &quot;(6, 4)&quot;, &quot;(7, 4)&quot;, &quot;(8, 4)&quot;) probabilidades &lt;- c(0.0625, 0.1250, 0.0625, 0.1250, 0.1250, 0.1250, 0.0625, 0.1250, 0.1250, 0.0625) # Criar a tabela tabela &lt;- data.frame(&quot;(x, y)&quot; = valores, &quot;P(X = x, Y = y)&quot; = probabilidades) # Formatar a tabela kable(tabela, caption = &quot;Tabela 2.26: Tabela representando a distribuição conjunta da soma (X) e o maior (Y) de dois lançamentos de um dado de quatro faces&quot;, col.names = c(&quot;$(X,Y)$&quot;, &quot;$P(X=x, Y=y)$&quot;)) %&gt;% kable_styling(bootstrap_options = &quot;striped&quot;) Table 4.1: Tabela 2.26: Tabela representando a distribuição conjunta da soma (X) e o maior (Y) de dois lançamentos de um dado de quatro faces \\((X,Y)\\) \\(P(X=x, Y=y)\\) (2, 1) 0.0625 (3, 2) 0.1250 (4, 2) 0.0625 (4, 3) 0.1250 (5, 3) 0.1250 (5, 4) 0.1250 (6, 3) 0.0625 (6, 4) 0.1250 (7, 4) 0.1250 (8, 4) 0.0625 como podemos ver, à medida que \\(n\\) cresce, a simulação converge para o verdadeiro valor do parâmetro, embora isso não seja linear. Por isso que é importante variar a configuração para ter certeza de que a simulação está próxima do verdadeiro valor do parâmetro. De maneira geral, uma simulação envolve os seguintes passos. 4.1 Configuração Definir o modelo de probabilidade, variáveis aleatórias e eventos a serem modelados. O modelo de probabilidade codifica todas as suposições do modelo. No exemplo acima, de que todos os lados têm a mesma probabilidade de sair, que existem apenas seis possibilidades, numeradas de \\(1-6\\) e assim por diante. 4.2 Simular Vamos considerar o exemplo dos dados novamente. Vamos usar a função sample para simular lançamento de um dado de quatro faces. Para “jogar o dado” uma vez, sorteio um número entre 1 e 4. X &lt;- sample(1:4, size=1) Como quero a frequência de longo prazo, preciso repetir esse processo (de maneira independente a cada jogada) \\(n\\) vezes. # número de jogadas/simulações n &lt;- 1000 # vetor X, para armazenar o resultado de cada uma das n jogadas X &lt;- numeric() # simulando n vezes for( i in 1:n){ X[i] &lt;- sample(1:4, size=1) } # visualizando as primeiras 20 jogadas head(X, 20) [1] 4 4 1 4 2 3 2 4 2 2 3 4 4 1 4 2 4 2 3 3 4.3 Resumir Após realizada a simulação, queremos não olhar todos os números simulados, mas resumir a simulação. Por exemplo, obtendo a minha distribuição de probabilidade, ou a esperança. # prob X = 1 sum(X==1)/n [1] 0.262 # prob X = 2 sum(X==2)/n [1] 0.26 # prob X = 3 sum(X==3)/n [1] 0.232 # prob X = 2 sum(X==4)/n [1] 0.246 ## resumo geral summary(X) Min. 1st Qu. Median Mean 3rd Qu. Max. 1.000 1.000 2.000 2.462 3.000 4.000 4.4 Análise de sensibilidade Como estamos simulando e aproximando com um \\(n\\) finito o que deveria ser infinito, temos de saber se nossa aproximação é boa. Se sabemos a verdade, podemos calcular como o erro se comporta com o \\(n\\) que escolhemos, ou ver como muda à medida que o \\(n\\) cresce. Vamos fazer isso. # número de jogadas/simulações n &lt;- 1000 # vetor X, para armazenar o resultado de cada uma das n jogadas X &lt;- numeric() # número de replicações da simulação k &lt;- 100 # vetor para armazenar o erro medio erro_medio &lt;- numeric() # simulando n vezes for (j in 1:k) { for( i in 1:n){ X[i] &lt;- sample(1:4, size=1) } p1 &lt;- sum(X==1)/n p2 &lt;- sum(X==2)/n p3 &lt;- sum(X==3)/n p4 &lt;- sum(X==4)/n erro_medio[j] = (abs(p1 - .25) + abs(p2 - .25) + abs(p3 - .25) + abs(p3 - .25)) /4 } summary(erro_medio) Min. 1st Qu. Median Mean 3rd Qu. Max. 0.00225 0.00675 0.00950 0.01080 0.01381 0.02725 Podemos ver que o erro médio é da ordem de menosde 3pp no pior dos casos. Exercício. Verifiquem como o erro médio muda com \\(n = 100\\), \\(n = 1000\\) e \\(n = 10000\\). 4.5 Aplicação de simulação Um dos resultados mais contraintuitivos de modelos de probabilidade está relacionado à chamada “falácia do jogador”. A falácia é a crença de que, porque saiu um número par (ou preto) na última jogada em uma roleta no cassino, então aumenta a chance de sair ímpar (vermelho) na próxima rodada, para que os números se equilibrem. Porém, cada jogada é independente da anterior. Um problema relacionado é o chamado problema da ruína do jogador. Exemplo 1.2. Ballot theorem (toerema da urna de votação? teorema da votação?) Esse teorema, provado em 1878 por W. A. Whitworth e independentemente em 1887 por J. Bertrand estabelece a probabilidade de que, durante a apuração dos votos em uma urna, sempre haja mais votos contabilizados para o candidato \\(P\\) do que para o candidato \\(Q\\), quando \\(P\\) possui \\(p\\) votos e \\(Q\\) possui \\(q\\) votos, com \\(p &gt; q\\). A fórmula para essa probabilidade é \\((p-q)/(p+q)\\). Vamos verificar esse teorema por meio de simulação? Configuração: suponha que \\(p = .51\\) e \\(q = .49\\). O que sigifnica que é \\((p-q)/(p+q) = .02/1 = 2\\%\\) Primeiro, faremos uma simulação, e depois repetiremos k vezes a simulação para obter a frequência relativa. set.seed(234) # número de votos na urna n = 1000 # 1 é o voto para P, -1 o voto para Q. # então geramos n votos e armazenamos em votos # vamos usar rbinom, que gera 0 ou 1. votos &lt;- rbinom(n, 1, p=.51) # transformado 0 em -1 votos &lt;- ifelse(votos == 0, -1, votos) # Agora, iremos contar o primeiro voto e registrar para quem vai. Depois o segundo e o terceiro etc. até não sobrarem votos a serem apurados. Vamso guardar essa sequência em um objeto apuração. apuracao &lt;- numeric() # em seguida, contamos se em algum momento Q está na frente. Se estiver, então não aconteceu de P liderar toda a apuração. contagem_lideranca &lt;- numeric() for ( i in 1:n) { apuracao[i] &lt;- votos[i] contagem_lideranca[i] &lt;- sum(apuracao) } # se a soma dos votos for positiva, tivemos mais votos 1 que -1. Se for negativa ou igual a zero, P não esteve liderando (esteve atrás ou empatando) # então, basta contar quantos casos aconteceram da soma dos votos apurados é menor ou igual a zero. if (sum(contagem_lideranca &lt;= 0)) print(&quot;p não liderou sempre&quot;) [1] “p não liderou sempre” #agora, vamos fazer a simulação mil vezes. Esperamos que aprox. 2% das vezes P lidere sempre. # simulando k = 1000 vezes apuracao &lt;- numeric() contagem_lideranca &lt;- numeric() freq_lideranca &lt;- numeric() # armazanear se liderou ou não em cada sim k for (k in 1:1000) { for ( i in 1:n) { apuracao[i] &lt;- votos[i] contagem_lideranca[i] &lt;- sum(apuracao) } freq_lideranca[k] &lt;- as.numeric(sum(contagem_lideranca &lt;= 0)) if( k%% 50 == 0) print(k) # para vermos a evolução da simulação a cada 50 passos } [1] 50 [1] 100 [1] 150 [1] 200 [1] 250 [1] 300 [1] 350 [1] 400 [1] 450 [1] 500 [1] 550 [1] 600 [1] 650 [1] 700 [1] 750 [1] 800 [1] 850 [1] 900 [1] 950 [1] 1000 # se a soma dos votos for positiva, tivemos mais votos 1 que -1. Se for negativa ou igual a zero, P não esteve liderando (esteve atrás ou empatando) # então, basta contar quantos casos aconteceram da soma dos votos apurados é menor ou igual a zero. print(sum(freq_lideranca)/k) [1] 0.281 # 0,281 ou 2,8%. Próximo dovalor verdadeiro de 2%. Se aumentarmos k, irá convergir para o valor verdadeiro. Suponha agora um problema diferente. Um dos dois candidatos vai ter a primeira liderança. Após \\(n\\) votos apurados, qual a probabilidade de a contagem ter empatado pela primeira vez? Dito de outro modo, após uma das candidatas assumir a liderança, quanto “tempo” (isto é, após quantos votos apurados) em média devemos esperar até que uma reversão de liderança ocorra (ou pelo menos um empate)? A intuição que as pessoas têm, talvez a partir da lei dos grandes números, é que deveríamos esperar que, se \\(p = q = 1/2\\), ambas candidatas deveriam liderar por 50% do tempo em uma apuração suficientemente longa (com \\(n\\) votos grande) e deveria haver troca constante de liderança, em vez de uma candidata liderar po um longo tempo. Essa intuição é errada, como iremos mostrar agora por simulação. A leitora interessada na demonstração matemática utilizando apenas análise combinatória (portanto, apenas estatística básica) deve consultar o clássico livro de Feller (1968). Aqui, faremos uma simulação para mostrar isso. "],["o-modelo-de-regressão.html", "Capítulo 5 O Modelo de Regressão 5.1 Modelo de Regressão - Teoria 5.2 Esperança Condicional 5.3 CEF 5.4 Objetivos de aprendizagem ao final do capítulo 5.5 Distribuição dos salários 5.6 Distribuição Condicional dos salários 5.7 Logaritmos 5.8 Gênero e Raça 5.9 Prevendo a partir da esperança condicional", " Capítulo 5 O Modelo de Regressão “Prediction is hard. Specially about the future”. Fonte desconhecida. 5.1 Modelo de Regressão - Teoria O modelo de regressão (não confundir com regressão linear) é uma forma bem ampla de modelar os dados para prever uma variável de interesse, usualmente designada pela letra \\(Y\\). Se eu quero prever os votos de candidatas em uma eleição, a votação de cada candidata é minha variável de interesse, \\(Y\\). Digamos que eu tenho uma amostra da intenção de votos das candidatas, obtidas por meio de uma pesquisa eleitoral. Então, a regressão é uma forma de modelar os dados para prever justamente essa variável \\(Y\\). Há muitas formas de apresentar ou motivar regressão linear. O método mais tradicional é pensar que a regressão linear é uma reta que é ajustada aos pontos observados. Porém, não tomaremos esse caminho aqui. Nós iremos tomar aqui o caminho de considerar que a regressão linear é uma forma de aproximar a chamada “Conditional Regression Function” (CEF, na sigla em inglês). O objetivo é entender “as far as possible with the available data how the conditional distribution of some response y varies across subpopulations determined by the possible values of the predictor or predictors” (Cook and Weisberg, apud Berk, p. 4). Portanto, vamos retomrar o conceito de esperança condicional, para introduzir em seguida a função de regressão condicional (CEF, em inglês) e então como a regressão pode ser pensada como uma forma de aproximar a CEF. 5.2 Esperança Condicional Uma das primeiras distinções que temos de fazer é sobre previsão e explicação (causal). Quando queremos prever, estamos interessados em saber quais os provaveis valores de variáveis no futuro, a parte de informações sobre a própria variável e outras no passado. Nesse sentido, é preciso algum tipo de suposição de que o futuro se assemelha ao passado de algum modo. Esse tipo de suposição usualmente toma a forma de um modelo probabilísitco, mas não apenas. Quando estamos interessados em explicações causais, temos dois tipos de perguntas de pesquisa possíveis. Uma sobre a chamada causa dos efeitos e outra sobre o efeito das causas (Gelman &amp; Imbens, 2013). A causa dos efeitos são perguntas do tipo: o que causal a II Grande Guerra? Ou qual a causa da eleição de Trump ou Bolsonaro? O que explica a desigualdade de renda no Brasil? São todas perguntas em que queremos explicar um efeito, isto é, identificar as causas de um fenômeno (efeito). Já o efeito das causas ão perguntas do tipo: qual o efeito da vacina de covid-19 sobre a mortalidade por Covid-19? Qual o efeito de checagem de notícias sobre a crença de pessoas em desinformação? Qual o efeito da magnitude eleitoral sobre fragmentação partidária? E assim por diante. Aqui, estamos interessados em entender o efeito causal de uma variável sobre outra, sem pretender esgotar todas as explicações de causa possíveis. A maior parte dos métodos quantitativos existentes são bons para responder perguntas de previsão e de causa dos efeitos. Grosso modo, não há método quantitativo para estimação do efeito das causas, exceto realizar uma série de estudos independentes sobre várias causas dos efeitos, olhando uma causa distinta do mesmo efeito por vez e esperar que isso gere um conhecimento combinado sobre essas múltiplas causas. Mas não há, contudo, uma metodologia bem definida de como combinar esses estudos independentes em um único conhecimento do efeito conjunto das causas. Assim, nosso curso será dedicado apenas a modelos de previsão e modelos de causa dos efeitos, que é o que temos de metodologias já desenvolvidas e consolidadas. Começamos por essa explicação porque uma perspectiva mais antiga, e ainda comum nas ciências sociais, é que modelos de regressão múltiplas permitem estimar o efeito de várias causas. Isso raramente é o caso e não adotaremos essa perspecitva aqui 1. 5.3 CEF O que consgtitui uma boa previsão? Tradicionalmente, empregamos a noção de Erro Quadrátco Médio (EQM) para quantificar boas previsões. Quanto menor o EQM, melhor uma previsão. Se o objetivo é, portanto, fazer previsões que minimizem o EQM, iremos apresertar e mostrar que a Função de Esperança Condicional (CEF, na sigla em inglês) é o melhor preditor global possível. Vamos dizer em outras palavras, porque esse resultado é verdadeiramente icnrível. A CEF é o melhor preditor possível dentre todos que existam ou possam vir a existir, entendendo melhor como ter o menor EQM. Por isso que a CEF é o ponto de partida de qualquer preditor que exista, seja uma regressão simples ou algoritmos de aprendizens de máquinas como “random forest” ou mesmo algorítimos de deep learning de redes neurais por traz dos recentes avanços na inteligência artificial. Mesmo os algorítmos mais avançados de inteleigência artificial, como os Large Language Models, que estão na base de ferramentas como ChatGPT, não podem ter desempenho melhor que a função de experança condicional, CEF, ao fazer uma previsão. Naturalmente, se esse é o caso, a próxima pergunta que todos nós iremos fazer é: por que não aprender apenas a usar a CEF, que é o melhor preditor possível, e ser feliz para sempre? Porque a natureza não nos diz qual é a CEF. Nós nunca sabemos qual a verdadeira função de esperança condicional. Então tentamos aproximar o melhor possível a CEF, a partir de simplificações da realidade. Em particular, nosso curso pode ser pensado em torno das seguintes perguntas: como aproximar a CEF por meio de regressão linear (combinação lineares de preditores)? Quais as propridades dessa aproximação? Em que condições ela é uma boa aproximação e em que sentido (quantitativo e preciso) podemos falar de boa aproximação? Mais para o final do curso faremos a conexão entre a CEF, modelos preditivos e modelos causais. 5.4 Objetivos de aprendizagem ao final do capítulo Estudantes deverão ter aprendido ao final do capítulo: Reconhecer que a função de esperança condicional, a CEF, em sua forma pura, é o melhor preditor possível para uma variável alvo, dadas as informações de outras variáveis. Memorizar que todos os outros predidores, sejam lineares ou não-lineares, incluindo preditores de deep learning, são tentativas de aproximar a CEF. Apreciar que o melhor predito linear (isto é, considerando apenas preditores que incluem combinação lineares de variáveis) produz previsões razoáveis, e antecipar que esse preditor nosleva à regressão linear. 5.5 Distribuição dos salários Vamos considerar os salários de brasileiros. A amostra da PNAD contínua é uma boa forma de analisarmos como é a distribuição de salários. Vamos supor que conseguimos obter uma amostra aleatória dos salários (o que não é exatamente o caso na PNAD contínua, mas vamso abstrair isso aqui para simplificar). Os gráficos 5.1 e 5.2 apresentam a distribuição dos salários e do logaritmo dos salários no Brasil, a partir de dados da PNAD para 2017. library(ggplot2) df &lt;- data %&gt;% filter(!is.na(renda)) %&gt;% filter(!is.na(horas_trabalhadas)) %&gt;% filter(renda &gt; 0) %&gt;% filter(horas_trabalhadas &gt; 0) %&gt;% mutate(salario = renda/(4.5*horas_trabalhadas)) %&gt;% mutate(log_salario = log(salario)) %&gt;% mutate(genero = as.character(genero)) p1 &lt;- df %&gt;% ggplot(aes(salario)) + geom_density(aes(weight=V1028)) + theme_bw(base_size = 22) print(p1) Figure 5.1: Distribuição dos salários p2 &lt;- df %&gt;% ggplot(aes(log_salario)) + geom_density(aes(weight=V1028)) + theme_bw(base_size = 22) print(p2) ## Warning in density.default(x, weights = w, bw = bw, adjust = adjust, kernel = kernel, : Selecting bandwidth *not* using &#39;weights&#39; Figure 5.2: Distribuição do log dos salários Nós vemos que a distribuição dos salários por hora é tão concentrada em torno do zero, com uma cauda longo, que é difícil visualizar os dados direito. Nesse e em outros casos, usar o logaritmo natural é uma boa alternativa. E nós vemos que o salário por hora está concentrado um pouco antes do \\(4\\), que quer dizer 54.6 reais por hora. De fato, podemos calcular a esperança \\(E[log(salário)]\\), dada por 2.15. Ou podemos computar diretamente, \\(E[salário]\\) 14 reais. 5.6 Distribuição Condicional dos salários Nós sabemos que os saládios são distribuídos desigulamente entre subpopulações, como por exemplo, entre trabalhadores homens e trabalhadoras mulheres. Os valores 2.18 e 2.1 são o logaritmo do salário na subpopulação de homens e mulheres. Como nós vimos, podemos dizer que temos a esperança condicional do salário, dado (ou condicional a) o gênero. \\[ \\mathbb{E}[salário| gênero = Mulher] \\] \\[ \\mathbb{E}[salário| gênero = Homem] \\] Chamamos essas esperanças de condicionais porque estamos condicionando no valor da variável gênero. Embora o gênero possa não parecer aleatório, do nosso ponto de vista é, pois se selecionarmos uma indivíduo aleatoriamente, seu gênero é desconhecido é será homem ou mulher com uma certa probabilidade. Os nossos dados estão agrupados para apenas dois gêneros e, por isso, por simplicidade, estamos tratando apenas esses dois gêneros aqui. O gráfico 5.3 apresenta as densidades sobrepostas por gênero. p3 &lt;- df %&gt;% ggplot(aes(x=log_salario)) + geom_density(aes(weight = V1028, group=genero, colour=genero)) + theme_bw(base_size = 22) print(p3) Figure 5.3: Distribuição dos salários A diferença fica um pouco mais claro se restringirmos os extremos da distribuição. p4 &lt;- df %&gt;% dplyr::filter(log_salario &gt; -2.5) %&gt;% dplyr::filter(log_salario &lt; 7.5) %&gt;% ggplot(aes(x=log_salario)) + geom_density(aes(weight = V1028, group=genero, colour=genero)) + theme_bw(base_size = 22) print(p4) Figure 5.4: Distribuição dos salários Fica claro que a maior parte da diferença entre homens e mulheres está no fato de que há mais mulheres no salário mediano do que homens (uma diferença de quase 20 pontos percentuais), com um pouco mais de homens à direita do valor modal. Vale lembrar também que nós já descontamos o efeito das horas trabalhadas. Se uma das formas pela qual homens ganham mais que mulheres é trabalhando mais, então a diferença no salário por hora trabalhada será menor. De fato, se olharmos para o salário total, e não mais salário por hora trabalhada, temos: p5 &lt;- df %&gt;% mutate(log_renda = log(renda)) %&gt;% ggplot(aes(x=log_renda)) + geom_density(aes(weight = V1028, group=genero, colour=genero)) + theme_bw(base_size = 22) print(p5) Figure 5.5: Distribuição dos salários E qual a diferença na média dos logaritmos da renda, condicional ao gênero? A resposta é 7.34 - 7.1, que é igual a: 0.24. Essa diferença é importante porque ela é uma medida da diferença percentual entre as duas rendas, conforme veremos a seguir. 5.7 Logaritmos É muito comum a utilização de logaritmos em regressões, de modo que vale uma digresssão sobre a correta interpretação de logaritmos. A primeira razão é que a diferença de logaritmos é aproximadamente igual à diferença entre porcentagens. Vamos ilustrar isso por meio de um exemplo, e então mostaremos matematicamente que isso sempre é o caso. De acodo com dados da PNADc de 2017, quarto trimstre, que temos usado neste capítulo, o salário por hora dos homens é em média \\(14.72\\) reais, e o das mulheres \\(12.57\\) reais. A diferença percentual no salário é, portanto, \\(100*(14.72 - 12.57)/12.57\\) ou \\(17,1%\\). Se em vez de calcular a diferença percentual, usar a aproximação \\(log(14.72) - log(12.57) = 0.1578941\\) ou 15,8%, tenho um valor próximo do verdadeiro valor de 17,1%. O caso geral pode ser visto do seguinte modo. Sejam dois números positivos \\(a\\) e \\(B\\), com \\(a &gt; b\\). A diferença percentual \\(p\\) entre \\(a\\) e \\(b\\) é dada por: \\[ 100*(a - b)/b = p \\] Que é o mesmo que: \\[ a/b = 1 + p/100 \\] Se passarmos o logaritmo natural de ambos os lados, temos: \\[ log(a/b) = log(1 + p/100) \\] \\[ log(a) - log(b) = log(1 + 17,1%/100) \\] Iremos usar a aproximação \\(log( 1 + x) \\approx x\\) para \\(x\\) pequeno. Logo: \\[ log(a) - log(b) \\approx p/100 \\] A segunda razão é que se aplicarmos essa lógica para médias, estamos aproximando a diferença percentua na méda geométrica. Usando essa aproximação, vemos que se a diferença no salário por hora é aproximadamente 16%, a diferença no salário total é 24%, oito pontos percentuais a mais. 5.8 Gênero e Raça Do mesmo jeito que condicionamos o salário e a renda ao gênero, podemos também condicionar ao gênero e à raça simultaneamente. table1 &lt;- df %&gt;% mutate(log_renda = log(renda), raca1 = ifelse(raca %in% c(&quot;Preta&quot;, &quot;Parda&quot;), &quot;Negra&quot;,as.character(raca))) %&gt;% group_by(raca1, genero) %&gt;% summarise(salario = round(weighted.mean(salario, w=V1028),2)) kable(table1) raca1 genero salario Amarela Homem 33.21 Amarela Mulher 25.76 Branca Homem 19.05 Branca Mulher 15.64 Ignorado Homem 12.09 Ignorado Mulher 48.41 Indígena Homem 11.06 Indígena Mulher 10.73 Negra Homem 11.02 Negra Mulher 9.62 E se não agruparmos pretos e pardos como negros, temos: table2 &lt;- df %&gt;% mutate(total_n = sum(V1028), log_renda = log(renda)) %&gt;% group_by(raca, genero) %&gt;% summarise(salario = round(weighted.mean(salario, w=V1028),2)) kable(table2) raca genero salario Branca Homem 19.05 Branca Mulher 15.64 Preta Homem 11.05 Preta Mulher 9.40 Amarela Homem 33.21 Amarela Mulher 25.76 Parda Homem 11.02 Parda Mulher 9.66 Indígena Homem 11.06 Indígena Mulher 10.73 Ignorado Homem 12.09 Ignorado Mulher 48.41 Para essa variável, vemos que faz sentido agrupar pretos e pardos como negros. As entradas nas tabelas nos dão a esperança condicional do salário dado gênero e raça. Por exemlo: \\[ \\mathbb{E}[salario| gênero = Mulher, raça = branca] = 15,64 \\] E se computássemos as esperanças condicionais no logaritmo natural, teríamos uma diferença percentual na média geométrica. Esperanças condicionais são descrições (resumos) dos dados. Para explicar as descrições encontradas, precisamos de um modelo causal, o que vamos adiar por enquanto. 5.9 Prevendo a partir da esperança condicional No gráfico abaixo, as esperanças condicionais para os dois valores de \\(X\\) são dadas pelos triângulos vermelhos. Como vemos, há uma grande variabilidade nos dados de salário em torno da esperança condicional. Isso significa que se só tenho a variável gênero, prever \\(Y\\) a partir da esperança condicional \\(\\mathbb{E}[Y|X]\\) resulta em um erro grande. df %&gt;% ggplot(aes(y=log_salario, x=genero)) + geom_point(shape = 1) + scale_y_continuous(labels = scales::dollar) + stat_summary( geom = &quot;point&quot;, fun.y = &quot;mean&quot;, col = &quot;black&quot;, size = 3, shape = 24, fill = &quot;red&quot; ) + theme_bw(base_size = 22) De fato, o erro que cometemos pode ser calculado por meio do Erro Quadrático Médio, que é definido como a soma dos erros ao quadrado (para não cancelar). No R, isso pode ser calculado como: df_erro &lt;- df %&gt;% group_by(genero) %&gt;% mutate(cond_exp = mean(log_salario)) %&gt;% ungroup() %&gt;% mutate(erro = log_salario - cond_exp) df_erro %&gt;% select(log_salario, genero, cond_exp, erro) %&gt;% head() %&gt;% kable() log_salario genero cond_exp erro 2.407946 Mulher 2.027614 0.3803316 2.051271 Homem 2.051232 0.0000387 2.547708 Mulher 2.027614 0.5200935 2.407946 Homem 2.051232 0.3567137 2.407946 Homem 2.051232 0.3567137 2.407946 Mulher 2.027614 0.3803316 df_erro %&gt;% summarise(eq = round(sum(erro),4), eqm = sum(erro^2)) %&gt;% kable() eq eqm 0 170978.7 Como iremos mostrar depois, não existe nenhuma outra forma melhor de prever \\(Y\\), se só tivermos informação de \\(Y\\) e \\(X\\). Ou seja, esse é o menor erro quadrático médio possível. Claro que, com mais variávels, podemos condicionais em mais informação e melhorar a previsão. Por exemplo, se temos a variável raça, além de gênero, podemos condicionar nas duas variáveis. log_salario genero raca cond_exp erro 2.407946 Mulher Parda 1.852722 0.5552238 2.051271 Homem Parda 1.858977 0.1922938 2.547708 Mulher Parda 1.852722 0.6949858 2.407946 Homem Branca 2.309516 0.0984293 2.407946 Homem Parda 1.858977 0.5489687 2.407946 Mulher Parda 1.852722 0.5552238 eq eqm 0 161619 Vemos que o EQM diminuiu ao condicionar em mais variáveis. Para reforçar, até aqui estamos falando da população. Embora no exemplo tenhamos dados da PNAD, que é uma amostra, e em teoria deveríamos trabalhar com o censo, estou simplificando e assumindo que a PNAD representa a população. Fazendo essa suposição, o erro que é cometido pela esperança condicional para prever \\(Y\\) é o menor erro (quantificado pelo EQM) possível (iremos demonstrar isso no próximo capítulo). A maior parte do curso será dedicadas a modelos preditivos, e apenas pontualmente falaremos de modelos causais↩︎ "],["cef-1.html", "Capítulo 6 - CEF 6.1 Erro da CEF 6.2 Simulando para entender a CEF 6.3 Propriedades da CEF 6.4 Regressão Linear e a CEF 6.5 Linearidade e Causalidade 6.6 Modelo só com intercepto 6.7 Variância da Regressão 6.8 Variância condicional 6.9 Efeito Marginal 6.10 CEF linear com peditores não-lineares", " Capítulo 6 - CEF Em geral, iremos simplificar a notação usando variáveis com uma letra. Assim, a variável dependente, salário (ou log do salário), será dado por \\(Y\\), como constuma ser feito. Já os preditores, como gênero ou raça, por \\(X_1, X_2\\) etc. O subscrito indica que é uma variável diferente. Assim, a esperança condicional pode ser reescrita para muitas variáveis preditoras como: \\[\\begin{equation} \\mathbb{E}[Y| X_1 = x_1, X_2 = x_2, ..., X_k=x_k] = m(x_1, x_2, ..., x_k) \\end{equation}\\] A essa esperança damos o nome de Função de Esperança Condicional (Conditional Expectation Function, em inglês). Usaremos a sigla em inglês, CEF, para nos referirmos à Função de Esperança Condicional. A CEF é uma função porque seu valor varia a depender dos valores particulares que \\(X_1, X_2, ..., X_k\\) assumem. Nós vimos que a raça pode ser, por exemplo, branca, preta etc. E a esperança condicional muda para cada valor das variáveis. O que é típico de uma função. Por fim, usamos a letra \\(m\\) do lado direito da equação porque estamos falamos da média de \\(Y\\) quando a CEF assume valores $x_1, x_2, …, x_k). Por fim, às vezes é útil pensar na CEF como uma função das variáveis aleatórias \\(X_1, X_2, ..., X_k\\), de modo que escrevemos simplesmente: \\[ \\mathbb{E}[Y| X_1, X_2, ..., X_k] \\] Se tivermos apenas um único preditor, \\(X\\), então a CEF é dada por: \\[ \\mathbb{E}[Y| X] = m(X) \\] 6.1 Erro da CEF Quando estamos fazendo previsões sobre \\(Y\\) a partir da CEF, cometemos um erro. Esse erro (para o caso de uma variável) é definido por: \\[ e = Y - m(X) \\] Se rearranjarmos, isso nos leva à seguinte equação: \\[ Y = m(X) + e \\] Notem que o erro \\(e\\) depende de \\(X\\) e \\(Y\\) e, portanto, é também uma variável aleatória. E uma propriedade (não um pressuspoto) da CEF é que o erro \\(e\\) tem esperança condiconal a \\(X\\) igual a zero. 6.2 Simulando para entender a CEF Vamos fazer simulações no R para entender o erro da CEF. Suponha que \\(Y = X^2 + U\\), em que \\(U \\sim norm(0,1)\\) e \\(X \\sim norm(0,1)\\). set.seed(234) n &lt;- 1000 x &lt;- rnorm(n) u &lt;- rnorm(n) y &lt;- x^2 + u m1 &lt;- mean(y) m2 &lt;- median(y) erro1 &lt;- y - m1 erro2 &lt;- y - m2 print(sum(erro1^2)) ## [1] 2987.139 print(sum(erro2^2)) ## [1] 3062.869 Nós imprimimos os dois erros quadráticos médios calculados, um usando a média de \\(Y\\) como minha estimativa, e outro usando a mediana. A média teve um desempenho melhor. De maneira geral, é possível mostrar que a média é é a melhor estimativa não condicional possível. 6.3 Propriedades da CEF Vejam que: \\[ \\mathbb{E}[e] = \\mathbb{E}[Y - m(X)] \\] e \\[ \\mathbb{E}[e|X] = \\mathbb{E}[Y - m(X)|X] \\] Pela propriedade de lineraridade da esperança, temos então: \\[ \\mathbb{E}[e|X] = \\mathbb{E}[Y|X] - \\mathbb{E}[m(X)|X] \\] Do que segue: \\[ \\mathbb{E}[e|X] = m(X) - \\mathbb{E}[m(X)|X] \\] 6.3.1 A Esperança do Erro Condicional a X é zero Existe um teorema, que não irei demonstrar, chamando de teorema do condicionamento, que diz que em situações como \\(\\mathbb{E}[m(X)|X]\\), isso é igual e \\(m(X)\\). O que torna nossa equação igual a zero. Ou seja: \\[ \\mathbb{E}[e|X] = m(X) - m(X) = 0 \\] 6.3.2 A esperança (não-condicional) do erro é zero. E existe um outro teorema, chamdo de lei das esperanças iteradas (Law of Iterated Expectations) que diz que a esperança da esperança condicional é a esperança não-condicional. Ou seja, \\(\\mathbb{E}[\\mathbb{E}[Y\\|X]] = \\mathbb{E}[Y]\\). Utilizando esse fato, temos que a esperança não-condicional do erro também é zero. \\[ \\mathbb{E}[e] = \\mathbb{E}[\\mathbb{E}[e|X]] = \\mathbb{E}[0] = 0 \\] 6.3.2.1 Intuição da LIE Como a LIE é muito importante em muitas demonstração de estatística, vale a penda dar pelo menos a intuição de sua validade. Para isso, vamos usar a lei da probabilidade total. Digamos que tenho um dado de 6 faces, dado \\(A\\), e outro de 4 faces, o dado \\(B\\). E \\(Y\\) é o valor que sai de jogar um dado e \\(X\\) é qual dado eu joguei. Queremos mostrar que: \\[ \\mathbb{E}[\\mathbb{E}[Y|X]] = \\mathbb{E}[Y] \\] A média das médias condicionais é dado pela média quando escolho o dado A (vezes sua probabilidade) mais a média quando escolho o dado B (vezes sua probabilidade). Se \\(X = A\\), o dado de 6 faces, então \\(\\mathbb{E}[Y|X=A]]\\), isto é, a média do valor do dado é \\((1+2+3+4+5+6)/6 = 3.5\\) Se \\(X = B\\), o dado de 4 faces, então \\(\\mathbb{E}[Y|X=A]]\\), isto é, a média do valor do dado é \\((1+2+3+4)/4 = 2.5\\) Se eu escolher cada dado aleatoriamente, isto é, com probabilidade 50%, então o valor médio de \\(Y\\), dado por \\(\\mathbb{E}[y]\\) é simplesmente \\(3.5*.5 + 3.5*.5 = 3\\). . No caso dos salários condicional ao gênero, efetivamente o que estamos dizendo é o seguinte: \\[ \\mathbb{E}[log(salário)|gênero = homem]*P[gênero = homem] + \\mathbb{E}[log(salário)|gênero = mulher]*P[gênero = mulher] \\] 6.3.3 O erro da CEF é não-correlacionado com X Por fim, uma última propriedade que não iremos demonstrar é que o erro da CEF é não-correlacionado com qualquer função de \\(X\\). Formalmente isso é formulado como \\(\\mathbb{E}[h(x)*e] = 0\\). Em particular, se \\(h(x)=x\\), então \\(\\mathbb{E}[x*e] = 0\\). Esse resultado implica que o erro é não-correlacionado com \\(X\\) (ou qualquer função de \\(X\\)). Dizemos que o erro é independente na média de \\(X\\), ou seja, \\(\\mathbb{E}[e|X]=0\\) e isso implica que o erro não é correlacionado com x. Vejam que já sabemos que \\(\\mathbb{E}[e]=0\\), portanto, \\(\\mathbb{E}[e|X]=0\\) é uma forma de dizer que a média do erro condicional a \\(X\\) é a mesma coisa que a média do erro sem condicionar em \\(X\\). Em outras palavras, a informação de \\(X\\) não muda minha estimativa da média do erro. Voltemos para o exemplo dos dados. Agora, suponha que tenho dois dados de \\(6\\) faces. Nesse caso, saber que escolhi o dado \\(A\\) não muda a médiade \\(Y\\), pois será igualmente \\(3.5\\). É fácil ver que não há correlação entre \\(X\\) e \\(Y\\) nesse caso. Aqui tem uma intuição de porque uma variável ser independente na média significa que não é correlacionada. Suponha que eu tenho uma medida da altura de pessoas, alguma medida do vocabulário delas e suas idades. Condicional à idade, saber a altura de uma pessoa não diz nada sobre o vocabulário dela (e vice-versa). Isso significa que não estão correlacionadas. Nós vimos que o log do salário (por hora) médio não condicional \\(\\mathbb{E}[log(salário)]\\) é 2.15. A tabela abaixo quebra esse valor por gênero, juntamente com a frequência relativa de categoria: table2 &lt;- df %&gt;% mutate(total_n = sum(V1028), log_renda = log(renda)) %&gt;% group_by(genero) %&gt;% summarise(salario = round(weighted.mean(salario, w=V1028),2), freq=sum(V1028)/total_n) table2 %&gt;% head(10) %&gt;% kable() genero salario freq Homem 14.72 0.5789522 Homem 14.72 0.5789522 Homem 14.72 0.5789522 Homem 14.72 0.5789522 Homem 14.72 0.5789522 Homem 14.72 0.5789522 Homem 14.72 0.5789522 Homem 14.72 0.5789522 Homem 14.72 0.5789522 Homem 14.72 0.5789522 Vejam que: \\[ \\mathbb{E}[log(salário)] = \\mathbb{E}[log(salário)|homem]*p(homem) + \\mathbb{E}[log(salário)|mulher]*p(mulher) \\] \\[ x*p(x) + y*p(y) = x1*y1 = 2.15 \\] Como vimos, é sempre verdade que (para o caso de \\(X\\) assumir dois valores), que: \\[ \\mathbb{E}[\\mathbb{E}[Y|X]] = \\mathbb{E}[Y|X=x_{1}]*p(X_1) + \\mathbb{E}[Y|X =x_2]*p(x_2) \\] Então, voltando ao nosso caso do erro, temos o seguinte (novamente para o caso particular de X assumindo dois valores). \\[ \\mathbb{E}[e]*\\mathbb{E}[X] = \\mathbb{E}[e*\\mathbb{E}[X]] = \\mathbb{E}[e*\\mathbb{E}[e|X]] = \\mathbb{E}[X*Y|X=x_{1}]*p(X_1) + \\mathbb{E}[X*Y|X =x_2]*p(x_2) \\] ## A esperança condicional é o melhor preditor Nós já antecipamos que a esperança conidcional é o melhor preditor de \\(Y\\) no sentido de minimizar o EQM. vamos agora mostrar isso. Anteriormente definidmos a esperança condicional \\(\\mathbb{E}[Y|X]\\) como \\(m(x)\\). Vamos por um momento esquecer essa definição e assumir que \\(m(x)\\) é valor que minimizar o EQM. Iremos provar que nesse caso, \\(m(x) = \\mathbb{E}[Y|X]\\) \\[ EQM = \\mathbb{E}[(Y - m(X))^2] \\] Somando e subtraindo \\(\\mathbb{E}[Y|X]\\) não altera a equação. Portanto \\[ EQM = \\mathbb{E}[(Y - \\mathbb{E}[Y|X] + \\mathbb{E}[Y|X] - m(X))^2] \\] Chamando por um momento \\(Y - \\mathbb{E}[Y|X] = a\\) e \\(\\mathbb{E}[Y|X] - m(X) = b\\), podemos expandir o quadrado para: \\[ EQM = \\mathbb{E}[a^2 + 2*a*b + b^2] \\] Substituindo os conteúdos e \\(a\\) e \\(b\\) de volta, temos: \\[ EQM = \\mathbb{E}[(Y - \\mathbb{E}[Y|X])^2 + 2*(Y - \\mathbb{E}[Y|X])*(\\mathbb{E}[Y|X] - m(X)) + (\\mathbb{E}[Y|X] - m(X))^2] \\] Qualquer se seja o valort de \\(m(x)\\) não altera o primeiro termo. Então, se quero achar o menor \\(m(x)\\) posso desprezá-lo. O segundo termo pode ser reescrito como \\(h(x)*e\\), em que \\(h(x) = 2*(Y - \\mathbb{E}[Y|X])\\) e \\(e = \\mathbb{E}[Y|X] - m(X)\\). E já vimos que essa esperança é zero. Portanto, o que realmente importa é escolher \\(m(x)\\) que torne \\(\\mathbb{E}[Y|X] - m(X)\\) mínimo. E o valor que minimizar o EQM é, portanto, \\(m(x) = \\mathbb{E}[Y|X]\\). Como Queríamos Demonstrar. 6.4 Regressão Linear e a CEF O modelo de regressão que iremos rodar com nossos dados pode ser conectado com a CEF em três maneiras diferentes (pelo menos). 6.4.1 Mínimos Quadrados Ordinários O modelo de regreessão linear supõe que queremos achar a melhor reta que se ajusta aos nossos dados. Se tivermos apenas um preditor, \\(X\\) e uma variável a ser predita \\(Y\\), a equação da reta de regressão (populacional) é: \\[ Y = \\alpha + \\beta*X + e \\] Essa reta é chamada de reta de regressão populacional ou até função de regressão populacional. Eu posso ter infinitas combinações de valores de \\(\\alpha\\) e \\(\\beta\\) formando infinitas retas de regressões. Porém, só uma delas me dará a menor soma dos erros quadráticos. Estou interesado em achar quais são esses valores de \\(\\alpha\\) e \\(\\beta\\). Antes de determinar a fórmula para achar esses valores, vamso relembrar como interpretar uma equação da reta. 6.4.1.1 Equação da Reta Na nossa equação da reta, \\(\\alpha\\) é o intercepto, isto é, o ponto onde a reta cruza o eixo \\(y\\) e \\(\\beta\\) é a inclinção ou coeficiente angular da reta. Se nós simularmos dados e plotarmos um scatter plot, veremos que podemos ajustar muitas retas aos dados. set.seed(1234) n &lt;- 1000 x &lt;- rnorm(n) u &lt;- rnorm(n) y &lt;- 2 + 1.3*x + u df &lt;- data.frame(y=y, x=x) df %&gt;% ggplot(aes(x, y)) + geom_point() + geom_smooth(se=F, method=&quot;lm&quot;) + geom_abline(slope= .5, intercept = 1, colour=&quot;red&quot;) + geom_abline(slope= 3, intercept = 3, colour=&quot;green&quot;, size=1) + geom_abline(slope= 0, intercept = 2, colour=&quot;grey&quot;, size=1) No gráfico ??, vemos que a reta em azul é melhor do que a verde e vermelha. Para achar os valores da reta azul (que aqui sabemos ser \\(\\alpha =2\\) e \\(\\beta = 1.3\\)), podemos novamente minimizar o EQM. A derivação requer calcular a derivada ou utilização de álgebra linerar, requisitos matemáticos para além do curso. Para um preditor, a fórmula do \\(\\beta\\) é dada por \\(COV(Y,X)/Var(X)\\) e a constante, \\(\\alpha\\) é dada por \\(\\mathbb{E}[(Y)] - \\beta*\\mathbb{E}[X]\\). Expressando a mesma fórmula com somatório (em vez de esperança), temos: \\[ \\alpha = \\bar{Y} - \\beta*\\bar{X} \\] \\[ \\beta = \\sum\\limits_{i=1}^n{}(y_i - \\bar{y})(x_i - \\bar{x})/\\sum\\limits_{i=1}^n{}(x_i - \\bar{x})^2 \\] Se nós aplicarmos essa fórmula para os dados do gráfico acima, iremos recuperar um valor aproximado da verdadeira reta de regressão (aproximado porque no fundo estamos simulando uma amostra. Mas se tivéssemos infinitos valores, teríamos exatamente os parâmetros populacionais). df %&gt;% summarise(cov_yx = cov(y,x), var_x = var(x), beta = cov_yx/var_x, alpha = mean(y) - beta*mean(x)) %&gt;% kable(digits=3) cov_yx var_x beta alpha 1.349 0.995 1.356 2.016 Vejam que é a mesma estimativa se eu usar a função “lm” do R, que estima uma modelo de regressão linear. lm(y ~x, df) ## ## Call: ## lm(formula = y ~ x, data = df) ## ## Coefficients: ## (Intercept) x ## 2.016 1.356 Agora podemos justificar porque faz sentido ajustar uma reta de regressão para aproximar a CEF. 6.4.2 Suponha que a CEF é Linear (Justificativa I) Se a CEF for de fato linear, então devemos usar a reta de regressão para estimar a CEF. É possível demonstrar esse resultado, mas não vou fazê-lo. Ao leitor interessado, recomendo, por exemplo, o livro Mostly Harmless Econometrics (p. 37), a quem estou seguindo de perto. Quando a CEF será linear? Um dos casos é quando a distribuição conjunta de probabilidade de \\(Y\\) e \\(X\\) for normal (normal bivariada, no caso de duas variáveis, ou multivariada, no caso de muitas variáveis). Foi o que Galton observou em seu estudo sobre a altura de pais e filhos, por exemplo, já que a altura é aproximadamente normal. O outro caso conhecido é o chamado “modelo saturado”. Em um modelo de regressão linear saturado, existe um parâmetro para cada possível valor que os preditores podem assumir. Voltaremos a discutir modelos saturados no futuro e, portanto, postergarei a discussão desse caso. Assim, fora esses dois cenários, não temos muito motivo para supor que a CEF é linear, posto que é improvável a priori que a CEF seja linear em geral. O que nos leva à segunda justificativa. 6.4.3 É o melhor preditor linear do \\(Y\\) (justificativa II) Outra justificativa para a CEF linear é que o modelo linear de regressão gera a melhor previsão possível (no sentido de menor Erro Quadrático Médio) para minha variável dependente, dentre todos os modelos lineares possíveis. Ou seja, se eu minimizar o erro entre o \\(Y\\) observado e minha previsão dada pelo modelo linear \\(\\alpha + \\beta*x\\), em que \\(\\alpha\\) e \\(\\beta\\) são definidos pelas fórmulas derivadas de MQO, terei o menor erro possível. Formalmente estou minimizando \\(\\mathbb{E}[Y - m(x)]\\). 6.4.4 A reta de regressão é a aproximação com o menor erro quadrático médio (justificativa III) Justificativa II e III são parecidas, mas distintas. No primeiro caso estou pensando em termos de previsão. No terceiro, em termos de aproximação. Ou seja, se a CEF for não-linear, e quiser aproximá-la por meio de uma reta, a melhor aproximação possível é por meio da fórmula de regressão. Formalmente, estou minimizando \\(\\mathbb{E}[\\mathbb{E}[Y|X] - m(x)]\\). Matematicamente fica claro que estou falando de coisas distintas (na justificativa II, minimizo a diferença entre \\(Y\\) e \\(m(x)\\), e na justificativa III minimizo a diferença entre \\(\\mathbb{E}[Y|X]\\) e \\(m(x)\\)). Em palavras, aqui estou aproximando a CEf (a esperança condicional), antes eu estava prevendo minha variável dependente. 6.5 Linearidade e Causalidade Toda a discussão de lineraridade é importante pelo motivo óbvio de que estaremos estimando modelos lineares, então é bom ter justificativas para usar modelos lineares para além do fato de que usamos porque aprendemos a usar ou porque é o que sabemos fazer. Mas além disso, estamos muitaz vezes interessados em modelo causais e a pergunta natural é: Sob que condições a CEF nos diz o efeito causal de \\(x\\) sobre \\(y\\)? Voltaremos a esse ponto em um capítulo futuro, uma vez que tenhamos aprendido a estimar modelos de regressão linear para fins de previsão. 6.6 Modelo só com intercepto O exemplo mais simples de modelo de regressão linear é o modelo apenas com intercepto, isto é, sem nenhum preditor \\(X\\). Nesse caso, \\(m(x) = E[Y] = \\mu\\), a média não condicional de \\(Y\\). Escrevendo a equação de regressão, temos: \\[ Y = \\mu + e \\] Com \\(\\mathbb{E}[e] = 0\\). 6.7 Variância da Regressão A variância não condicional do erro da CEF é dada por: \\[ \\sigma^2 = var[e] = \\mathbb{E}[(e - \\mathbb{E}[e])^2] = \\mathbb{E}[e^2] \\] A variância da regressão mede a porção da variância que não é “explicada” ou predita pela esperança condicional, já que é definida pela variância do erro \\(e\\). Além disso, ela depende dos preditores. Se temos preditores diferentes, a varância da regressão será diferente. 6.7.1 Chuva na Jamaica Uma propriedade da variância da regressão é que adicionar preditores não piora a variância e quase sempre melhora (reduz). É uma relação não-monotônica, isto é, a variância com mai spreditores é sempre menor ou igual que a variância com menos preditores (se a com mais preditores incluir os mesmos preditores da com menos regressores). Por isso que uma professor meu dizia: se você mediu chuva na Jamaica, pode colocar essa variável como preditora da regressão que isso irá reduzir a variância não explicada. 6.8 Variância condicional Sabemos que a esperança condicional é o melhor preditor que existe. Porém, ainda assim pode ser uma previsão ruim, como vimos no caso dos salários condicional ao gênero. Nesses casos, é útil olhar também para a variância condicional. \\[ \\sigma^2(x) = var[Y|X=x] = \\mathbb{E}[(Y - \\mathbb{E}[Y|X=x])^2] = \\mathbb{E}[e^2|X=x] \\] A variância condicional do erro (ou de \\(Y\\)) em geral depende de \\(X\\). Quando não depende, e portanto é uma constante, dizemos que o erro é homocedástico. Quando depende, que é o caso geral, dizemos que o erro é heterocedástico. 6.9 Efeito Marginal A regressão pode ser intepretada como efeito marginal. Se estivéssemos usando cálculo diferencial, o efeito marginal poderia ser definito como a derivada parcial de um preditor \\(x\\) com relação à variável dependente \\(y\\). Sem cálculo, podemos pensar como o coeficiente angular da variável, ou seja, quanto um aumento em x prediz um aumento em y. Quando temos vários regressoes, eles são mantidos constantes. É o que em economia se chama de ceteris paribus. Isso significa que a intepretação do efeito marginal só mantém constante os regressores incluído na regressão. 6.10 CEF linear com peditores não-lineares Uma aspecto importante que deve fcar cláro que é o modelo de regressão linear que usamos para aproximar a CEF é linear nos parâmetros, mas pode ser não-linear nas variáveis. Assim, a seguinte equação de regressão é linear nos parâmetros: \\[ y = a + b*x + c*x^2 + e \\] Porém, essa outra equação não é linear nos parâmetros: \\[ y = a + b*x + c*x^d + e \\] Vejam que o parâmetro \\(d\\) entra exponenciado e, portanto, não linearmente. "],["estimação.html", "Capítulo 7 - Estimação 7.1 Regressão por mínimos quadrados 7.2 Modelo de Média amostral 7.3 Modelo de Regressão Linear", " Capítulo 7 - Estimação É possível mostrar que o preditor linear ótimo com um único regressor é possui intercepto \\(\\alpha = \\mathbb{E}[Y] - \\beta*\\mathbb{E}[X]\\) e inclinação \\(\\beta = Cov(X,Y)/Var(X)\\). Isso significa que, se nós considerarmos o modelo de regressão \\(y_i = \\alpha + \\beta*x_i + e_i\\), e usarmos essas fórmulas para calcular os valores de \\(\\alpha\\) e \\(\\beta\\) em uma população, obteríamos uma reta ajustada que é o melhor preditor linear. Se tivermos uma amostra, e não a população, é razoável pensar que uma boa estimativa para os valores populacionais de \\(\\alpha\\) e \\(\\beta\\) são justamente essas fórmulas, calculadas para os dados amostrais. Esse estimador dos parâmetros populacionais nós chamamos de plug-in estimates, pois sem maiores teorias, assumimos que o que vale para a população vale para a amostra. Para diferenciar as estimativas amostrais dos valores populacionais, é comum usarmos \\(\\hat{\\beta}\\) em vez de \\(\\beta\\), ou então letras latinas \\(b\\) em vez das gregas \\(\\beta\\). Vou usar letras latinas, mas outros textos utilizam letras gregas com o “chapéu”. 7.1 Regressão por mínimos quadrados Quando temos uma amostra, é comum supormos que os dados são independentes e identicamente distribuídos, i.i.d. Esse tipo de amostra às vezes é chamada de amostra aleatória simples. Isso é tipicamente válido em dados de corte transversal (cross-section) como uma pesquisa de opinião (survey) de intenção de voto, amostra pequena (mil, dois mil) de alunos do Brasil a partir de dados doe Censo Escolar e assim por diante. O ponto é que os dados precisam ser dispersos. Se você coletar alunos de uma mesma escola, eles não tenderão a ser independentes. Se amostrar eleitores de um mesmo bairro, tampouco haverá independência. Há formas de lidar com isso, e aprenderemos mais tarde. Por enquanto, tratemos do caso mais simples. A suposição de que dados são i.i.d significa que, em uma amostra com duas variáveis, \\(X\\) e \\(Y\\), o par \\(x_i, y_i\\) é independente de \\(x_j, y_j\\), com \\(i \\neq j\\) e identicamente distribuídos, isto é, com a mesma distribuição. Essa suposição é importante para calcular a variância dos nossos estimadores, não para a estimativa pontual deles. 7.2 Modelo de Média amostral O modelo de regressão mais simples é aquele sem preditores, em que \\(Y = \\mu + e\\). O estimador de mínimos quadrados para \\(\\hat{\\mu}\\) é a média amostral, \\(\\bar{y}\\). Notem que \\(\\mathbb{E}[Y]\\) é a esperança populacional e \\(\\mu\\) é o valor da esperança, já que a esperança do erro \\(e\\) é zero. Vamso então calcular a média de nosso estimador \\(\\bar{y}\\) e sua variância. \\[ \\mathbb{E}[\\bar{y}] = \\mathbb{E}[\\sum{y_i}/n] = (1/n) * \\sum{\\mathbb{E}[y_i]} = (1/n)*\\mu*n = \\mu \\] Portanto, o valor esperando do estimador de mínimos quadrados (a média amostral) é igual à média populacional. Quando isso acontece, isto é, quando a esperança de um estimador é igual ao parâmetro populacional dizemos que o estimador é não-viesado. Definição 7.1. Um estimador \\(\\hat{\\theta}\\) é não-viesado quando \\(\\mathbb{E}[\\hat{\\theta}] = \\theta\\). Agora, vamos calcular a variância da média amostral sob a sposição de que a amostra é i.i.d. Considerando \\(Y_i = \\mu + e_i\\), temos: \\[ \\bar{y} = \\sum{y_i}/n = \\sum{(\\mu + e_i)}/n = \\sum{\\mu}/n + \\sum{e_i}/n = \\mu + \\sum{e_i}/n \\] Rearranjando, temos: \\(\\bar{y} - \\mu = \\sum{e_i}/n\\) A variância então pode ser calculada como: \\[ \\mathbb{Var}[\\bar{y}] = \\mathbb{E}[(\\bar{y} - mu)^2] \\] \\[ = \\mathbb{E}[( \\sum\\limits_{i=1}^n {e_i}/n)*( \\sum\\limits_{j=1}^n{e_j}/n)] = (1/n^2)* \\sum\\limits_{i=1}^n{} \\sum\\limits_{j=1}^n{}\\mathbb{E}[e_ie_j] \\] Se nós somarmos o somatório indexado no \\(j\\), o primeiro erro fica constante (indexado no i) e vamos por todo os erros de \\(1\\) até \\(n\\). Eventualmente, vamos passar por \\(i\\) e todos os outros diferentes de \\(i\\). Sabendo que os erros são i.i.d, isso significa que erros diferentes são não correlacionados e possuem esperança zero, ou seja, \\(\\mathbb{E}[e_ie_j] = 0, i \\neq j\\), e erros iguais, por serem da mesma distribuição, têm \\(\\mathbb{E}[e_ie_i] = \\sigma^2\\). Então o resultado do último somatório é uma soma de zeros, exceto para o caso em que \\(i = j\\), em que dá \\(\\sigma^2\\). De forma que a equação fica: \\[ (1/n^2)* \\sum\\limits_{i=1}^n{}\\sigma^2 = (1/n^2)*n*\\sigma^2 = \\sigma^2/n \\] Ou seja, a variância de nosso estimador é igual à variância populacional dividida pelo tamanho da amostra. A variância amostral do estimador também é chamada de erro padrão. 7.3 Modelo de Regressão Linear Tendo mostrado que o estimador é não-viesado e calculado a variância amostral para o caso mais simples, sem preditor, vamos agora fazer o mesm ocálculo com um preditor. Vamos assumir que o modelo de regressão linear é uma boa aproximação para CEF. Portanto, vamos supor que: 1. O modelo linear \\(Y = \\alpha + \\beta*X + e\\) é adequado. 2. \\(\\mathbb{E}[e|X] = 0\\), isto é, o erro é não correlacionado com x. Lembremos que a fórmula para o \\(\\hat{\\beta}\\) é \\(\\mathbb{Cov}[X,Y]/\\mathbb{Var}[X]\\). Substituindo nossa equaçã ode regressão na fórmula, temos: \\[ \\hat{\\beta} = n^{-1}*\\sum({x_i - \\bar{x}})*({y_i - \\bar{y}})/\\mathbb{Var}[x] = n^{-1}*(\\sum({x - \\bar{x}})*y_i + \\sum({x -\\bar{x}})*\\bar{y})/\\mathbb{Var}[x] \\] Vamos agora usar o fato de que, para qualquer variável \\(z\\), a diferença média de \\(z\\) para a média amostral de \\(z\\) é zero, isto é, \\(n^{-1}*\\sum\\limits_{i=1}^n{}z - \\bar{z} = 0\\) Por fim, segue-se disso que, para qualquer constante \\(w\\) que não varia com \\(i\\), podemos escrever: \\(n^{-1}*\\sum\\limits_{i=1}^n{}(z - \\bar{z})*w = 0\\) Portanto, \\(\\sum({x -\\bar{x}})*\\bar{y}) = 0\\). Logo, \\[ \\hat{\\beta} = n^{-1}*\\sum({x - \\bar{x}})*y_i/\\mathbb{Var}[x] \\] Lembremos que, \\(y_i = \\alpha + \\beta*X_i + e_i\\). Se eu adicionar e subtrair \\(\\beta*\\bar{x}\\) da equação, não altero ela. Utilizando esse truque e rearranjando, temos: \\[ y_i = \\alpha + \\beta*X_i + e_i = \\alpha + \\beta*\\bar{x} + \\beta*X_i - \\beta*\\bar{x} + e_i = \\alpha + \\beta*\\bar{x} + \\beta*(x_i - \\bar{x}) + e_i \\] Substituindo na equação anterior, temos: \\[ \\hat{\\beta} = n^{-1}*\\sum({x - \\bar{x}})*(\\alpha + \\beta*\\bar{x} + \\beta*(x_i - \\bar{x}) + e_i)/\\mathbb{Var}[x] \\] \\[ \\hat{\\beta} = n^{-1}*(\\alpha + \\beta)\\bar{x}\\sum({x - \\bar{x}})*(\\beta*(x_i - \\bar{x}) + e_i)/\\mathbb{Var}[x] \\] \\[ \\hat{\\beta} = n^{-1}*(\\alpha + \\beta)\\bar{x}\\sum({x - \\bar{x}}) + n^{-1}\\beta\\sum(x_i - \\bar{x})^2 + n^{-1}\\sum({x - \\bar{x}})*e_i)/\\mathbb{Var}[x] \\] A priemria soma é uma constante, vezes o somatório de \\(x_i\\) menos sua média, o que é zero. A segunda soma é a variância de x multiplicada por \\(\\beta\\). Então, simplifcando, temos: \\[ \\hat{\\beta} = \\beta + n^{-1}\\sum({x - \\bar{x}})*e_i)/\\mathbb{Var}[x] \\] Portanto, nós mostramos que o estimador \\(\\hat{\\beta}\\) pode ser decomposto no parâmetro populacional \\(\\beta\\) mais uma soma ponderada do termo de erro. Uma vez que nossa amostra é i.i.d., sabemos que é uma média ponderada dos erros não-correlacionados. Podemos agora mostrar que o estimador é não-viesado. \\[ \\mathbb{E}[\\hat{\\beta}] = \\beta + n^{-1}\\mathbb{E}[\\sum({x - \\bar{x}})*e_i)]/\\mathbb{Var}[x] = \\beta +\\mathbb{E}[e]*\\mathbb{E}[\\sum({x - \\bar{x}})]\\mathbb{Var}[x] = \\beta \\] Podemos similarmente calcular a variância do nosso estimador, e mostrar que ela é a variância do erro dividida pela variância de x vezes o tamanho da amostra, \\(n\\). \\[ \\mathbb{Var}[\\hat{\\beta}] = \\mathbb{Var}[\\beta + n^{-1}\\sum({x - \\bar{x}})*e_i)/\\mathbb{Var}[x]] \\] Sabemos que \\(\\mathbb{Var}[x + a] = \\mathbb{Var}[x]\\). Então, podemos eliminar o \\(\\beta\\) da equação. \\[ \\mathbb{Var}[\\hat{\\beta}] = \\mathbb{Var}[n^{-1}\\sum({x - \\bar{x}})*e_i)/\\mathbb{Var}[x]] \\] Todas as observações do \\(x\\) foram observadas e, portanto, a soma da diferença para a média é uma constante. Sabemos que \\(\\mathbb{Var}[x*a] = a^2\\mathbb{Var}[x]\\) \\[ \\mathbb{Var}[\\hat{\\beta}] = (n^{-1}\\sum({x - \\bar{x}}))^2*\\mathbb{Var}[e_i/\\mathbb{Var}[x]] \\] Supondo homocedasticidade, [e_i] = ^2$ \\[ \\mathbb{Var}[\\hat{\\beta}] = (n^{-1}\\sum({x - \\bar{x}}))^2*\\mathbb{Var}[e_i/\\mathbb{Var}[x]] \\] Vamos começar com um exemplo, importando dados do site Base de Dados, que possui muitas bases de dados públicas, já tratadas. Para tanto, precisaremos criar um projeto no Google cloud, conforme os passos aqui: https://basedosdados.github.io/mais/access_data_packages/ Depois, só escolher quais dos dados iremos olhar https://basedosdados.org/ Escolhi mexer com dados do censo escolar # instalando a biblioteca # install.packages(&#39;basedosdados&#39;) # carregando a biblioteca na sessão library(basedosdados) # para importar os dadosdiretamente no R, precisamos criar um id para acessar a base de dados set_billing_id(&quot;aula-reg-manoel&quot;) # checando que deu certo get_billing_id() ## [1] &quot;aula-reg-manoel&quot; # importando dados knitr::include_graphics(here(&quot;imagens&quot;, &quot;autenticacao base dados.jpg&quot;)) query &lt;- &quot;SELECT * FROM `basedosdados.br_inep_censo_escolar.escola` where sigla_uf = &#39;AL&#39; and id_municipio = &#39;2704302&#39; and ano = 2019&quot; escola &lt;- read_sql(query) # query &lt;- &quot;SELECT count(*) as contagem, id_municipio FROM `basedosdados.br_inep_censo_escolar.escola` where sigla_uf = &#39;AL&#39; and ano = 2019 group by id_municipio&quot; # escola &lt;- read_sql(query) query &lt;- &quot;SELECT * FROM `basedosdados.br_inep_censo_escolar.dicionario`&quot; dicionario &lt;- read_sql(query) query &lt;- &quot;SELECT * FROM `basedosdados.br_inep_censo_escolar.matricula` where sigla_uf = &#39;AL&#39; and id_municipio = &#39;2704302&#39; and ano = 2019&quot; aluno &lt;- read_sql(query) # dicionário de gênero e raça # dicionario %&gt;% # dplyr::filter(id_tabela == &quot;matricula&quot;, nome_coluna %in% c(&quot;sexo&quot;, &quot;raca_cor&quot;)) aluno_gen &lt;- aluno %&gt;% dplyr::filter(regular == 1) %&gt;% group_by(id_escola, sexo) %&gt;% summarise(num_aluno_gen = n()) %&gt;% mutate(sexo = gsub(&quot;1&quot;, &quot;Male&quot;, sexo), sexo = gsub(&quot;2&quot;, &quot;Female&quot;, sexo)) %&gt;% mutate(total = sum(num_aluno_gen), percent = num_aluno_gen/total) %&gt;% filter(sexo == &quot;Female&quot;) %&gt;% rename(percent_female = percent) %&gt;% dplyr::select(id_escola, percent_female) aluno_raca &lt;- aluno %&gt;% dplyr::filter(regular == 1) %&gt;% group_by(id_escola, raca_cor) %&gt;% summarise(num_aluno_raca = n()) %&gt;% mutate(raca_cor = gsub(&quot;1&quot;, &quot;Branca&quot;, raca_cor), raca_cor = gsub(&quot;2&quot;, &quot;Preta&quot;, raca_cor), raca_cor = gsub(&quot;3&quot;, &quot;Parda&quot;, raca_cor)) %&gt;% mutate(total = sum(num_aluno_raca), percent = num_aluno_raca/total) %&gt;% filter(raca_cor %in% c(&quot;Branca&quot;, &quot;0&quot;, &quot;Preta&quot;, &quot;Parda&quot;)) %&gt;% mutate(raca_cor = gsub(&quot;0&quot;,&quot;não_declarado&quot;, raca_cor)) %&gt;% select(id_escola, raca_cor, percent) %&gt;% pivot_wider(names_from = raca_cor , values_from = percent ) aluno_escola &lt;- aluno_gen %&gt;% inner_join(aluno_raca, by = &quot;id_escola&quot;) %&gt;% inner_join(escola, by = &quot;id_escola&quot;) aluno_escola_reg &lt;- aluno_escola %&gt;% ungroup() %&gt;% mutate(negra = Preta + Parda) %&gt;% select(negra, percent_female, tipo_localizacao, agua_potavel, esgoto_rede_publica, lixo_servico_coleta, area_verde, biblioteca, quantidade_profissional_psicologo) %&gt;% filter(across(everything(), ~!is.na(.))) %&gt;% mutate_if(bit64::is.integer64, as.factor) reg &lt;- lm(negra ~ percent_female + tipo_localizacao + agua_potavel + esgoto_rede_publica + lixo_servico_coleta + area_verde + biblioteca + quantidade_profissional_psicologo, data= aluno_escola_reg) summary(reg) ## ## Call: ## lm(formula = negra ~ percent_female + tipo_localizacao + agua_potavel + ## esgoto_rede_publica + lixo_servico_coleta + area_verde + ## biblioteca + quantidade_profissional_psicologo, data = aluno_escola_reg) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.46731 -0.16217 -0.01934 0.17534 0.55176 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.070584 0.204310 0.345 0.72991 ## percent_female 0.400443 0.256195 1.563 0.11880 ## tipo_localizacao2 -0.251352 0.224599 -1.119 0.26373 ## agua_potavel1 0.067306 0.052684 1.278 0.20212 ## esgoto_rede_publica1 -0.069037 0.022951 -3.008 0.00279 ** ## lixo_servico_coleta1 0.144155 0.158554 0.909 0.36378 ## area_verde1 0.047138 0.025681 1.836 0.06714 . ## biblioteca1 0.009462 0.022531 0.420 0.67473 ## quantidade_profissional_psicologo1 -0.069969 0.033593 -2.083 0.03787 * ## quantidade_profissional_psicologo2 -0.120828 0.092870 -1.301 0.19396 ## quantidade_profissional_psicologo3 -0.306991 0.130754 -2.348 0.01935 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2222 on 418 degrees of freedom ## Multiple R-squared: 0.07237, Adjusted R-squared: 0.05017 ## F-statistic: 3.261 on 10 and 418 DF, p-value: 0.0004498 "],["causalidade.html", "Capítulo 8 - Causalidade 8.1 Resultados Potenciais 8.2 Modelo estrutural e modelo de regressão 8.3 Simulação de Resultados Potenciais e Causalidade", " Capítulo 8 - Causalidade Até o momento estivemos estudando regressão em um sentido puramente preditivo (ou descritivo). Mas com frequência nosso interesse está em utilizar regressão para inferir causalidade. E uma regressão pode ser interpretada causalmente quando a CEF for causal. Obviamente, essa resposta é insatisfatória, pois a próxima pergunta é: quando a CEF é causal? A resposta é: A CEF é causal quando ela descreve diferenças nos resultados ponteciais (potencial outcomes) em uma dada população alvo. Vamos então definir causalidade a partir da noção de resultados potenciais, para em seguida ver como podemos utilizar regressão para fazer inferência causal. 8.1 Resultados Potenciais Digamos que estamos interessados em estimar o efeito causal de pertencer à base do governo sobre o percentual de votos de um deputado que será conforme indicado pelo líder do governo. Donald Rubin, em uma série de artigos nos anos 70, introduziu a noção de resultados potenciais a partir da noção mais familiar de contrafactual. Imagine um deputado que está na oposição e ele passa a fazer parte do governo. Se nós pudéssemos observar uma série de votações tanto no mundo factual em que é do governo, quando no contrafactual em que não é do governo, teríamos o efeito causal para aquele deputado de fazer parte do governo sobre seu percentual de votação em acordo com a indicação do líder do governo. Porém, não é possível observar ambos os mundos, o factual e o contrafactual ao mesmo tempo, o que é conhecido como o problema fundamental da inferência causal. Se pudéssemos observar duas terras paralelas, exatamente iguais, exceto por esse único fato, conseguiríamos calcular o efeito causal. Seja \\(Y_{1i}\\) o percentual de votação potencial do deputado \\(i\\) quando ele é da base do governo (indicado pelo índice “1”) e seja \\(Y_{0i}\\) o percentual de votação potencial do deputado \\(i\\) quando ele não é da base do governo (indicado pelo índice “0”). \\(Y_{1i} - Y_{0i}\\) é o efeito causal para \\(i\\) de fazer parte da base do governo e é o que gostaríamos de saber para cada deputado. Se eu pudesse observar essa diferença para cada deputado, poderia calcular o Efeito Causal Médio, \\(\\mathbb{E}[Y_{1i} - Y_{0i}]\\), em inglês, Average Causal Effect (ACE). Seja \\(B_i\\) uma variável que indica se um deputado é ou não da base do governo (\\(1\\) se é, \\(0\\) caso contrário). O mundo factual (observado) pode ser descrito em termos dos resultados potenciais pela eqquação abaixo. É importante enfatizar que essa equação conecta os resultados observados com os resultados potenciais. Vale notar também a diferença de notação entre um resultado observado (indexado apenas o indivíduo \\(i\\)) e um potencial (indexado para o indivíduo \\(i\\) e para o tratamento potencialmente recebido). \\[\\begin{equation} \\tag{8.1} Y_i = Y_{0i} + (Y_{1i} - Y_{0i})*B_i \\end{equation}\\] A equação nos lembra também que nós só podemos observar um dos resultados potenciais, associado a ser ou não da base do governo, mas não ambos ao mesmo tempo. Veja que se para um indivíduo \\(i\\) ele é da base do governo, isto é, \\(B_i = 1\\), então tenho para este individuo em particularque \\(Y_i = Y_{1i}\\), isto é, o resultado potencial coincide com o resultado observado. Essa igualdade é na verdade uma suposição, chamada de consistência. Consistência: Essa suposição implica que o resultado potencial de um indivíduo quando recebe um tratmento é igual ao observado na realidade. Em outras palavras, consistência significa que o que você observa é o que você supunha que aconteceria se a pessoa recebesse o tratamento. Para imaginar que consistência falhasse, imagine quando um deputado \\(i\\) distante ideologicamente do governo vira da base do governo ele, buscando reduzir dissonância cognitiva, muda sua ideologia para algo mais proximo do governo. Já um deputado mais alinhado não precisa fazer isso. Então, na verdade temos dois “tratamentos” acontecendo, e não um, e a hipótese de consistência não será verificada. 8.1.1 Decomposição do Efeito Causal A equação 8.1 conecta resultados observados com resultados potenciais. Isso é particularmente útil quando quisermos estimar o efeito causal a prtir de dados observacionais. Digamos que tenho dados sobre deputados que fazem parte da base do governo e que não fazem. Nós iremos mostrar agora porque correlação (associação) não implica causalidade. Digamos que queremos computar a média percentual de votação conforme indicação do governo para deputados da base e fora da base. Isso é dado pela equação 8.2 abaixo. \\[\\begin{equation} \\tag{8.2} \\mathbb{E}[Y_{i}|B_i = 1] - \\mathbb{E}[Y_{i}| B_i = 0] \\end{equation}\\] Veja que por enquanto estamos tratando apenas de dados observados e, portanto, não podemos falar de dados potenciais. Contudo, se a suposição de consistência for válida, podemos utilizar a equação 8.1 para conectar o factual e o contrafactual. \\[\\begin{equation} \\begin{aligned} \\mathbb{E}[Y_{i}|B_i = 1] = \\mathbb{E}([Y_{0i} + (Y_{1i} - Y_{0i})*B_i)|B_i=1] \\\\ \\mathbb{E}[Y_{i}|B_i = 1] = \\mathbb{E}[(Y_{0i} + (Y_{1i} - Y_{0i})*1)|B_i=1] \\\\ \\mathbb{E}[Y_{i}|B_i = 1] = \\mathbb{E}[Y_{1i}|B_i=1] \\end{aligned} \\end{equation}\\] E, similarmente, \\[\\begin{equation} \\mathbb{E}[Y_{i}|B_i = 0] = \\mathbb{E}[Y_{0i}|B_i=0] \\end{equation}\\] De modo que a equação 8.2 pode ser reescrita do seguinte modo (supondo consistência): \\[\\begin{equation} \\mathbb{E}[Y_{i}|B_i = 1] - \\mathbb{E}[Y_{i}| B_i = 0] = \\mathbb{E}[Y_{1i}|B_i=1] - \\mathbb{E}[Y_{0i}| B_i = 0] \\end{equation}\\] Agora, podemos utilizar aquele truque de subtrair e somar a mesma coisa que não altera a equação. Em particular, somar um resultado pottencial contrafactual, isto é, o que em média um deputado que faz parte da base do governo faria se não estivesse no governo (denotado por \\(Y_{0i}|B_i = 1\\)). Em vermelho está o truqe matemático de subtrair e somar a mesma coisa sem alterar a equação. \\[\\begin{equation} \\begin{aligned} \\mathbb{E}[Y_{i}|B_i = 1] - \\mathbb{E}[Y_{i}| B_i = 0] = \\\\ \\mathbb{E}[Y_{1i}|B_i=1] \\color{red}{- \\mathbb{E}[Y_{0i}|B_i = 1] + \\mathbb{E}[Y_{0i}|B_i = 1]} - \\mathbb{E}[Y_{0i}| B_i = 0] \\end{aligned} \\end{equation}\\] Rearranjando a equação e utilizando as propriedades do operador esperança ,temos: \\[\\begin{equation} \\tag{8.3} \\begin{aligned} \\mathbb{E}[Y_{i}|B_i = 1] - \\mathbb{E}[Y_{0i}|B_i=0] = \\\\ \\color{green}{\\mathbb{E}[Y_{1i} - Y_{0i}|B_i = 1]} + \\color{blue}{\\mathbb{E}[Y_{0i}|B_i = 1] - \\mathbb{E}[Y_{0i}| B_i = 0]} \\end{aligned} \\end{equation}\\] A equação 8.3 apresenta em verde o que chamamos de efeito Médio do Tratamento entre as pessoas Tratadas (em inglês, Average effect of the Treatment on the Treated, ATT) e em azul o viés de seleção. O ATT é o efeito médio porque estamos calculando a média (esperança), do efeito do Tratamento por causa do termo \\(Y_{1i} - Y_{0i}\\), e entre as pessoas Tratadas, isto é, que pertencem à base do governo, por que condiciona em \\(B_i = 1\\). E a parte em azul é o viés de seleção porque reflete a diferença média (no percentual de votos seguindo o governo) entre as pessoas que escolheram ser da base do governo \\(|B_i = 1\\) e as que escolheram não ser \\(|B_i = 0\\) no mundo de resultados potenciais em que ambos não foram tratados (\\(Y_{0i}\\)). Ou seja, se pudéssemos observar o mundo contrafactual em que deputados da base estão fora do governo e computar o percentual médio de voto deles seguindo o governo, e comparar com o observado para os deputados fora da base, se houver diferença, isso é o viés de seleção. Quando o viés de seleção é diferente de zero, então a nossa comparação ingênua da diferença média de comportamento entre governo e oposição não recupera o efeito causal médio. Por exemplo, se deputados de esquerda possuem maior chance de fazer parte do governo Lula e, pelo fato de serem mais próximos ideologicamente, tendem a votar com o governo por esse motivo, então teriam um comportamento médio diferente da oposição, mesmo se não estivessem na base do governo. Nesse exemplo, a diferença média entre governo e oposição superestima o efeito causal médio de ser da base do governo. Esse resultado permite visualizar também porque, em um contexto experimental, podemos garantir que o viés de seleção é zero em média. Se cada pessoa é atribuída ao tratamento ou controle aleatoriamente (exemplo fictício, se der cara, é da base do governo, se der coroa, não é da base), então não há diferença média entre um grupo e outro. Obviamente, temos de supor que há compliance perfeito (isto é, um Bolsonarista alocado aleatorimnte para compor o governo deveria aceitar fazê-lo e, similarmente, um petista alocado na oposição deveria aceitar isso e serem ambos tratados assim pelo governo). Quando o compliance não é perfeito, há formas de tratar isso, mas se torna mais complicado. Em estudos observacionais, o mecanismo de alocação de pessoas entre tratamento e controle não está sob controle do pesquisador e, portanto, não podemos garantir que o viés de seleção é zero. Em um contexto dos assim chamados experimentos naturais ou quase experimentos, a aleatorização existe, mas não é feita pelo pesquisador (caso de uma política governamental usada segundo alguma regra aleatória, como no caso do draft do Vietnam) ou a aleatorização não existe, mas é plausível supor que, para um certo subconjunto dos dados, a alocação entre tratamento e controle é como se fosse aleatório (exemplo, quando da introdução da urna eletrônica de acordo com tamanho da população, cidades com um eleitor acima do ponto de corte versus com um abaixo possui diferença aleatória em relação a, por exemplo, percentual de votos nulos. Então é possível estimar o efeito causal da urna eletrônica sobre percentual de votos nulos nesse subgrupo de cidades próximas do ponto de corte). E em contextos mais gerais de estudos observacionais, para além de contextos quasi-experimentais, o efeito causal só pode ser recuperado (identificado) se a suposição de Conditional Independence Assumption (CIA) for válida. Essa suposição diz que, condicional a variáveis de controle, não há relação entre a alocação de tratamento e os resultados potenciais. No nosso exemplo, isso significa que, condicional a, por exemplo, a ideologia do deputado, não há correlação entre alocação para tratamento e controle e resultados potenciais. Ou seja, o viés de seleção seria zero em média. Se houver alguma variável que causa quem éselecionado (ou se seleciona) entre tratamento e controle e tmbém causa os resultados potenciais, o viés de seleção não é zero e o efeito causal não é estimável. Por exemplo, digamos que a execução de emendas parlamentares fosse diferente entre governo e oposição e que essa variável também influencia a votação. Ao não controlar para essa variável, a suposição de CIA não é atendida. 8.2 Modelo estrutural e modelo de regressão A distinção entre CEF e CEF causal é um ponto muito importante mas no geral negligenciado. Chen &amp; Pearl (2013), ao avaliaram livros de econometria, notaram que a maioria não fazia a distinção adequada entre modelo de regressão e modelo estrutural de regressão. Como a compreensão desse ponto é crítica para qualquer análise causal com regressão e frequentemente não feita, quero insistir nesse aspecto. Vamos recapitular o que nós aprendemos sobre regressão e sobre causalidade. O erro da regressão (na população) é definido como a diferença entre o \\(Y\\) observado e a esperança condicional \\(\\mathbb{E}[Y|X]\\). Uma consequência dessa definição é que o erro é não correlacionado com o regressor \\(X\\), sejam os dados observacionais ou não. A regressão linear pode ser jusitifcada de três modos distintos: 1. se a CEF for linear (como em um modelo saturado); 2. se queremos minimizar o erro quadrático médio das previsões; 3. é a melhor aproximação linear que existe para a CEF, seja linear ou não. Em estudos observacionais, a análise de regressão linear identifica o modelo causal se o viés de seleção for zero. Isso requer que a suposição de indepenência (ou independência condicional) seja verdade, isto é, que a correlação entre o erro e os regressores seja zero. Ou seja, é uma suposição do modelo (e não uma propriedade ou consequência da definição do erro). Para a suposição ser plausível, precisamos ter um modelo estrutural (como o de Resultados Potenciais) em que seja plausível (é uma suposição não testável) a independência condicional. Muitos livros textos de regressão não diferenciam as propriedades da CEF e as suposições de um modelo estrutural (causal). Vamos ilustrar, por meio de um exemplo, o ponto de como é possível o erro da CEF ser não correlacionado com o regressor e o modelo estrutural ter erro e regressor correlacionados (e portanto a causalidade não identificada se não controlarmos para variável que garanta independência condicional. 8.2.1 Exemplo de CEF e CEF estrutural Suponha que estou interessado em estimar o efeito causal de uma variável, que vou designar por tratamento. Meus dados são observacionais e há viés de seleção nos dados. Uma outra variável, \\(X\\), causa tanto o tratamento quando minha variável dependente, \\(Y\\). Porém, condicional a \\(X\\), o tratamento é independente dos resultados potenciais, ou seja, é válida a suposição de independência condicional. Vou mostrar que a o erro da CEF é independente dos regressores e, por outro lado, uma regressão sem controlar para \\(X\\) tem resíduo correlacionado com o regressor e, portanto, não identifica o efeito causal. Por fim, uma regressão que controle para \\(X\\) identifica o efeito causal. Chen, B., &amp; Pearl, J. (2013). Regression and causation: a critical examination of six econometrics textbooks. Real-World Economics Review, Issue, (65), 2-20. 8.3 Simulação de Resultados Potenciais e Causalidade Vamos rodar uma simulação Fonte da simulação: https://stats.stackexchange.com/questions/554113/potential-outcomes-selection-bias library(tidyverse) # U affects both treatment and outcome (Y). # simulation # set.seed(123) n = 1000 u = rnorm(n, 1, 1) # generate latent variable affecting treatment selection d = rbinom(n, 1, prob = plogis(u)) # treatment e = rnorm(n, 0, 1) # homogenous causal effect # causal_effect = 2 # Y(0) y0 = u + e # Y(1) y1 = causal_effect + u + e # Observed Y yobs = d*causal_effect + u + e # df = data.frame(u, d, y0, y1, yobs) # # individual causal effect # df$y1y0 = df$y1 - df$y0 # grand mean # mean(df$y1y0) ## [1] 2 # ATT / ATC # df %&gt;% group_by(d) %&gt;% summarise(mean(y1 - y0)) ## # A tibble: 2 × 2 ## d `mean(y1 - y0)` ## &lt;int&gt; &lt;dbl&gt; ## 1 0 2 ## 2 1 2 # naive group comparison # naive_group_comparison = df %&gt;% group_by(d) %&gt;% summarise(m = mean(yobs)) %&gt;% summarise(m[d == 1] - m[d == 0]) naive_group_comparison ## # A tibble: 1 × 1 ## `m[d == 1] - m[d == 0]` ## &lt;dbl&gt; ## 1 2.76 # ATT `Y(1) Y(0) | D =1` = df[df$d==1,] %&gt;% summarise(mean(y1 - y0)) # ATC `Y(1) Y(0) | D =0` = df[df$d==0,] %&gt;% summarise(mean(y1 - y0)) # unobserved quantities # `Y(0) | D =1` = df[df$d==1,] %&gt;% summarise(mean(y0)) `Y(1) | D =0` = df[df$d==0,] %&gt;% summarise(mean(y1)) # observed `Y(1) | D =1` = df[df$d==1,] %&gt;% summarise(mean(y1)) `Y(0) | D =0` = df[df$d==0,] %&gt;% summarise(mean(y0)) # ATT selection_bias_1 = `Y(0) | D =1` - `Y(0) | D =0` # ATT + selection bias 1 # `Y(1) Y(0) | D =1` + selection_bias_1 ## mean(y1 - y0) ## 1 2.757471 # We get the correct result as # naive_group_comparison # ATC selection_bias_2 = `Y(1) | D =0` - `Y(1) | D =1` # ATC + selection bias 1 # `Y(1) Y(0) | D =1` + selection_bias_2 ## mean(y1 - y0) ## 1 1.242529 "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
