[["glm.html", "Capítulo 13 - GLM 13.1 PLM 13.2 Logística e Probit 13.3 Probabilidade e chance 13.4 Logística como GLM", " Capítulo 13 - GLM ## Warning: package &#39;ggplot2&#39; was built under R version 4.3.2 ## Warning: package &#39;purrr&#39; was built under R version 4.3.1 ## Warning: package &#39;lubridate&#39; was built under R version 4.3.2 ## Warning: package &#39;here&#39; was built under R version 4.3.2 VD Categórica 13.1 PLM Até o momento, variáveis respostas categóricas binárias foram modeladas com regressão linear. \\[ y_i \\sim N(\\alpha + \\beta x_i, \\sigma^2) \\] Nessa parametrização do modelo de regressão, vemos que a resposta é modelada como uma variável Gaussiana, e portanto, é uma variável contínua (em vez de binária), além disso, não está limitada, possuindo suporte entre \\(-\\infty\\) e \\(+\\infty\\). Esses modelos são chamados também de modelos de probabilidade linear (MPL, ou LPM na sigla em inglês). A introdução dos modelos de regres~sao logística e probit deixam claro porque são chamados desse modo, de forma que faz sentido apresentarmos o que é a regressão logística e probit, antes de voltarmos À discussão da adequação ou inadequação do MPL. 13.2 Logística e Probit Há várias formas de apresentar e/ou justificar a regressão logística. Qual delas você irá usar para pensar esse tipo de modelo depende do seu problema de pesquisa e das suas preferências sobre o que funciona melhor para você. 13.2.1 Logística como melhoria em relação ao MPL Como nós vimos, o problema da regressão linear para dados binários é que considera que a variável resposta possui distribuição Gaussiana. Faz muito mais sentido modelar dados binários como seguindo uma distribuição de Bernoulli, com parÂmetro \\(p_i\\). Poderíamos portanto tentar reescrever um modelo para \\(y\\) binário da seguinte forma: \\[ y_i \\sim Ber(p_i) \\] Em que \\(p_i\\) é a probabilidade de sucesso para a unidade \\(1\\). Dessa forma, contudo, não incluí nenhum preditor para estimar o \\(p_i\\), e gostaríamos de fazê-lo. Posso então escrever algo como: \\[ p_i(x_i) = \\alpha + \\beta x_i \\] O problema dessa formulação é que probabilidades devem estar entre \\(0\\) e \\(1\\), e nada garante que \\(\\alpha + \\beta x_i\\) seja um número entre \\(0\\) e \\(1\\). Então, uma saída é tentar achar uma função \\(f\\) que transforme \\(\\alpha + \\beta x_i\\) em números entre \\(0\\) e \\(1\\), ou seja, \\(0 \\le f(\\alpha + \\beta x_i) \\le 1\\). E uma função que tem essa propriedade é a logística padrão (também chamada de sigmóide), dada por: \\[ f(x) = \\frac{1}{1 + \\exp(-x)} \\] Então, nossa relação entre a probabilidade e os preditores fica: \\[ p_i = \\frac{1}{1 + \\exp(-(\\alpha + \\beta x_i))} \\] Conectando com nossa variável resposta, temos: \\[ y_i \\sim Ber(\\frac{1}{1 + \\exp(-(\\alpha + \\beta x_i))}) \\] Às vezes a logística é designada como a função inversa da logito, em que a logito é dada por: \\[ logito(p) = log(\\frac{p}{1-p}) \\] Para entender isso, vamos lembrar que, se \\(f(x) = x + 2\\) é uma função, sua inversa \\(f^{-1}(x)\\), se existir, pode ser descoberta pelo algoritmo em que chamamos \\(f(x)\\) de \\(y\\), trocamos \\(y\\) por \\(x\\) e resolvemos para \\(y\\): \\[\\begin{align} y = x + 2 \\\\ x = y + 2 \\\\ y = x - 2 \\\\ f^{-1}(x) = x - 2 \\end{align}\\] Para uma função \\(f(x) = log(x+2)\\), a inversa é: \\[\\begin{align} y = log(x + 2) \\\\ x = log(y + 2) \\\\ \\exp(x) = \\exp(log(y + 2)) \\\\ \\exp(x) = y + 2 \\\\ y = \\exp(x) + 2 \\\\ f^{-1}(x) = \\exp(x) + 2 \\end{align}\\] Então, se \\(f(X) = log(x)\\), \\(f^{-1}(x) = x\\). Disso se segue que: \\[\\begin{align} f(x) = log(\\frac{x}{1-x}) y = log(\\frac{x}{1-x}) \\\\ x = log(\\frac{y}{1-y}) \\\\ \\exp(x) = \\exp(log(\\frac{y}{1-y})) \\\\ \\exp(x) = \\frac{y}{1-y} \\\\ \\exp(x)(1-y) = y \\\\ \\exp(x)- y\\exp(x) = y \\\\ \\exp(x) = y + y\\exp(x) \\\\ \\exp(x) = y(1 + \\exp(x)) \\\\ \\frac{\\exp(x)}{(1 + \\exp(x))} = y \\\\ f^{-1}(x) = \\frac{\\exp(x)}{(1 + \\exp(x))} \\\\ f^{-1}(x) = \\frac{\\frac{\\exp(x)}{\\exp(x)}}{\\frac{(1 + \\exp(x))}{\\exp(x)}} \\\\ f^{-1}(x) = \\frac{1}{\\exp(-x) + 1}\\\\ \\end{align}\\] 13.3 Probabilidade e chance Se eu jogo uma moeda e observo 4 caras de um total de 9 lançamentos. Isso significa que observei cara \\(44,44\\%\\) das vezes e podemos pensar isso como uma probabilidade de observar cara. Outra forma de dizer a mesma coisa é notar que observamos \\(4\\) caras e \\(5\\) coroas. E podemos falar na razão \\(4/5\\), ou seja, o número de caras para o número de não-caras (que nesse caso é igual ao de coroas) que chamamos de chance. Ou seja, esperamos que para cada 4 caras observemos 5 coroas. A razão pode ser dada como fração, então \\(0,8\\). E podemos converter as chances em probabilidades e vice-versa de maneira relativamente fácil (mas não muito intuitiva). \\[ probabilidade = \\frac{chance}{1+chance} \\] E \\[ chance = \\frac{probabilidade}{1- probabilidade} \\] Notem que a definição de chance em termos de probabilidade é o que está na equação da logito. Isso sugere que podemos transformar a logito na logística e a logística na logito. \\[ f(x) = \\frac{1}{1 + \\exp(-x)} = \\frac{\\exp(x)}{1 + \\exp(x)} \\] Se eu passar o logaritmo (de base \\(exp\\)) em ambos os lados, temos: \\[ log(f(x)) =log(\\frac{1 + \\exp(-x)}{\\exp(-x)}) = 1 - 1 - exp(-x) \\] \\[ \\exp(logito(p)) = \\exp(log(\\frac{p}{1-p})) = \\] ### Logística como variável latente 13.4 Logística como GLM "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
